---
- type: model
  name: InternVideo
  organization: Shanghai AI Laboratory
  description: ''
  created_date: 2022-12-06
  url: https://arxiv.org/pdf/2212.03191.pdf
  model_card: ''
  modality: text, video; video
  analysis: ''
  size: 1.3B parameters (dense)
  dependencies:
    - Kinetics-400
    - WebVid-2M
    - WebVid-10M
    - HowTo100M
    - AVA
    - Something-Something-v2
    - Kinetics-710
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Lego-MT
  organization: Shanghai AI Laboratory
  description: Lego-MT is a multilingual large language model which uses a more
    efficient approach of being an effective detachable model.
  created_date: 2023-05-29
  url: https://arxiv.org/pdf/2212.10551.pdf
  model_card: ''
  modality: text; text
  analysis: Evaluated based on own constructed dataset covering 433 languages.
  size: 1.2B parameters (dense)
  dependencies: [OPUS]
  training_emissions: unknown
  training_time: 15 days
  training_hardware: 32 A100 GPUs
  quality_control: ''
  access: open
  license: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: MathCoder
  organization: Shanghai AI Laboratory
  description: MathCoder is a family of models capable of generating code-based
    solutions for solving challenging math problems.
  created_date: 2023-10-05
  url: https://arxiv.org/pdf/2310.03731.pdf
  model_card: none
  modality: text; text
  analysis: Evaluated on GSM8K and the competition-level MATH dataset.
  size: 70B parameters (dense)
  dependencies: [GPT-4, LLaMA 2]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 32 NVIDIA A800 80GB GPUs
  quality_control: none
  access: open
  license: unknown
  intended_uses: bridging the gap between natural language understanding and computational
    problem-solving
  prohibited_uses: none
  monitoring: none
  feedback: none
- type: model
  name: InternLM
  organization: Shanghai AI Laboratory
  description: InternLM is a high-quality language model proficient in English,
    Chinese, and code.
  created_date: 2023-09-20
  url: https://github.com/InternLM/InternLM
  model_card: https://huggingface.co/internlm/internlm-20b
  modality: code, text; code, text
  analysis: Evaluated in comparison to LLaMA series models on standard benchmarks.
  size: 20B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/internlm/internlm-20b/discussions
- type: model
  name: InternVideo2
  organization: Shanghai AI Laboratory, Nanjing University, Zhejiang University
  description: InternVideo2 is a new video foundation model (ViFM) that achieves the state-of-the-art performance in action recognition, video-text tasks, and video-centric dialogue.
  created_date: 2024-03-22
  url: https://github.com/OpenGVLab/InternVideo2
  model_card: none
  modality: text, video; text
  analysis: Evaluated across a range of video-related tasks and compared to other multimodal models like CLIP, VideoPrism, and VideoCoCa. InternVideo 2 generally performs among the best of such models on these benchmarks.
  size: 6B parameters
  dependencies: [InternVL, VideoMAEv2, LAION, WebVid, InternVid, LLaVA, KMash]
  training_emissions: unknown
  training_time: 35 days
  training_hardware: 256 NVIDIA A100 GPUs for 32 days, and 64 GPUs for 3 days
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: none
