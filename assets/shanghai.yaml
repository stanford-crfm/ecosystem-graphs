---
- type: model
  name: InternVideo
  organization: Shanghai AI Laboratory
  description: ''
  created_date: 2022-12-06
  url: https://arxiv.org/pdf/2212.03191.pdf
  model_card: ''
  modality: text, video; video
  analysis: ''
  size: 1.3B parameters (dense)
  dependencies:
    - Kinetics-400
    - WebVid-2M
    - WebVid-10M
    - HowTo100M
    - AVA
    - Something-Something-v2
    - Kinetics-710
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Lego-MT
  organization: Shanghai AI Laboratory
  description: Lego-MT is a multilingual large language model which uses a more
    efficient approach of being an effective detachable model.
  created_date: 2023-05-29
  url: https://arxiv.org/pdf/2212.10551.pdf
  model_card: ''
  modality: text; text
  analysis: Evaluated based on own constructed dataset covering 433 languages.
  size: 1.2B parameters (dense)
  dependencies: [OPUS]
  training_emissions: unknown
  training_time: 15 days
  training_hardware: 32 A100 GPUs
  quality_control: ''
  access: open
  license: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: MathCoder
  organization: Shanghai AI Laboratory
  description: MathCoder is a family of models capable of generating code-based
    solutions for solving challenging math problems.
  created_date: 2023-10-05
  url: https://arxiv.org/pdf/2310.03731.pdf
  model_card: none
  modality: text; text
  analysis: Evaluated on GSM8K and the competition-level MATH dataset.
  size: 70B parameters (dense)
  dependencies: [GPT-4, LLaMA 2]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 32 NVIDIA A800 80GB GPUs
  quality_control: none
  access: open
  license: unknown
  intended_uses: bridging the gap between natural language understanding and computational
    problem-solving
  prohibited_uses: none
  monitoring: none
  feedback: none
- type: model
  name: InternLM
  organization: Shanghai AI Laboratory
  description: InternLM is a high-quality language model proficient in English,
    Chinese, and code.
  created_date: 2023-09-20
  url: https://github.com/InternLM/InternLM
  model_card: https://huggingface.co/internlm/internlm-20b
  modality: code, text; code, text
  analysis: Evaluated in comparison to LLaMA series models on standard benchmarks.
  size: 20B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/internlm/internlm-20b/discussions
- type: model
  name: InternVideo2
  organization: Shanghai AI Laboratory, Nanjing University, Zhejiang University
  description: InternVideo2 is a new video foundation model (ViFM) that achieves
    the state-of-the-art performance in action recognition, video-text tasks, and
    video-centric dialogue.
  created_date: 2024-03-22
  url: https://github.com/OpenGVLab/InternVideo2
  model_card: none
  modality: text, video; text
  analysis: Evaluated across a range of video-related tasks and compared to other
    multimodal models like CLIP, VideoPrism, and VideoCoCa. InternVideo 2 generally
    performs among the best of such models on these benchmarks.
  size: 6B parameters
  dependencies:
    - InternVL
    - VideoMAEv2
    - LAION
    - WebVid
    - InternVid
    - LLaVA
    - KMash
  training_emissions: unknown
  training_time: 35 days
  training_hardware: 256 NVIDIA A100 GPUs for 32 days, and 64 GPUs for 3 days
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: none
- type: model
  name: CosmicMan
  organization: Shanghai AI Laboratory
  description: CosmicMan is a text-to-image foundation model specialized for generating
    high-fidelity human images with meticulous appearance, reasonable structure,
    and precise text-image alignment.
  created_date: 2024-04-01
  url: https://cosmicman-cvpr2024.github.io/
  model_card: none
  modality: text; image
  analysis: The model was compared with SOTAs and has shown good performance in
    generating high-quality human images.
  size: unknown
  dependencies: [CosmicMan-HQ 1.0]
  training_emissions: unknown
  training_time: 1 week
  training_hardware: 32 80G NVIDIA A100 GPUs
  quality_control: The quality control measures taken include modeling the relationship
    between dense text descriptions and image pixels in a decomposed manner and
    enforcing attention refocusing without adding extra modules.
  access: open
  license: unknown
  intended_uses: The model is intended to generate high-quality, photorealistic
    human images from text descriptions. Applications include avatar generation
    and potentially virtual reality and video game character creation.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown

- type: dataset
  name: CosmicMan-HQ 1.0
  organization: Shanghai AI Laboratory
  description: CosmicMan-HQ 1.0 is a large-scale dataset with 6 million high-quality,
    real-world human images.
  created_date: 2024-04-28
  url: https://arxiv.org/pdf/2404.01294
  datasheet: none
  modality: image
  size: 6 million images
  sample: []
  analysis: Compared to other human image datasets on data quantity, image quality,
    and annotations.
  dependencies: []
  included: ''
  excluded: ''
  quality_control: unknown
  access: open
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: none
- type: model
  name: InternLM2.5-7B-Chat GGUF Model
  organization: Shanghai AI Laboratory
  description: The model is a conversational language model that can understand and communicate fluently in the language chosen by the user such as English and 中文. The model is provided in GGUF format and can be utilized by llama.cpp, an open-source framework for large language model inference.
  created_date: Unknown
  url: https://huggingface.co/internlm/internlm2_5-7b-chat-gguf
  model card: https://huggingface.co/internlm/internlm2_5-7b-chat-gguf
  modality: text; text
  analysis: Unknown
  size: 7B parameters
  dependencies: [llama.cpp]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: Unknown.
  access: Unknown
  license: Unknown
  intended_uses: The model is designed to be a helpful, honest, and harmless virtual assistant that can understand and communicate fluently in multiple languages.
  prohibited_uses: Unknown
  monitoring: Unknown
  feedback: Unknown.
