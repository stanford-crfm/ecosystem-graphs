---
- type: model
  name: Falcon-40B
  organization: UAE Technology Innovation Institute
  description: Falcon-40B is a 40B parameters causal decoder-only model built by
    TII and trained on 1,000B tokens ofÂ RefinedWeb enhanced with curated corpora.
  created_date: 2023-06-14
  url: https://arxiv.org/pdf/2311.16867.pdf
  model_card: https://huggingface.co/tiiuae/falcon-40b
  modality: text; text
  analysis: Evaluated in 1-shot against the PaLM models, with the tasks of the paper
    "Language models are few-shot learners" (Brown et al., 2020); (2) on a small
    set of few-shot tasks reported by the GPT-4 paper; (3) against state-of-the-art
    models across common sense, question answering, and code tasks; (4) against
    models which also report results from the EAI Harness, for which we are able
    to compare with identical prompts and metrics.
  size: 40B parameters (dense)
  dependencies: [RefinedWeb]
  training_emissions: unknown
  training_time: 2 months
  training_hardware: 384 A100 40GB GPUs
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: Research on large language models; as a foundation for further
    specialization for specific use cases.
  prohibited_uses: irresponsible or harmful use or production use without adequate
    assessment of risks and mitigation.
  monitoring: None
  feedback: https://huggingface.co/tiiuae/falcon-40b/discussions
- type: dataset
  name: RefinedWeb
  organization: UAE Technology Innovation Institute
  description: RefinedWeb is a high-quality five trillion tokens web-only English
    pretraining dataset.
  created_date: 2023-06-01
  url: https://arxiv.org/pdf/2306.01116.pdf
  datasheet: https://huggingface.co/datasets/tiiuae/falcon-refinedweb
  modality: text
  size: 600B tokens
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license:
    explanation: License can be found at https://huggingface.co/datasets/tiiuae/falcon-refinedweb
    value: custom
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Falcon-180B
  organization: UAE Technology Innovation Institute
  description: Falcon-180B is a 180B parameters causal decoder-only model built
    by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora.
  created_date: 2023-09-06
  url: https://arxiv.org/pdf/2311.16867.pdf
  model_card: https://huggingface.co/tiiuae/falcon-180B
  modality: text; text
  analysis: Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT on the Open
    LLM Leaderboard at https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.
  size: 180B parameters (dense)
  dependencies: [RefinedWeb]
  training_emissions: ''
  training_time: 9 months
  training_hardware: 4096 A100 40GB GPUs
  quality_control: ''
  access: open
  license: unknown
  intended_uses: Research on large language models; as a foundation for further
    specialization for specific use cases.
  prohibited_uses: Production use without adequate assessment of risks and mitigation;
    any use cases which may be considered irresponsible or harmful.
  monitoring: None
  feedback: https://huggingface.co/tiiuae/falcon-180b/discussions
- type: model
  name: JASCO
  organization: The Hebrew University of Jerusalem, Meta, FAIR Team
  description: JASCO is a temporally controlled text-to-music generation model utilizing both symbolic and audio-based conditions. The model is based on the Flow-Matching modeling paradigm and a novel conditioning method that allows music generation controlled both locally and globally. It can generate high-quality music samples conditioned on global text descriptions along with fine-grained local controls. The model incorporates various symbolic control signals and audio representations for versatility in music generation.
  created_date: 2024-06-12
  url: https://arxiv.org/pdf/2406.10970
  model card: https://arxiv.org/pdf/2406.10970
  modality: text; music
  analysis: JASCO was evaluated considering both generation quality and condition adherence, using a mix of objective metrics and human studies. It was compared to several baselines and found to provide comparable performance while offering a significantly richer set of controls.
  size: Unknown
  dependencies: [Flow-Matching, source separation network, F0 saliency detector model, chord progression extraction model]
  training_emissions: Unknown
  training_time: Unknown 
  training_hardware: Unknown
  quality_control: Evaluations were done on generation quality and condition adherence. Comparisons were made with various baselines.
  access: open
  license: Unknown 
  intended_uses: The model can be used for music generation, especially for empowering content creators, advertisers, and video game designers. It's of particular value to users who need fine-grained local controls in their music creation, such as professional musicians.
  prohibited_uses: Unknown 
  monitoring: Unknown
  feedback: Likely through the authors or organizations affiliated with the model.
