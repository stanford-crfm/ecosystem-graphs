---
- access:
    explanation: Models are available for download from [[GitHub repository]](https://github.com/facebookresearch/esm#available-models)
    value: open
  analysis: ''
  created_date:
    explanation: The date the [[model paper]](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2.full.pdf+html)
      was released
    value: 2022-10-31
  dependencies: [UniRef50, UniRef90]
  description: ESM-2 is a series of protein language models trained on protein sequences
  feedback: ''
  intended_uses: ''
  license:
    explanation: "The license is provided in the [[Github repository]](https://github.com/facebookresearch/esm#available-models)\n"
    value: MIT
  modality:
    explanation: protein sequence
    value: text; text
  model_card: none
  monitoring: ''
  name: ESM-2
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 15B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2.full.pdf+html
- access: closed
  analysis: none
  created_date:
    explanation: The date the model paper was released
    value: 2021-12-08
  datasheet: none
  dependencies:
    - COCO
    - YFCC100M
    - SBU Captions
    - Localized Narratives
    - Visual Genome
    - Wikipedia
    - Conceptual Captions
    - Red Caps
  description: PMD (Public Multimodal Datasets) is a collection of image-text datasets
    introduced in the FLAVA work.
  excluded: YFCC100M is filtered for non-English captions and very short (< 2 word)
    captions.
  feedback: none
  included: none
  intended_uses: unknown
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality:
    explanation: text, image
    value: image, text
  monitoring: none
  name: PMD
  organization: Meta
  prohibited_uses: unknown
  quality_control: Beyond filtering mentioned in excluded, nothing further is done.
  sample: []
  size: 70M
  type: dataset
  url: https://arxiv.org/abs/2112.04482
- access:
    explanation: "Model checkpoints are available for download from the [[HuggingFace\
      \ repository]](https://huggingface.co/facebook/flava-full)\n"
    value: open
  analysis: FLAVA is benchmarked on a range of vision-only (e.g. CIFAR-10), language-only
    (e.g. GLUE), and multimodal (e.g. Hateful Memes) standard evaluations.
  created_date:
    explanation: The date the model paper was released
    value: 2021-12-08
  dependencies: [PMD]
  description: FLAVA is a multimodal model composed of an image encoder, text encoder,
    and multimodal encoder.
  feedback: https://huggingface.co/facebook/flava-full/discussions
  intended_uses: "Per the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full),\
    \ \"The model is intended to serve as a reproducible research artifact for research\
    \ communities in the light of models whose exact reproduction details are never\
    \ released such as CLIP and SimVLM.\"\n"
  license:
    explanation: "The license is provided in the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full)\n"
    value: BSD-3-Clause
  modality:
    explanation: Text (English) and Image
    value: image, text; image, text
  model_card: https://huggingface.co/facebook/flava-full
  monitoring: none
  name: FLAVA
  organization: Meta
  prohibited_uses: "Per the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full),\
    \ \"Any deployed use case of the model - whether commercial or not\" - is currently\
    \ out of scope.\n"
  quality_control: FLAVA introduces a variety of new modeling techniques, specifically
    with an interest in improved text-image alignment through contrastive objectives.
  size: 306M parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://arxiv.org/abs/2112.04482
- access: closed
  analysis: none
  created_date:
    explanation: "The date the Galactica paper was released\n"
    value: 2022-11-15
  datasheet: none
  dependencies: [CommonCrawl, Wikipedia, arXiv]
  description: The Galactica Corpus is a collection of scientific datasets introduced
    in the Galactica work.
  excluded: ''
  feedback: none
  included: Prompts and reasoning data is explicitly included to improve model capabilities
    derived from this data.
  intended_uses: unknown
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n"
    value: unknown
  modality: text
  monitoring: none
  name: The Galactica Corpus
  organization: Meta
  prohibited_uses: unknown
  quality_control: ''
  sample: []
  size: 106B tokens
  type: dataset
  url: https://galactica.org/static/paper.pdf
- access:
    explanation: Model checkpoints freely available at https://github.com/paperswithcode/galai
    value: open
  analysis: ''
  created_date:
    explanation: "The date the Galactica paper was released\n"
    value: 2022-11-15
  dependencies: [The Galactica Corpus]
  description: Galactica is a family of autoregressive language models.
  feedback: ''
  intended_uses: ''
  license:
    explanation: https://github.com/paperswithcode/galai/blob/main/LICENSE-MODEL.md
    value: CC BY-NC 4.0
  modality:
    explanation: Text (English), Code, Math, Chemistry, Biology
    value: code, text; code, text
  model_card: https://huggingface.co/facebook/galactica-6.7b
  monitoring: ''
  name: Galactica
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 120B parameters (dense)
  training_emissions: unknown
  training_hardware: Meta AI Cluster. Trained on 1024 80GB A100 GPUs (128 8xA100
    80GB nodes)
  training_time: unknown
  type: model
  url: https://galactica.org/static/paper.pdf
- access:
    explanation: Model weights are available via the [[HuggingFace repository]](https://huggingface.co/facebook/incoder-6B)
    value: open
  analysis: none
  created_date:
    explanation: The date the model paper was released
    value: 2022-04-12
  dependencies: []
  description: InCoder is a language model trained on code with a causal masking
    objective
  feedback: ''
  intended_uses: ''
  license:
    explanation: The license is provided in the [[HuggingFace repository]](https://huggingface.co/facebook/incoder-6B?text=My+name+is+Lewis+and+I+like+to)
    value: CC BY-NC 4.0
  modality:
    explanation: text (English) and code
    value: code, text; code, text
  model_card: none
  monitoring: ''
  name: InCoder
  organization: Meta, CMU, TTI-Chicago, UC Berkeley, University of Washington
  prohibited_uses: ''
  quality_control: unknown
  size: 6B parameters (dense)
  training_emissions: Unknown
  training_hardware: 248 V100 GPUs, according to [[the paper]](https://arxiv.org/pdf/2204.05999.pdf)
  training_time: 24 days, according to [[the paper]](https://arxiv.org/pdf/2204.05999.pdf)
  type: model
  url: https://arxiv.org/abs/2204.05999
- access:
    explanation: The 175B model requires manual approval from Meta to access. Other
      models are available through HuggingFace.
    value: limited
  analysis: ''
  created_date:
    explanation: "The date the OPT paper was submitted to Arxiv\n"
    value: 2022-05-01
  dependencies: [RoBERTa dataset, The Pile, PushShift.io Reddit]
  description: OPT is a family of autoregressive language models.
  feedback: ''
  intended_uses: ''
  license:
    explanation: "All released with the [[OPT-175B License]](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md),\
      \ except 66B (TBD) and 17B (requires manual approval)\n"
    value: OPT-175B License
  modality:
    explanation: text (English)
    value: text; text
  model_card: https://arxiv.org/pdf/2205.01068.pdf
  monitoring: ''
  name: OPT
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 175B parameters (dense)
  training_emissions:
    explanation: "Estimate by authors for the OPT-175B model only. Not including\
      \ ablations and baselines.\n"
    value: 75 tCO2e
  training_hardware: Meta AI cluster. Trained on 992 80GB A100 GPUs
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2205.01068
- access:
    explanation: "The datasets involved are public, but the full dataset is not\
      \ directly available, nor are filtering scripts.\n"
    value: limited
  analysis: ''
  created_date:
    explanation: "The date that Make-A-Video was posted to arXiv [[arXiv]] (https://arxiv.org/abs/2209.14792).\n"
    value: 2022-09-29
  datasheet: none
  dependencies: [LAION-5B, WebVid-10M, HD-VILA-100M]
  description: "The Make-A-Video dataset is the dataset used to train Make-A-Video,\
    \ which includes both image-text and video-only datasets with specific and significant\
    \ filtering.\n"
  excluded: "The LAION-5B dataset is filtered to 2.3B by removing NSFW images using\
    \ [https://github.com/GantMan/nsfw](https://github.com/GantMan/nsfw), toxic\
    \ words in text, and images with watermark probability > 0.5. The HD-VILA-100M\
    \ is randomly subsampled to 10M video clips.\n"
  feedback:
    explanation: "No feedback mechanism is mentioned by the authors.\n"
    value: none
  included:
    explanation: "Data from the three underlying datasets is filtered, but nothing\
      \ is included beyond this.\n"
    value: none
  intended_uses: unknown
  license:
    explanation: "No license was found, though the underlying datasets are public\
      \ and have licenses.\n"
    value: none
  modality:
    explanation: video, image-text
    value: image, text, video
  monitoring:
    explanation: "There is no information on how Meta is internally monitoring the\
      \ use of the dataset.\n"
    value: unknown
  name: Make-A-Video dataset
  organization: Meta
  prohibited_uses: unknown
  quality_control: "The authors exclude NSFW, toxic, and likely watermarked data\
    \ from LAION-5B.\n"
  sample: []
  size: 20M video clips, 2.3B image-text pairs
  type: dataset
  url: https://arxiv.org/pdf/2209.14792.pdf
- access:
    explanation: "The model has not been released; a form existed to potentially\
      \ acquire access but is now closed as of 2022-12-07 [[Access Form]](https://docs.google.com/forms/u/0/d/e/1FAIpQLSfMjC57wcXWUDV0UbS2Tn6VhjLEiCXaHvWZuWgWRa-Zx8-Few/closedform).\n"
    value: closed
  analysis: "Model performance was evaluated using automated (Frechet Video Distance;\
    \ Frechet Inception Distance) and human evaluation on two datasets (UCF-101,\
    \ MSR-VTT) in the zero-shot setting.\n"
  created_date:
    explanation: "The date that Make-A-Video was posted to arXiv [[arXiv]] (https://arxiv.org/abs/2209.14792).\n"
    value: 2022-09-29
  dependencies: [Make-A-Video dataset]
  description: "Make-A-Video is a model for Text-to-Video Generation without Text-Video\
    \ Data.\n"
  feedback:
    explanation: "Authors do not mention or provide a feedback mechanism.\n"
    value: none
  intended_uses:
    explanation: "Authors do not report the intended uses.\n"
    value: unknown
  license:
    explanation: No license was found.
    value: none
  modality:
    explanation: text, video
    value: text, video; text, video
  model_card: none
  monitoring:
    explanation: "Authors do not report the monitoring process for Make-A-Video\
      \ internally at Meta.\n"
    value: unknown
  name: Make-A-Video
  organization: Meta
  prohibited_uses:
    explanation: "Authors do not report the prohibited uses.\n"
    value: unknown
  quality_control:
    explanation: "Authors do not report specific quality control steps taken in\
      \ modeling, though filtering is done in producing the Make-A-Video dataset.\n"
    value: none
  size: unknown
  training_emissions:
    explanation: "Authors do not report the training emissions.\n"
    value: unknown
  training_hardware:
    explanation: "Authors do not report the training hardware or provider.\n"
    value: unknown
  training_time:
    explanation: "Authors do not report the training time.\n"
    value: unknown
  type: model
  url: https://arxiv.org/pdf/2209.14792.pdf
- access: open
  analysis: ''
  created_date: 2023-02-24
  dependencies:
    - CommonCrawl
    - C4
    - Github
    - Wikipedia
    - BooksCorpus
    - arXiv
    - StackExchange
  description: ''
  feedback: ''
  intended_uses: ''
  license: LLaMa License (model weights), GPLv3 (code)
  modality:
    explanation: Text
    value: text; text
  model_card: ''
  monitoring: ''
  name: LLaMA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 65B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2302.13971
- access: open
  analysis: Evaluated on standard academic benchmarks and internal Meta libraries.
  created_date: 2023-07-18
  dependencies: []
  description: LLaMA 2 is an updated version of LLaMA trained on a new mix of publicly
    available data.
  feedback: ''
  intended_uses: LLaMA 2 is intended for commercial and research use in English.
    Tuned models are intended for assistant-like chat, whereas pretrained models
    can be adapted for a variety of natural language generation tasks.
  license:
    explanation: The license can be found at https://ai.meta.com/resources/models-and-libraries/llama-downloads/
    value: custom
  modality:
    explanation: text
    value: text; text
  model_card: Can be found at appendix of paper at https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
  monitoring: ''
  name: LLaMA 2
  organization: Meta
  prohibited_uses: Use in any manner that violates applicable laws or regulations
    (including trade compliance laws). Use in languages other than English. Use
    in any other way that is prohibited by the Acceptable Use Policy and Licensing
    Agreement for LLaMA 2.
  quality_control: ''
  size: 70B parameters (dense)
  training_emissions: 539 tCO2eq
  training_hardware: NVIDIA A100-80GB GPUs (TDP of 350-400W)
  training_time: ''
  type: model
  url: https://ai.meta.com/resources/models-and-libraries/llama/
- access: open
  analysis: ''
  created_date: 2022-12-22
  dependencies: [OPT, OPT-IML Bench]
  description: ''
  feedback: ''
  intended_uses: ''
  license: OPT-IML 175B License
  modality:
    explanation: text
    value: text; text
  model_card: ''
  monitoring: ''
  name: OPT-IML
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 175B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2212.12017
- access:
    explanation: "The full dataset can be downloaded at [[SA-1B Download]](https://ai.facebook.com/datasets/segment-anything-downloads/).\
      \ A 50k image preview of the full dataset is available [[here]](https://segment-anything.com/dataset/index.html).\n"
    value: open
  analysis: ''
  created_date:
    explanation: The date the [[Meta blog post]](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)
      was released.
    value: 2023-04-05
  datasheet:
    explanation: Datasheet can be found in the Appendix section of the Segment Anything
      paper.
    value: https://arxiv.org/pdf/2304.02643.pdf#page=25
  dependencies: []
  description: "SA-1B (Segment Anything 1 Billion) is a dataset designed for training\
    \ general-purpose object segmentation models from open world images. It consists\
    \ of 11M diverse, high-resolution, privacy protecting images and 1.1B high-quality\
    \ segmentation masks.\n"
  excluded:
    explanation: See [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25)
    value: "\"We withheld ~2k randomly selected images for testing purposes.\" \
      \ \"Each image is accompanied by a short caption that describes the content\
      \ and place of the photo in a free form text. Per our agreement with the photo\
      \ provider we are not allowed to release these captions.\"\n"
  feedback: Feedback can be given via the feedback form on their website [segment-anything.com](https://segment-anything.com/)
    or by emailing at segment-anything at meta.com.
  included:
    explanation: According to section [[Segment Anything Dataset]](https://arxiv.org/pdf/2304.02643.pdf#section.5)
      of the paper and [[SA-1B website]](https://ai.facebook.com/datasets/segment-anything/).
    value: "SA-1B consists of 11M diverse, high-resolution (averaging 1500×2250\
      \ pixels), and privacy protecting images collected and licensed from a third\
      \ party photo company. The images are photos taken from a camera, i.e. not\
      \ artwork. The images vary in subject matter. Common themes of the images\
      \ include: locations, objects, scenes. The dataset includes 1.1B high-quality\
      \ segmentation masks collected with the Segment Anything Data Engine. SA-1B\
      \ only includes automatically generated masks (99.1%), as the authors conclude\
      \ after experiments that the automatic masks are high quality and effective\
      \ for training models. The masks range from large scale objects such as buildings\
      \ to fine grained details such as door handles. Masks are provided in the\
      \ COCO run-length encoding (RLE) annotation format.\n"
  intended_uses:
    explanation: See [[SA-1B website]](https://ai.facebook.com/datasets/segment-anything/)
    value: SA-1B is intended to be used for research purposes only. It allows access
      to a privacy protecting and copyright friendly large-scale image dataset.
      Researchers can use it to train and evaluate generic object segmentation models.
  license:
    explanation: SA-1B is released under a favorable license agreement for certain
      research uses and with protections for researchers. See [[SA-1B Dataset Research
      License]](https://ai.facebook.com/datasets/segment-anything-downloads/).
    value: SA-1B Dataset Research License
  modality: image
  monitoring:
    explanation: See [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25)
    value: "The dataset will be hosted at https://ai.facebook.com/datasets/segment-anything\
      \ and maintained by Meta AI. \"If a user observes objectionable image(s) in\
      \ the dataset, we invite them to report the image(s) at segment-anything at\
      \ meta.com for removal\" \"To aid reproducibility of research using SA-1B,\
      \ the only updates (to the dataset) will be to remove reported images.\" \"\
      We encourage users to gather further annotations for SA-1B. Any users who\
      \ generate annotations will be liable for hosting and distributing their annotations.\"\
      \n"
  name: SA-1B
  organization: Meta
  prohibited_uses:
    explanation: See [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25)
    value: "Authors note the following limitations of the dataset:\n  The masks\
      \ are generated by a segmentation model, so there may be errors\nor inconsistencies\
      \ in the masks.\n  While no two images are the same, there are instances of\
      \ images of the same\nsubject taken close together in time.\n  The dataset\
      \ contains scenes of protests, or other gatherings that may suggest\nreligious\
      \ beliefs, political opinions or union memberships that may be offensive.\n"
  quality_control:
    explanation: According to sections [[Segment Anything Dataset]](https://arxiv.org/pdf/2304.02643.pdf#section.5)
      and [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25) of the paper.
    value: "- Dataset quality:\n  Due to potential accessibility and storage challenges,\
      \ the original high-resolution images (averaging 3300×4950 pixels) were downsampled\
      \ to an average resolution of 1500×2250 pixels. Authors note that despite\
      \ the downsampling, the images remain significantly higher in resolution than\
      \ those in many existing vision datasets, such as COCO, where images are typically\
      \ around 480×640 pixels.\n  The images were processed to blur faces and license\
      \ plates to protect the identities of those in the image.\n  To estimate the\
      \ quality of the masks in the images, a random sample of 500 images (∼50k\
      \ masks) was taken and professional annotators were asked to improve the quality\
      \ of all masks in those images.\n- Safety measures:\n  Authors implemented\
      \ two safety measures to prevent objectionable content:\n    (1) Photos are\
      \ licensed from a photo provider and had to meet the terms of service of the\
      \ photo provider. Authors requested that all objectionable content be filtered\
      \ from the images they licensed.\n    (2) Users who observe objectionable\
      \ images in the dataset are invited to report them for removal at segment-anything@meta.com.\n\
      \  Despite these measures, they observed that a small portion of images contain\
      \ scenes of protests or other gatherings that focus on a diverse spectrum\
      \ of religious beliefs or political opinions that may be considered offensive.\
      \ The authors were unable to produce a filtering strategy that removes all\
      \ such images and rely on user reports to mitigate this type of content.\n"
  sample: []
  size: 11M images, 1.1B mask annotations
  type: dataset
  url: https://ai.facebook.com/datasets/segment-anything/
- access:
    explanation: "Inference code and model checkpoints are available on the model's\
      \ [[GitHub repository]](https://github.com/facebookresearch/segment-anything).\
      \ Its training dataset SA-1B can be used for research purposes and is available\
      \ for download [here](https://ai.facebook.com/datasets/segment-anything-downloads/).\n"
    value: open
  analysis:
    explanation: See [[Zero-Shot Transfer Experiments]](https://arxiv.org/pdf/2304.02643.pdf#section.7)
      for more details.
    value: "\"We extensively evaluate SAM. First, using a diverse new suite of 23\
      \ segmentation datasets, we find that SAM produces high-quality masks from\
      \ a single foreground point, often only slightly below that of the manually\
      \ annotated ground truth. Second, we find consistently strong quantitative\
      \ and qualitative results on a variety of downstream tasks under a zero-shot\
      \ transfer protocol using prompt engineering, including edge detection, object\
      \ proposal generation, instance segmentation, and a preliminary exploration\
      \ of text-to-mask prediction.\"\n"
  created_date:
    explanation: The date the [[Meta blog post]](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)
      was released.
    value: 2023-04-05
  dependencies: [SA-1B]
  description: SAM (Segment Anything Model) is a foundation model for image segmentation.
    The model is designed and trained to be promptable, and supports flexible prompts
    (point, box, mask and free-form text) to compute masks in real-time to allow
    interactive use.
  feedback: Feedback can be given via the feedback form on their website [segment-anything.com](https://segment-anything.com/)
    or by emailing at segment-anything at meta.com.
  intended_uses:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: "\"SAM is intended to be used for any prompt-based segmentation task.\
      \ We explored its use in segmenting objects from a point, edge detection,\
      \ segmenting all objects, and segmenting detected objects. We explored how\
      \ SAM can integrate with other vision models to segment objects from text.\"\
      \n"
  license:
    explanation: See [[LICENSE]](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)
    value: Apache 2.0
  modality:
    explanation: image, text
    value: image, text; image, text
  model_card:
    explanation: Model card can be found in the Appendix section of the paper.
    value: https://arxiv.org/pdf/2304.02643.pdf#page=28
  monitoring: ''
  name: SAM
  organization: Meta
  prohibited_uses:
    explanation: See [[Discussion]](https://arxiv.org/pdf/2304.02643.pdf#section.8)
    value: "For out-of-scope use cases see terms of use in [[LICENSE]](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE).\
      \ Authors also discuss the following limitations of the model: \"While SAM\
      \ performs well in general, it is not perfect. It can miss fine structures,\
      \ hallucinates small disconnected components at times, and does not produce\
      \ boundaries as crisply as more computationally intensive methods that “zoom-in”,\
      \ e.g. [18]. In general, we expect dedicated interactive segmentation methods\
      \ to outperform SAM when many points are provided, e.g. [67]. Unlike these\
      \ methods, SAM is designed for generality and breadth of use rather than high\
      \ IoU interactive segmentation. Moreover, SAM can process prompts in real-time,\
      \ but nevertheless SAM's overall performance is not real-time when using a\
      \ heavy image encoder. Our foray into the text-to-mask task is exploratory\
      \ and not entirely robust, although we believe it can be improved with more\
      \ effort. While SAM can perform many tasks, it is unclear how to design simple\
      \ prompts that implement semantic and panoptic segmentation. Finally, there\
      \ are domain-specific tools, such as [7], that we expect to outperform SAM\
      \ in their respective domains.\"\n"
  quality_control:
    explanation: See [[Segment Anything RAI Analysis]](https://arxiv.org/pdf/2304.02643.pdf#section.6)
      for more details.
    value: "\"We perform a Responsible AI (RAI) analysis of our work by investigating\
      \ potential fairness concerns and biases when using SA-1B and SAM. We focus\
      \ on the geographic and income distribution of SA-1B and fairness of SAM across\
      \ protected attributes of people.\"\n"
  size: unknown
  training_emissions:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: 2.8 metric tons of carbon dioxide
  training_hardware:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: 256 A100 GPUs
  training_time:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: 68 hours
  type: model
  url: https://arxiv.org/pdf/2304.02643.pdf
- access: closed
  analysis: Evaluated on zero-shot text-to-speech benchmarks, with Voicebox outperforming
    the current state-of-the-art English model VALL-E.
  created_date: 2023-06-16
  dependencies: []
  description: Voicebox is the first generative AI model for speech to generalize
    across tasks with state-of-the-art performance.
  feedback: ''
  intended_uses: ''
  license: ''
  modality:
    explanation: text and audio input and audio output
    value: audio, text; audio
  model_card: ''
  monitoring: ''
  name: Voicebox
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 330M parameters (dense)
  training_emissions: unknown
  training_hardware: 32 GPUs of unspecified type
  training_time: 750,000 iterations
  type: model
  url: https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/
- access: open
  analysis: PEER is evaluated on core research questions intended to gauge language
    understanding, proper use of citations, instruction following, and iterative
    use.
  created_date: 2022-08-24
  dependencies: []
  description: PEER is a collaborative language model that is trained to imitate
    the entire writing process itself. PEER can write drafts, add suggestions, propose
    edits and provide explanations for its actions.
  feedback: ''
  intended_uses: adapting LLMs to work with collaborative writing and updating.
  license: ''
  modality:
    explanation: natural language text
    value: text; text
  model_card: ''
  monitoring: ''
  name: PEER
  organization: Meta
  prohibited_uses: ''
  quality_control: Heuristics and edit filtering was used on data set, which consisted
    mostly of Wikipedia pages.
  size: 3B parameters (dense)
  training_emissions: ''
  training_hardware: 64 GPUs
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2208.11663.pdf
