---
- type: dataset
  name: LAION-1B
  organization: Alibaba
  description: ''
  created_date: 2023-02-20
  url: https://arxiv.org/pdf/2302.09778.pdf
  datasheet: ''
  modality: image, text
  size: 1B image-text pairs
  sample: []
  analysis: ''
  dependencies: [LAION-5B]
  included: ''
  excluded: We eliminate duplicates, low resolution images, and images potentially
    contain harmful content from the LAION dataset.
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Composer
  organization: Alibaba
  description: ''
  created_date: 2023-02-20
  url: https://arxiv.org/pdf/2302.09778.pdf
  model_card: ''
  modality: image, text; image
  analysis: ''
  size: 4.4B parameters (dense)
  dependencies: [ImageNet, WebVision, LAION-1B]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Qwen
  organization: Alibaba
  description: 'QWEN is a comprehensive language model series that encompasses distinct
    models with varying parameter counts. Qwen series, now including Qwen, the base
    language models, namely Qwen-7B and Qwen-14B, as well as Qwen-Chat, the chat
    models, namely Qwen-7B-Chat and Qwen-14B-Chat. '
  created_date: 2023-08-03
  url: https://arxiv.org/abs/2309.16609
  model_card: https://huggingface.co/Qwen
  modality: image, text; text
  analysis: Evaluated on MMLU, C-Eval, GSM8K, MATH, HumanEval, etc.
  size: 14B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: ''
  training_hardware: ''
  quality_control:
    explanation: According to [[Model Description Section 2]](https://arxiv.org/pdf/2302.09778.pdf)
    value: They filter out low-quality data, they employ a combination of rule-based
      and machine-learning-based methods. Specifically, they use multiple models
      to score the content, including language models, text-quality scoring models,
      and models for identifying potentially offensive or inappropriate content.
      They also manually sample texts from various sources and review them to ensure
      their quality. To further enhance the quality of our data, they selectively
      up-sample data from certain sources, to ensure that our models are trained
      on a diverse range of high-quality content.
  access:
    explanation: "Model checkpoints are available for download from the [[HuggingFace\
      \ repository]](https://huggingface.co/Qwen)\n"
    value: open
  license:
    explanation: Model license can be found at https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT.
      Code license is under Apache 2.0
    value: custom
  intended_uses: ''
  prohibited_uses: ''
  monitoring: Governed by the laws of China, without regard to conflict of law principles,
    and the UN Convention on Contracts for the International Sale of Goods does
    not apply to this Agreement. And The People's Courts in Hangzhou City shall
    have exclusive jurisdiction over any dispute arising out of this Agreement.
  feedback: ''

- type: model
  name: Qwen 1.5
  organization: Alibaba
  description: Qwen 1.5 is the next iteration in their Qwen series, consisting of
    Transformer-based large language models pretrained on a large volume of data,
    including web texts, books, codes, etc.
  created_date: 2024-02-04
  url: https://qwenlm.github.io/blog/qwen1.5/
  model_card: https://huggingface.co/Qwen/Qwen1.5-72B
  modality: text; text
  analysis: Base models are evaluated on MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP,
    BBH, CMMLU, all standard English and Chinese benchmarks, and chat models are
    evaluated on Chatbot Arena, AlpacaEval, MT-Bench, etc.
  size: 72B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: open
  license:
    explanation: Model license can be found at https://huggingface.co/Qwen/Qwen1.5-72B/blob/main/LICENSE
    value: custom
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/Qwen/Qwen1.5-72B/discussions

- type: model
  name: Qwen 1.5 MoE
  organization: Qwen Team
  description: Qwen 1.5 is the next iteration in their Qwen series, consisting of
    Transformer-based large language models pretrained on a large volume of data,
    including web texts, books, codes, etc. Qwen 1.5 MoE is the MoE model of the
    Qwen 1.5 series.
  created_date: 2024-03-28
  url: https://qwenlm.github.io/blog/qwen-moe/
  model_card: https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B
  modality: text; text
  analysis: Base models are evaluated on MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP,
    BBH, CMMLU, all standard English and Chinese benchmarks, and chat models are
    evaluated on Chatbot Arena, AlpacaEval, MT-Bench, etc.
  size: 14B parameters with 2.7B parameters for activation (MoE)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: open
  license:
    explanation: Model license can be found at https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B/blob/main/LICENSE
    value: custom
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B/discussions

- type: model
  name: SeaLLM v2.5
  organization: DAMO Academy, Alibaba
  description: SeaLLM v2.5 is a multilingual large language model for Southeast
    Asian (SEA) languages.
  created_date: 2024-04-12
  url: https://github.com/DAMO-NLP-SG/SeaLLMs
  model_card: https://huggingface.co/SeaLLMs/SeaLLM-7B-v2.5
  modality: text; text
  analysis: The model was evaluated on 3 benchmarks (MMLU for English, M3Exam (M3e)
    for English, Chinese, Vietnamese, Indonesian, and Thai, and VMLU for Vietnamese)
    and it outperformed GPT-3 and Vistral-7B-chat models across these benchmarks
    in the given languages.
  size: 7B parameters
  dependencies: [Gemma]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: Despite efforts in red teaming and safety fine-tuning and enforcement,
    the creators suggest, developers and stakeholders should perform their own red
    teaming and provide related security measures before deployment, and they must
    abide by and comply with local governance and regulations.
  access: open
  license:
    explanation: License can be found at https://huggingface.co/SeaLLMs/SeaLLM-13B-Chat/blob/main/LICENSE
    value: custom
  intended_uses: The model is intended for multilingual tasks such as knowledge
    retrieval, math reasoning, and instruction following. Also, it could be used
    to provide multilingual assistance.
  prohibited_uses: The model should not be used in a way that could lead to inaccurate,
    misleading or potentially harmful generation. Users should comply with local
    laws and regulations when deploying the model.
  monitoring: unknown
  feedback: https://huggingface.co/SeaLLMs/SeaLLM-7B-v2.5/discussions
- type: model
  name: SenseVoice
  organization: Alibaba
  description: SenseVoice is a speech foundation model with multiple speech understanding capabilities, such as automatic speech recognition (ASR), spoken language identification (LID), speech emotion recognition (SER), and audio event detection (AED). The model demonstrates high-accuracy multilingual speech recognition, speech emotion recognition, and audio event detection capabilities. It supports over 50 languages and was trained with more than 400,000 hours of data. It is efficient in inference and offers convenient finetuning scripts and strategies, as well as service deployment pipeline.
  created_date: unknown
  url: https://huggingface.co/FunAudioLLM/SenseVoiceSmall
  model card: https://huggingface.co/FunAudioLLM/SenseVoiceSmall
  modality: audio; text
  analysis: The model has been evaluated for multilingual speech recognition, speech emotion recognition, and audio event detection. The evaluations compared SenseVoice with other models like the Whisper model and found that it surpasses these in nearly all tasks.
  size: unknown
  dependencies: Based on the code and the description provided, it appears that the model may depend on the [Whisper model], although this is not clearly stated. It could potentially also depend on several other models or libraries, such as the [AutoModel] and [funasr] that are mentioned in the provided Python scripts.
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: Not explicitly stated, but given the evaluations mentioned and the performance of the model on multiple benchmark datasets, it seems that significant measures were taken to ensure its quality and accuracy.
  access: Open
  license: unknown but it's part of a cloned repo from GitHub which suggests that usage may be subject to GitHub's Terms and Conditions.
  intended_uses: The model can be used for tasks such as automatic speech recognition, spoken language identification, speech emotion recognition, and audio event detection. It could be useful in a variety of fields, including translation services, virtual assistants, voice-activated systems, and emotion-detection services, among others.
  prohibited_uses: Not stated
  monitoring: There is no specific mention of monitoring measures being taken for downstream uses of the model.
  feedback: Issues could be reported through the GitHub project page.
