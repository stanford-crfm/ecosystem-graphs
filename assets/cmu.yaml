---

- type: model
  name: PolyCoder
  organization: CMU
  description: PolyCoder is a code model trained on 2.7B parameters based on the
    GPT-2 architecture, which was trained on 249GB of code across 12 programming
    languages on a single machine.
  created_date:
    value: 2022-02-26
    explanation: The date the model paper was released
  url: https://arxiv.org/abs/2202.13169
  model_card: https://huggingface.co/NinedayWang/PolyCoder-2.7B
  modality: Code (12 programming languages)
  analysis: Reports results on standard code benchmarks across a variety of programming
    languyages.
  size: 2.7B parameters (dense)
  dependencies: [Github]
  training_emissions: Unknown
  training_time: 6 weeks
  training_hardware: 8 NVIDIA RTX 8000
  quality_control: No specific quality control is mentioned in model training, though
    details on data processing and how the tokenizer was trained are provided in
    the paper.
  access:
    value: Full public access
    explanation: Model checkpoints are available for download at https://github.com/VHellendoorn/Code-LMs
  license:
    value: MIT
    explanation: The license is provided in the [[Github repository]](https://github.com/VHellendoorn/Code-LMs)
  intended_uses: Unknown
  prohibited_uses: None
  monitoring: None
  feedback: https://huggingface.co/NinedayWang/PolyCoder-2.7B/discussion
