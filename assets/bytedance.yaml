---
- type: model
  name: SALMONN
  organization: ByteDance, Tsinghua University
  description: SALMONN is a large language model (LLM) enabling speech, audio event,
    and music inputs.
  created_date: 2023-10-20
  url: https://github.com/bytedance/SALMONN
  model_card: https://huggingface.co/MSIIP/SALMONN
  modality: audio, text; text
  analysis: Evaluated on benchmarks pertaining to speech, music, and other audio
    recognition.
  size: unknown
  dependencies: [Whisper, BEATs, Vicuna]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/MSIIP/SALMONN/discussions
- type: model
  name: SDXL-Lightning
  organization: ByteDance
  description: SDXL-Lightning is a lightning-fast text-to-image generation model.
    It can generate high-quality 1024px images in a few steps. The models are distilled
    from stabilityai/stable-diffusion-xl-base-1.0. This repository contains checkpoints
    for 1-step, 2-step, 4-step, and 8-step distilled models.
  created_date: 2024-02-21
  url: https://arxiv.org/pdf/2402.13929.pdf
  model_card: https://huggingface.co/ByteDance/SDXL-Lightning
  modality: text; image
  analysis: Evaluated via qualitative comparison relative to other SoTA image generation
    models.
  size: unknown
  dependencies: [Stable Diffusion XL]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 64 A100 80G GPUs
  quality_control: unknown
  access: open
  license: OpenRail++
  intended_uses: The model can be used for fast, high-quality text-to-image generation.
    It supports 1-step, 2-step, 4-step, and 8-step distilled models which provide
    varying generation quality.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: https://huggingface.co/ByteDance/SDXL-Lightning/discussions
- type: model
  name: LLaVA-Critic
  organization: ByteDance, University of Maryland, College Park
  description: LLaVA-Critic is an open-source large multimodal model (LMM), developed as a generalist evaluator to assess performance across a variety of multimodal tasks. It is designed to provide evaluation scores that are comparable to or exceed those of GPT models and to provide reward signals for preference learning, thereby enhancing model alignment capabilities. It builds on a high-quality dataset for critic instruction-following, enabling it to provide quantitative judgment and reasoning for its evaluations.
  created_date: 2024-10-06
  url: https://arxiv.org/pdf/2410.02712
  model_card: unknown
  modality: text, image; text, evaluation scores (judgement)
  analysis: The model's effectiveness was demonstrated in providing evaluation scores reliably, showing high correlation with commercial GPT models and outperforming other models in preference learning by offering enhanced AI-generated feedback.
  size: unknown
  dependencies: [GPT-4V, LLaVA-Instruction-150k, SVIT, ComVint, LLaVAR, LRV-Instruction, M3IT, LLaVA-Med, PCA-EVAL, VLFeedback]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: The model uses a well-curated critic instruction-following dataset, provides transparency and consistency with its evaluations, and ensures clarity and comprehensiveness in the evaluation process.
  access: open
  license: unknown
  intended_uses: Designed to serve as a reliable evaluator in multimodal contexts, useful for conducting model evaluations, generating reward signals for preference learning, and enhancing alignment in large multimodal models.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown

