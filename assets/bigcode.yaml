---
- type: model
  name: StarCoder
  organization: BigCode
  description: StarCoder is a Large Language Model for Code (Code LLM) trained on
    permissively licensed data from GitHub, including from 80+ programming languages,
    Git commits, GitHub issues, and Jupyter notebooks.
  created_date: 2023-05-09
  url: https://arxiv.org/pdf/2305.06161.pdf
  model_card: https://huggingface.co/bigcode/starcoder
  modality: code; code
  analysis: Tested on several benchmarks, most notably Python benchmark HumanEval.
  size: 15.5B parameters (dense)
  dependencies: [The Stack]
  training_emissions: 16.68 tons of CO2eq
  training_time: 2 days
  training_hardware: 64 NVIDIA A100 GPUs
  quality_control: No specific quality control is mentioned in model training, though
    details on data processing and how the tokenizer was trained are provided in
    the paper.
  access: open
  license: Apache 2.0
  intended_uses: With a Tech Assistant prompt and not as an instruction model given
    training limitations.
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/bigcode/starcoder/discussions
- type: model
  name: SantaCoder
  organization: BigCode
  description: Multilingual code model derived from findings of BigCode Project
    analysis on Github stars' association to data quality.
  created_date: 2023-02-24
  url: https://arxiv.org/pdf/2301.03988.pdf
  model_card: ''
  modality: code; code
  analysis: Evaluated on MultiPL-E system benchmarks.
  size: 1.1B parameters (dense)
  dependencies: [The Stack, BigCode Dataset]
  training_emissions: ''
  training_time: 3.1 days
  training_hardware: 96 NVIDIA Tesla V100 GPUs
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: The Stack
  organization: BigCode
  description: The Stack is a 3.1 TB dataset consisting of permissively licensed
    source code inteded for use in creating code LLMs.
  created_date: 2022-11-20
  url: https://arxiv.org/pdf/2211.15533.pdf
  datasheet: https://huggingface.co/datasets/bigcode/the-stack
  modality: code
  size: 3.1 TB
  sample: []
  analysis: Evaluated models trained on The Stack on HumanEval and MBPP and compared
    against similarly-sized models.
  dependencies: [GitHub]
  included: ''
  excluded: ''
  quality_control: allowed users whose data were part of The Stack's training data
    to opt-out
  access: open
  license: Apache 2.0
  intended_uses: creating code LLMs
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: StarCoder2
  organization: BigCode
  description: A 15 billion parameter language model trained on 600+ programming
    languages from The Stack v2. The training was carried out using the Fill-in-the-Middle
    objective on 4+ trillion tokens.
  created_date: 2024-02-28
  url: https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html
  model_card: https://huggingface.co/bigcode/starcoder2-15b
  modality: text; text
  analysis: unknown
  size: 15B parameters (dense)
  dependencies: [The Stack v2]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 1024 x H100 GPUs
  quality_control: The model was filtered for permissive licenses and code with
    no license only. A search index is provided to identify where generated code
    came from to apply the proper attribution.
  access: open
  license: BigCode OpenRail-M
  intended_uses: Intended to generate code snippets from given context, but not
    for writing actual functional code directly.
  prohibited_uses: Should not be used as a way to write fully functioning code without
    modification or verification.
  monitoring: unknown
  feedback: https://huggingface.co/bigcode/starcoder2-15b/discussions
