---
- type: model
  name: StarCoder
  organization: BigCode
  description: StarCoder is a Large Language Model for Code (Code LLM) trained on
    permissively licensed data from GitHub, including from 80+ programming languages,
    Git commits, GitHub issues, and Jupyter notebooks.
  created_date: 2023-05-09
  url: https://arxiv.org/pdf/2305.06161.pdf
  model_card: https://huggingface.co/bigcode/starcoder
  modality: code; code
  analysis: Tested on several benchmarks, most notably Python benchmark HumanEval.
  size: 15.5B parameters (dense)
  dependencies: [The Stack]
  training_emissions: 16.68 tons of CO2eq
  training_time: 320,256 GPU hours
  training_hardware: 512 A100 80GB GPUs distributed across 64 nodes
  quality_control: No specific quality control is mentioned in model training, though
    details on data processing and how the tokenizer was trained are provided in
    the paper.
  access: open
  license: BigCode Open RAIL-M v1.0
  intended_uses: As a foundation model to fine-tune and create more specialized
    models that support use cases such as code completion, fill-in-the-middle, and
    text summarization. Can also be used as a Tech Assistant prompt and not as an
    instruction model given training limitations.
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  monitoring: ''
  feedback: https://huggingface.co/bigcode/starcoder/discussions
- type: model
  name: SantaCoder
  organization: BigCode
  description: Multilingual code model derived from the findings of BigCode Project
    analysis on Github stars' association to data quality.
  created_date: 2023-02-24
  url: https://arxiv.org/pdf/2301.03988.pdf
  model_card: https://huggingface.co/bigcode/santacoder
  modality: code; code
  analysis: Evaluated on MultiPL-E system benchmarks.
  size: 1.1B parameters (dense)
  dependencies: [The Stack, BigCode Dataset]
  training_emissions: 124 kg of CO2eq
  training_time: 14,284 GPU hours
  training_hardware: 96 NVIDIA Tesla V100 GPUs
  quality_control: ''
  access: open
  license: BigCode Open RAIL-M v1
  intended_uses: The model was trained on GitHub code. As such it is not an instruction
    model and commands do not work well. You should phrase commands like they occur
    in source code such as comments or write a function signature and docstring
    and let the model complete the function body.
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  monitoring: ''
  feedback: https://huggingface.co/bigcode/santacoder/discussions
- type: dataset
  name: The Stack
  organization: BigCode
  description: The Stack contains over 6TB of permissively-licensed source code
    files covering 358 programming languages. The Stack serves as a pre-training
    dataset for Code LLMs, i.e., code-generating AI systems which enable the synthesis
    of programs from natural language descriptions as well as other from code snippets.
  created_date: 2022-11-20
  url: https://arxiv.org/pdf/2211.15533.pdf
  datasheet: https://huggingface.co/datasets/bigcode/the-stack
  modality: code
  size: 6 TB
  sample:
    - https://huggingface.co/datasets/bigcode/the-stack/viewer/default/train
  analysis: Evaluated models trained on The Stack on HumanEval and MBPP and compared
    against similarly-sized models.
  dependencies: [GitHub]
  included: ''
  excluded: ''
  quality_control: allowed users whose data were part of The Stack's training data
    to opt-out
  access: open
  license: The Stack is a collection of source code from repositories with various
    licenses. Any use of all or part of the code gathered in The Stack must abide
    by the terms of the original licenses, including attribution clauses when relevant.
    Provenance information is provided for each data point.
  intended_uses: creating code LLMs
  prohibited_uses: See https://huggingface.co/datasets/bigcode/the-stack
  monitoring: ''
  feedback: https://huggingface.co/datasets/bigcode/the-stack/discussions
- type: model
  name: StarCoder2-15B
  organization: BigCode
  description: StarCoder2-15B model is a 15B parameter model trained on 600+ programming
    languages from The Stack v2, with opt-out requests excluded. The training was
    carried out using the Fill-in-the-Middle objective on 4+ trillion tokens.
  created_date: 2024-02-28
  url: https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html
  model_card: https://huggingface.co/bigcode/starcoder2-15b
  modality: code; text
  analysis: See https://arxiv.org/pdf/2402.19173.pdf
  size: 15B parameters (dense)
  dependencies: [The Stack v2]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 1024 x H100 GPUs
  quality_control: The model was filtered for permissive licenses and code with
    no license only. A search index is provided to identify where generated code
    came from to apply the proper attribution.
  access: open
  license: BigCode OpenRail-M
  intended_uses: The model was trained on GitHub code as well as additional selected
    data sources such as Arxiv and Wikipedia. As such it is not an instruction model
    and commands like "Write a function that computes the square root." do not work
    well. Intended to generate code snippets from given context, but not for writing
    actual functional code directly.
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  monitoring: unknown
  feedback: https://huggingface.co/bigcode/starcoder2-15b/discussions
- type: model
  name: StarCoder2-7B
  organization: BigCode
  description: StarCoder2-7B model is a 7B parameter model trained on 17 programming
    languages from The Stack v2, with opt-out requests excluded. The model uses
    Grouped Query Attention, a context window of 16,384 tokens with a sliding window
    attention of 4,096 tokens, and was trained using the Fill-in-the-Middle objective
    on 3.5+ trillion tokens.
  created_date: 2024-02-28
  url: https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html
  model_card: https://huggingface.co/bigcode/starcoder2-7b
  modality: code; text
  analysis: See https://arxiv.org/pdf/2402.19173.pdf
  size: 7B parameters (dense)
  dependencies: [The Stack v2]
  training_emissions: 29,622.83 kgCO2eq
  training_time: 145,152 hours (cumulative)
  training_hardware: 432 H100 GPUs
  quality_control: The model was filtered for permissive licenses and code with
    no license only. A search index is provided to identify where generated code
    came from to apply the proper attribution.
  access: open
  license: BigCode OpenRail-M
  intended_uses: Intended to generate code snippets from given context, but not
    for writing actual functional code directly. The model has been trained on source
    code from 17 programming languages. The predominant language in source is English
    although other languages are also present. As such the model is capable of generating
    code snippets provided some context but the generated code is not guaranteed
    to work as intended. It can be inefficient and contain bugs or exploits. See
    the paper for an in-depth discussion of the model limitations.
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  monitoring: unknown
  feedback: https://huggingface.co/bigcode/starcoder2-7b/discussions
- type: model
  name: StarCoder2-3B
  organization: BigCode
  description: StarCoder2-3B model is a 3B parameter model trained on 17 programming
    languages from The Stack v2, with opt-out requests excluded. The model uses
    Grouped Query Attention, a context window of 16,384 tokens with a sliding window
    attention of 4,096 tokens, and was trained using the Fill-in-the-Middle objective
    on 3+ trillion tokens.
  created_date: 2024-02-28
  url: https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html
  model_card: https://huggingface.co/bigcode/starcoder2-3b
  modality: code; text
  analysis: See https://arxiv.org/pdf/2402.19173.pdf
  size: 3B parameters (dense)
  dependencies: [The Stack v2]
  training_emissions: 16,107.01 kgCO2eq
  training_time: 97,120 hours (cumulative)
  training_hardware: 160 A100 GPUs
  quality_control: The model was filtered for permissive licenses and code with
    no license only. A search index is provided to identify where generated code
    came from to apply the proper attribution.
  access: open
  license: BigCode OpenRail-M
  intended_uses: Intended to generate code snippets from given context, but not
    for writing actual functional code directly. The model has been trained on source
    code from 17 programming languages. The predominant language in source is English
    although other languages are also present. As such the model is capable of generating
    code snippets provided some context but the generated code is not guaranteed
    to work as intended. It can be inefficient and contain bugs or exploits. See
    the paper for an in-depth discussion of the model limitations.
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  monitoring: unknown
  feedback: https://huggingface.co/bigcode/starcoder2-3b/discussions
- type: model
  name: Re-LAION-5B
  organization: LAION e.V.
  description: Re-LAION-5B is an updated version of the web-scale LAION-5B, a text-link to images pair dataset that has been thoroughly cleaned of known links to suspected child sexual abuse material (CSAM). It's designed to ensure safer use while preserving usability for research purposes. It is targeted for usage in fully reproducible research on language-vision learning.
  created_date: 2024-08-30
  url: https://laion.ai/blog/relaion-5b/
  model_card: 
  modality: Text; Images
  analysis: The revisions and fixes implemented in Re-LAION-5B were necessitated by a report from Stanford Internet Observatory which highlighted links to potential illegal content. The filtering and cleaning process was done in partnership with Internet Watch Foundation (IWF) and the Canadian Center for Child Protection (C3P).
  size: 5.5B text-link to images pairs.
  dependencies: [LAION-5B]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: Collaborations with the Internet Watch Foundation (IWF), the Canadian Center for Child Protection (C3P), and uses of link and image hashes provided by these partners ensured quality control, safety, and removed harmful content.
  access: Open
  license: Apache 2.0
  intended_uses: Intended for use in studies and researches relating to language-vision learning, and to serve as a reference dataset for pre-training open foundation models like openCLIP.
  prohibited_uses: It cannot be used for any purpose that contradicts the Apache 2.0 license agreement and ethical procedures, including illegal activities.
  monitoring: Continuous scrutiny by the broad community, in a common effort to make open datasets better and safer.
  feedback: The organization's contact mechanisms or public channels for discussing/improving the dataset can be used to report downstream problems with the model.
