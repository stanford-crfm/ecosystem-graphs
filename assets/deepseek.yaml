---
- type: model
  name: Deepseek
  organization: Deepseek AI
  description: Deepseek is a 67B parameter model with Grouped-Query Attention trained
    on 2 trillion tokens from scratch.
  created_date: 2023-11-28
  url: https://github.com/deepseek-ai/DeepSeek-LLM
  model_card: https://huggingface.co/deepseek-ai/deepseek-llm-67b-base
  modality: text; text
  analysis: Deepseek and baseline models (for comparison) evaluated on a series
    of representative benchmarks, both in English and Chinese.
  size: 67B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: Training dataset comprised of diverse data composition and pruned
    and deduplicated.
  access: open
  license:
    explanation: Model license can be found at https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL.
      Code license is under MIT
    value: custom
  intended_uses: ''
  prohibited_uses: none
  monitoring: unknown
  feedback: https://huggingface.co/deepseek-ai/deepseek-llm-67b-base/discussions
- type: model
  name: Deepseek Chat
  organization: Deepseek AI
  description: Deepseek Chat is a 67B parameter model initialized from Deepseek
    and fine-tuned on extra instruction data.
  created_date: 2023-11-29
  url: https://github.com/deepseek-ai/DeepSeek-LLM
  model_card: https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat
  modality: text; text
  analysis: Deepseek and baseline models (for comparison) evaluated on a series
    of representative benchmarks, both in English and Chinese.
  size: 67B parameters (dense)
  dependencies: [Deepseek]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: Training dataset comprised of diverse data composition and pruned
    and deduplicated.
  access: open
  license:
    explanation: Model license can be found at https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL.
      Code license is under MIT
    value: custom
  intended_uses: ''
  prohibited_uses: none
  monitoring: unknown
  feedback: https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat/discussions
- type: model
  name: Deepseek Coder
  organization: Deepseek AI
  description: Deepseek Coder is composed of a series of code language models, each
    trained from scratch on 2T tokens, with a composition of 87% code and 13% natural
    language in both English and Chinese.
  created_date: 2023-11-03
  url: https://github.com/deepseek-ai/DeepSeek-Coder
  model_card: https://huggingface.co/deepseek-ai/deepseek-coder-33b-base
  modality: text; code
  analysis: Evaluated on code generation, code completion, cross-file code completion,
    and program-based math reasoning across standard benchmarks.
  size: 33B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: 8 NVIDIA A100 GPUs and 8 NVIDIA H800 GPUs
  quality_control: ''
  access: open
  license:
    explanation: Model license can be found at https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/LICENSE-MODEL.
      Code license is under MIT
    value: custom
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknkown
  feedback: https://huggingface.co/deepseek-ai/deepseek-coder-33b-base/discussions
- type: model
  name: DeepSeek-V2-Chat-0628
  organization: DeepSeek-AI
  description: DeepSeek-V2-Chat-0628 is an improved version of DeepSeek-V2-Chat. It has significantly enhanced the user experience for immersive translation, RAG, and other tasks. The model has achieved notable rankings on the LMSYS Chatbot Arena Leaderboard, showcasing exceptional capabilities in coding tasks and strong performance on challenging prompts.
  created_date: 2024 (Exact date is not provided.)
  url: https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628
  model_card: https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628
  modality: Text; Text (Takes text input to generate text-based output.)
  analysis: Performed remarkably well in benchmarks where it substantially improved upon the performance of its previous version, DeepSeek-V2-Chat, in various evaluation metrics including HumanEval, MATH, BBH, IFEval, and Arena-Hard rankings.
  size: Unknown
  dependencies: ['DeepSeek-V2-Chat', 'Huggingface Transformers', 'vLLM']
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: 80GB*8 GPUs for local inference.
  quality_control: Benchmarking on different evaluation metrics like HumanEval, MATH, BBH, IFEval, Arena-Hard, and JSON Output.
  access: Open
  license: MIT
  intended_uses: Can be utilized for various tasks like chat applications, coding task assistance, challenging prompt responses, immersive translations, RAG, and more.
  prohibited_uses: Not stated explicitly.
  monitoring: Unknown
  feedback: Users can raise an issue or contact the DeepSeek-AI team at service@deepseek.com for reporting downstream problems.
