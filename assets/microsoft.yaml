---
- type: model
  name: VLMo
  organization: Microsoft
  description: VLMo is a model for text-to-image generation
  created_date:
    explanation: The date the model paper was released
    value: 2021-11-03
  url: https://arxiv.org/abs/2111.02358
  model_card: none
  modality: text; image
  analysis: ''
  size: 562M parameters (dense)
  dependencies:
    - Conceptual Captions
    - SBU Captions
    - COCO
    - Visual Genome
    - Wikipedia
    - BooksCorpus
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access:
    explanation: Microsoft does not provide VLMo to external researchers. One author
      commented that code would be pushed "soon" in [[November of 2021]](https://github.com/microsoft/unilm/issues/532),
      but the repository does not contain relevant changes.
    value: closed
  license: none
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: T-ULRv5
  organization: Microsoft
  description: T-ULRv5 is a language model trained with two unique training objectives
  created_date:
    explanation: Date model blog post was released
    value: 2022-09-28
  url: https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/
  model_card: ''
  modality: text; text
  analysis: ''
  size: 2.2B parameters (dense)
  dependencies: []
  training_emissions: ''
  training_time: Less than two weeks
  training_hardware: 256 A100
  quality_control: ''
  access:
    explanation: Manual approval through early access request form required.
    value: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Turing NLR-v5
  organization: Microsoft
  description: ''
  created_date:
    explanation: Date model blog post was released
    value: 2021-12-02
  url: https://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/?OCID=msr_blog_TNLRV5_tw
  model_card: ''
  modality: text; text
  analysis: ''
  size: 5B parameters (dense)
  dependencies: []
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access:
    explanation: Manual approval through early access request form required.
    value: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Megatron-Turing NLG
  organization: Microsoft, NVIDIA
  description: "Megatron-Turing NLG is a 530B parameter autoregressive language\
    \ model.\n"
  created_date:
    explanation: "The date of the Microsoft Research blog announcing MT-NLG [[Microsoft\
      \ Research Blog]](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/).\n"
    value: 2022-01-28
  url: https://arxiv.org/abs/2201.11990
  model_card: ''
  modality: text; text
  analysis: ''
  size: 530B parameters (dense)
  dependencies: [The Pile]
  training_emissions: ''
  training_time: ''
  training_hardware: 4480 A100s (560 x 8)
  quality_control: ''
  access:
    explanation: Megatron-Turing NLG can be accessed through the [[Turing Academic
      Program]](https://www.microsoft.com/en-us/research/collaboration/microsoft-turing-academic-program/)
    value: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: VALL-E
  organization: Microsoft
  description: Vall-E is a neural code model for text-to-speech synthesis
  created_date:
    explanation: The date the [[model paper]](https://arxiv.org/abs/2301.02111)
      was released
    value: 2023-01-05
  url: https://valle-demo.github.io/
  model_card: none
  modality: text; audio
  analysis: ''
  size: unknown
  dependencies: []
  training_emissions: ''
  training_time: ''
  training_hardware: 16 V100 32GB GPUs
  quality_control: ''
  access:
    explanation: Microsoft does not provide public access to VALL-E
    value: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: GitHub CoPilot
  organization: Microsoft
  description: "GitHub CoPilot is a coding pair programmer assisting programmers\
    \ as they write code.\n"
  created_date:
    explanation: "Date of the blog post introducing CoPilot [[GitHub Blog Post]]\
      \ (https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/).\n"
    value: 2021-06-29
  url: https://copilot.github.com/
  dependencies: [Codex]
  adaptation: unknown
  output_space: Code completions
  quality_control: "GitHub is working on a filter to detect and suppress code generations\
    \ that are verbatim from the training set [[GitHub Research Recitation]] (https://docs.github.com/en/github/copilot/research-recitation).\
    \ According to the FAQ, GitHub implemented a simple filter that blocks emails\
    \ in standard formats to protect personally identifiable data that may be present\
    \ in the training data [[GitHub CoPilot]](https://copilot.github.com/).\n"
  access:
    explanation: "The feature is available to developers in a restricted technical\
      \ preview [[GitHub CoPilot]](https://copilot.github.com/).\n"
    value: limited
  license: unknown
  terms_of_service: "https://docs.github.com/en/site-policy/github-terms/github-terms-of-service\n"
  intended_uses: "GitHub CoPilot is intended to be used as a coding assistant.\n"
  prohibited_uses: "Access to GPT-3 is governed by GitHub Acceptable Use Policies\
    \ and Terms of Service, both of which list a set of prohibited uses [[Use Policies]]\
    \ (https://docs.github.com/en/site-policy/acceptable-use-policies/github-acceptable-use-policies)\
    \ [[Terms of Service]] (https://docs.github.com/en/site-policy/github-terms/github-terms-of-service).\n"
  monitoring: "value: unknown explanation: >\n  There may be internal monitoring\
    \ mechanisms unknown to the public.\n"
  feedback: "Feedback can be provided in the CoPilot feedback project [[CoPilot\
    \ feedback]] (https://github.com/github/feedback/discussions/categories/copilot-feedback).\n"
  monthly_active_users: "GitHub Copilot reportedly has over 1 million sign-ups [[Tweet\
    \ Source]](https://twitter.com/sama/status/1539737789310259200?s=21&t=YPaYd0ZueJzrR6rLslUqzg).\n"
  user_distribution: unknown
  failures: unknown
- type: model
  name: BioGPT
  organization: Microsoft
  description: ''
  created_date: 2022-09-24
  url: https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9&login=true
  model_card: ''
  modality: text; text
  analysis: ''
  size: 1.5B parameters (dense)
  dependencies: [PubMed]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: Bing Search
  organization: Microsoft
  description: AI-powered Bing search engine and Edge browser, available in preview
    now at Bing.com, to deliver better search, more complete answers, a new chat
    experience and the ability to generate content. We think of these tools as an
    AI copilot for the web.
  created_date: 2023-02-07
  url: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/
  dependencies: [ChatGPT API]
  adaptation: unknown
  output_space: Search results
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Search engine
  prohibited_uses: ''
  monitoring: ''
  feedback: "Feedback can be submitted at [bing.com](bing.com).\n"
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: KOSMOS-1
  organization: Microsoft
  description: KOSMOS-1 is a multimodal language model that is capable of perceiving
    multimodal input, following instructions, and performing in-context learning
    for not only language tasks but also multimodal tasks.
  created_date: 2023-03-01
  url: https://arxiv.org/pdf/2302.14045.pdf
  model_card: ''
  modality: image, text; image, text
  analysis: ''
  size: 1.6B parameters (dense)
  dependencies:
    - The Pile
    - CommonCrawl
    - LAION-2B-en
    - LAION-400M
    - COYO-700M
    - Conceptual Captions
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Prometheus
  organization: Microsoft
  description: In the context of Bing, we have developed a proprietary way of working
    with the OpenAI model that allows us to best leverage its power. We call this
    collection of capabilities and techniques the Prometheus model. This combination
    gives you more relevant, timely and targeted results, with improved safety.
  created_date: 2023-02-07
  url: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/
  model_card: ''
  modality: unknown
  analysis: ''
  size: unknown
  dependencies: []
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Florence
  organization: Microsoft
  description: ''
  created_date: 2022-11-23
  url: https://arxiv.org/abs/2111.11432
  model_card: ''
  modality: text; image
  analysis: ''
  size: 900M parameters (dense)
  dependencies: [FLD-900M]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: FLD-900M
  organization: Microsoft
  description: ''
  created_date: 2022-11-23
  url: https://arxiv.org/abs/2111.11432
  datasheet: ''
  modality: image, text
  size: 900M image-text pairs
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: Azure Cognitive Services for Vision
  organization: Microsoft
  description: Cost-effective, production-ready computer vision services in Azure
    Cognitive Service for Vision. The improved Vision Services enables developers
    to create cutting-edge, market-ready, responsible computer vision applications
    across various industries.
  created_date: 2023-03-07
  url: https://azure.microsoft.com/en-us/blog/announcing-a-renaissance-in-computer-vision-ai-with-microsofts-florence-foundation-model/?utm_content=buffer16fa0&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
  dependencies: [Florence]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license in the Microsoft Terms of Use govern the license\
      \ rules for Azure services as outlined in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://azure.microsoft.com/en-us/support/legal/
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: VisualChatGPT
  organization: Microsoft
  description: ''
  created_date: 2023-03-08
  url: https://arxiv.org/pdf/2303.04671.pdf
  model_card: ''
  modality: text; image, text
  analysis: ''
  size: unknown
  dependencies: [OpenAI API]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: none
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: Microsoft 365 Copilot
  organization: Microsoft
  description: It combines the power of language models with your data in the Microsoft
    Graph and the Microsoft 365 apps to turn your words into the most powerful productivity
    tool on the planet.
  created_date: 2023-03-16
  url: https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
  dependencies: [GPT-4 API]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Business Chat
  organization: Microsoft
  description: Business Chat works across the langugae model, the Microsoft 365
    apps, and your data — your calendar, emails, chats, documents, meetings and
    contacts — to do things you’ve never been able to do before. You can give it
    natural language prompts like “Tell my team how we updated the product strategy,”
    and it will generate a status update based on the morning’s meetings, emails
    and chat threads.
  created_date: 2023-03-16
  url: https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Excel
  organization: Microsoft
  description: Microsoft Excel is the industry leading spreadsheet software program,
    a powerful data visualization and analysis tool.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/excel
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Outlook
  organization: Microsoft
  description: Microsoft Outlook is a personal information manager software system
    from Microsoft, available as a part of the Microsoft Office and Microsoft 365
    software suites.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/outlook/email-and-calendar-software-microsoft-outlook
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Power Platform
  organization: Microsoft
  description: Microsoft Power Platform is a line of business intelligence, app
    development, and app connectivity software applications.
  created_date: unknown
  url: https://powerplatform.microsoft.com/en-us/
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft PowerPoint
  organization: Microsoft
  description: Microsoft PowerPoint empowers you to create clean slideshow presentations
    and intricate pitch decks and gives you a powerful presentation maker.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/powerpoint
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Teams
  organization: Microsoft
  description: Microsoft Teams is a proprietary business communication platform
    developed by Microsoft, as part of the Microsoft 365 family of products.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-teams/group-chat-software
  dependencies: [Microsoft 365 Copilot, Microsoft Business Chat]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Word
  organization: Microsoft
  description: Microsoft Word is a word processing software developed by Microsoft
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/word
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Inside Look
  organization: Microsoft
  description: "Inside look is a Microsoft Office feature, composing document insights\
    \ highlighting key points, expected time to read, and popularity among others.\n"
  created_date: unknown
  url: https://support.microsoft.com/en-us/office/see-file-insights-before-you-open-a-file-87a23bbc-a516-42e2-a7b6-0ecb8259e026
  dependencies: []
  adaptation: unknown
  output_space: Document level insights for users.
  quality_control: unknown
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Providing document insights to users.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: application
  name: Microsoft Suggested Replies
  organization: Microsoft
  description: "Suggested replies is a Microsoft Outlook feature that suggests responses\
    \ to emails, available in: English, Spanish, Italian, French, German, Portuguese\
    \ Chinese Simplified, Chinese Traditional, Swedish, Russian, Korean, Czech,\
    \ Hungarian, Arabic, Hebrew, Thai, Turkish, Japanese, Dutch, Norwegian, Danish,\
    \ and Polish.\n"
  created_date: unknown
  url: https://support.microsoft.com/en-us/office/use-suggested-replies-in-outlook-19316194-0434-43ba-a742-6b5890157379
  dependencies: []
  adaptation: unknown
  output_space: Suggested emails.
  quality_control: unknown
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Suggesting email replies.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: application
  name: Microsoft Security Copilot
  organization: Microsoft
  description: "Microsoft Security Copilot is an AI-powered security analysis tool\
    \ that enables analysts to respond to threats quickly, process signals at machine\
    \ speed, and assess risk exposure in minutes.\n"
  created_date:
    explanation: The date Security Copilot was announced in the [[Microsoft blog
      post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: 2023-03-28
  url: https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/
  dependencies: [GPT-4, Microsoft security-specific model]
  adaptation:
    explanation: See [[blog post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: Security Copilot combines OpenAI's GPT-4 generative AI with a security-specific
      model from Microsoft. This security-specific model in turn incorporates a
      growing set of security-specific skills and is informed by Microsoft's unique
      global threat intelligence and more than 65 trillion daily signals.
  output_space:
    explanation: See [[product demo]](https://www.microsoft.com/en-us/security/business/ai-machine-learning/microsoft-security-copilot).
    value: Actionable responses to security-related questions (text and image).
      Security event, incident or threat reports (PowerPoint slide).
  quality_control:
    explanation: See [[blog post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: Security Copilot employs a closed-loop learning system that learns from
      user interactions and feedback, enabling it to provide more coherent, relevant,
      and useful answers that continually improve over time. Security Copilot is
      committed to delivering safe, secure, and responsible AI solutions, ensuring
      that customers' data and AI models are protected with enterprise compliance
      and security controls. Customer data is owned and controlled by them, and
      not used to train AI models for anyone outside their organization.
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Security Copilot is designed to enhance the capabilities of cybersecurity
    professionals. It leverages machine speed and scale to accelerate response to
    security incidents, discover and process threat signals, and assess risk exposure
    within minutes.
  prohibited_uses: unknown
  monitoring: ''
  feedback: unknown
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: model
  name: UniLM
  organization: Microsoft
  description: UniLM is a unified language model that can be fine-tuned for both
    natural language understanding and generation tasks.
  created_date: 2019-10-01
  url: https://proceedings.neurips.cc/paper_files/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf
  model_card: ''
  modality: text; text
  analysis: Evaluated on GLUE, SQuAD 2.0, and CoQA benchmarks.
  size: 340M parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: 10,000 steps in 7 hours
  training_hardware: 8 NVIDIA Tesla V100 32GB GPUs
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Docugami
  organization: Microsoft
  description: Docugami is a LLM focused on writing business documents and data
    using generative AI.
  created_date: 2021-04-12
  url: https://www.docugami.com/generative-ai
  model_card: ''
  modality: text; text
  analysis: ''
  size: 20B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: ''
  quality_control: ''
  access: limited
  license: ''
  intended_uses: analyzing, writing, and connecting business documents and data
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: BEiT-3
  organization: Microsoft
  description: BEiT-3 is a general-purpose multimodal foundation model for vision
    and vision-language tasks.
  created_date: 2022-08-31
  url: https://arxiv.org/pdf/2208.10442.pdf
  model_card: ''
  modality: image, text; image, text
  analysis: Evaluated on a range of standardized vision benchmarks, and achieves
    state of the art performance on all experimentally.
  size: 1.9B parameters (dense)
  dependencies: [Multiway Transformer network]
  training_emissions: unknown
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: WizardLM
  organization: Microsoft
  description: Starting with an initial set of instructions, we use our proposed
    Evol-Instruct to rewrite them step by step into more complex instructions. Then,
    we mix all generated instruction data to fine-tune LLaMA. We call the resulting
    model WizardLM.
  created_date: 2023-04-24
  url: https://arxiv.org/pdf/2304.12244v1.pdf
  model_card: https://huggingface.co/WizardLM/WizardLM-13B-1.0
  modality: text; text
  analysis: Reports results on standard LLM benchmarks in comparison to other LLMs
    and test sets.
  size: 7B parameters (dense)
  dependencies: [LLaMA, Evol-Instruct, Alpaca dataset]
  training_emissions: ''
  training_time: 70 hours on 3 epochs
  training_hardware: 8 V100 GPUs
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: Creating large amounts of instruction data, particularly with high
    complexity
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/datasets/WizardLM/evol_instruct_70k/discussions
- type: model
  name: WizardCoder
  organization: Microsoft
  description:
    explanation: WizardCoder refers to both a set of models fine-tuned on StarCoder
      and a set of models fined-tuned on Code LLaMA. This node refers to the set
      fine-tuned on StarCoder.
    value: WizardCoder empowers Code LLMs with complex instruction fine-tuning,
      by adapting the Evol-Instruct method to the domain of code.
  created_date: 2023-08-26
  url: https://arxiv.org/pdf/2306.08568.pdf
  model_card: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0
  modality:
    explanation: text; text
    value: text; text
  analysis: Evaluated on four prominent code generation benchmarks HumanEval, HumanEval+,
    MBPP, and DS100.
  size: 34B parameters (dense)
  dependencies: [Evol-Instruct, Alpaca dataset, StarCoder]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license:
    explanation: Model license can be found at https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/MODEL_WEIGHTS_LICENSE.
      Code license is under Apache 2.0
    value: BigCode Open Rail-M
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions
- type: model
  name: Florence-2
  organization: Microsoft
  description: WizardCoder empowers Code LLMs with complex instruction fine-tuning,
    by adapting the Evol-Instruct method to the domain of code.
  created_date: 2023-11-10
  url: https://arxiv.org/pdf/2311.06242.pdf
  model_card: none
  modality: image, text; text
  analysis: Evaluated on standard image processing benchmarks
  size: 771M parameters (dense)
  dependencies: [FLD-5B]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: dataset
  name: FLD-5B
  organization: Microsoft
  description: FLD-5B is the dataset that powers Florence-2
  created_date: 2023-11-10
  url: https://arxiv.org/pdf/2311.06242.pdf
  datasheet: ''
  modality: image, text
  size: 1.3B image-text annotations
  sample: []
  analysis: FLD-5B evaluated in comparison to datasets that power other large-scale
    image models on standard image benchmarks.
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: OpenOrca
  organization: Microsoft
  description: The OpenOrca dataset is a collection of augmented FLAN Collection
    data. Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is
    tabularized in alignment with the distributions presented in the ORCA paper
    and currently represents a partial completion of the full intended dataset,
    with ongoing generation to expand its scope.
  created_date: 2023-06-05
  url: https://huggingface.co/datasets/Open-Orca/OpenOrca
  datasheet: https://huggingface.co/datasets/Open-Orca/OpenOrca
  modality: text
  size: 4.5M text queries
  sample: []
  analysis: Models trained on OpenOrca compared to GPT-series on language benchmarks.
  dependencies: [GPT-3.5, GPT-4, Flan Collection]
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license: MIT
  intended_uses: training and evaluation in the field of natural language processing.
  prohibited_uses: none
  monitoring: ''
  feedback: none
- type: model
  name: LlongOrca
  organization: Microsoft
  description: LlongOrca is an attempt to make OpenOrca able to function in a Llong
    context.
  created_date: 2023-08-01
  url: https://huggingface.co/Open-Orca/LlongOrca-7B-16k
  model_card: https://huggingface.co/Open-Orca/LlongOrca-7B-16k
  modality: text; text
  analysis: LlongOrca evaluated on BigBench-Hard and AGIEval results.
  size: 7B parameters (dense)
  dependencies: [OpenOrca, LLongMA-2]
  training_emissions: unknown
  training_time: 37 hours
  training_hardware: 8x A6000-48GB (first-gen) GPUs
  quality_control: ''
  access: open
  license: LLaMA 2
  intended_uses: training and evaluation in the field of natural language processing.
  prohibited_uses: none
  monitoring: ''
  feedback: https://huggingface.co/Open-Orca/LlongOrca-7B-16k/discussions
- type: model
  name: Phi-1.5
  organization: Microsoft
  description: Phi-1.5 is a large language transformer model.
  created_date: 2023-09-11
  url: https://arxiv.org/pdf/2309.05463.pdf
  model_card: https://huggingface.co/microsoft/phi-1_5
  modality: text; text
  analysis: Evaluated on common sense reasoning, language understanding, and multi-step
    reasoning compared to other SOTA language models.
  size: 1.3B parameters (dense)
  dependencies: [phi-1]
  training_emissions: unknown
  training_time: 8 days
  training_hardware: 32 A100-40G GPUs
  quality_control: generic web-crawl data is removed from dataset.
  access: open
  license:
    explanation: can be found via the license tab at top of https://huggingface.co/microsoft/phi-1_5
    value: MIT
  intended_uses: Phi-1.5 is best suited for answering prompts using the QA format,
    the chat format, and the code format.
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/microsoft/phi-1_5/discussions
- type: model
  name: Orca 2
  organization: Microsoft
  description: Orca 2 is a finetuned version of LLAMA-2 for research purposes.
  created_date: 2023-11-21
  url: https://arxiv.org/pdf/2311.11045.pdf
  model_card: https://huggingface.co/microsoft/Orca-2-13b
  modality: text; text
  analysis: Orca 2 has been evaluated on a large number of tasks ranging from reasoning
    to grounding and safety.
  size: 13B parameters (dense)
  dependencies: [LLaMA 2]
  training_emissions: unknown
  training_time: 80 hours
  training_hardware: 32 NVIDIA A100 80GB GPUs
  quality_control: ''
  access: open
  license:
    explanation: can be found at https://huggingface.co/microsoft/Orca-2-13b/blob/main/LICENSE
    value: custom
  intended_uses: Orca 2 is built for research purposes only. The main purpose is
    to allow the research community to assess its abilities and to provide a foundation
    for building better frontier models.
  prohibited_uses: Any purposes other than research.
  monitoring: unknown
  feedback: https://huggingface.co/microsoft/Orca-2-13b/discussions
- type: model
  name: Phi-3
  organization: Microsoft
  description: Phi-3 is a 14 billion-parameter, lightweight, state-of-the-art open
    model trained using the Phi-3 datasets.
  created_date: 2024-05-21
  url: https://arxiv.org/abs/2404.14219
  model_card: https://huggingface.co/microsoft/Phi-3-medium-128k-instruct
  modality: text; text
  analysis: The model has been evaluated against benchmarks that test common sense,
    language understanding, mathematics, coding, long-term context, and logical
    reasoning. The Phi-3 Medium-128K-Instruct demonstrated robust and state-of-the-art
    performance.
  size: 14B parameters
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: The model underwent post-training processes viz. supervised fine-tuning
    and direct preference optimization to increase its capability in following instructions
    and aligning to safety measures.
  access: open
  license: MIT
  intended_uses: The model's primary use cases are for commercial and research purposes
    that require capable reasoning in memory or compute constrained environments
    and latency-bound scenarios. It can also serve as a building block for generative
    AI-powered features.
  prohibited_uses: The model should not be used for high-risk scenarios without
    adequate evaluation and mitigation techniques for accuracy, safety, and fairness.
  monitoring: Issues like allocation, high-risk scenarios, misinformation, generation
    of harmful content and misuse should be monitored and addressed.
  feedback: https://huggingface.co/microsoft/Phi-3-medium-128k-instruct/discussions
- type: model
  name: Aurora
  organization: Microsoft
  description: Aurora is a large-scale foundation model of the atmosphere trained on over a million hours of diverse weather and climate data.
  created_date: 2024-05-28
  url: https://arxiv.org/pdf/2405.13063
  model_card: none
  modality: text; climate forecasts
  analysis: Evaluated by comparing climate predictions to actual happened events.
  size: 1.3B parameters
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: 32 A100 GPUs
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: model
  name: Prov-GigaPath
  organization: Microsoft
  description: Prov-GigaPath is a whole-slide pathology foundation model pretrained on 1.3 billion 256 × 256 pathology image tiles.
  created_date: 2024-05-22
  url: https://www.nature.com/articles/s41586-024-07441-w
  model_card: none
  modality: image; embeddings
  analysis: Evaluated on a digital pathology benchmark comprising 9 cancer subtyping tasks and 17 pathomics tasks, with Prov-GigaPath demonstrating SoTA performance in 25 out of 26 tasks.
  size: unknown
  dependencies: [GigaPath]
  training_emissions: unknown
  training_time: 2 days
  training_hardware: 4 80GB A100 GPUs
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: model
  name: Ostris VAE - KL-f8-d16
  organization: Ostris
  description: A 16 channel Variational AutoEncoder (VAE) model with 8x downsample. The model has been trained from scratch on a balance of images including photos, artistic images, texts, cartoons, and vector images. This VAE model is faster, uses less VRAM and has similar scoring abilities compared to larger parameter-sized models, yet its model size is considerably lower.
  created_date: Unknown
  url: https://huggingface.co/ostris/vae-kl-f8-d16
  model card: https://huggingface.co/ostris/vae-kl-f8-d16
  modality: Image; Image
  analysis: This model has been evaluated to have a PSNR of 31.166 (higher is better) and LPIPS of 0.0198 (lower is better). This performance is quite similar to real images while maintaining a lower parameter count.
  size: 57,266,643 parameters (dense)
  dependencies: Unknown
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The model has been compared with other VAEs using image generation metrics like PSNR and LPIPS, ensuring its superior performance.
  access: Open
  license: MIT
  intended_uses: The model is an open source, lighter version of a 16 channel VAE. After training, it can be utilized for transformative image applications, among other creative works. 
  prohibited_uses: The model should not be used for applications requiring compatibility with SD3 as it has an entirely different latent space.
  monitoring: Unknown
  feedback: The model creator can be followed and contacted on Twitter for updates and related queries about the model.
- type: model
  name: Florence-2
  organization: Microsoft
  description: Florence-2 is a vision foundation model that uses a prompt-based approach to perform a variety of vision and vision-language tasks. This includes tasks like captioning, object detection, and segmentation. Florence-2 leverages the FLD-5B data set, containing 5.4 billion annotations across 126 million images, to master multi-task learning. Various versions of the Florence-2 model have been trained, including a base and a large model, and versions of these models that were fine-tuned on a collection of downstream tasks.
  created_date: Unknown
  url: https://huggingface.co/microsoft/Florence-2-large
  model card: https://huggingface.co/microsoft/Florence-2-large
  modality: Text; Image
  analysis: Evaluations were done on the model, with tables in the given document showing comparison of Florence-2's performance in tasks like image captioning, object detection and Visual Question Answering (VQA) to other models of a similar category.
  size: Florence-2-base and Florence-2-base-ft have 0.23 billion parameters. Florence-2-large and Florence-2-large-ft have 0.77 billion parameters. The models are dense.
  dependencies: [FLD-5B]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: Unknown
  access: Open
  license: Unknown
  intended_uses: Intended for a range of vision and vision-language tasks. Can perform tasks like captioning, object detection, segmentation and more by simply changing the text prompt.
  prohibited_uses: Unknown
  monitoring: Unknown
  feedback: Unknown
- type: model
  name: Phi-3-Mini-4K-Instruct
  organization: Microsoft
  description: The Phi-3-Mini-4K-Instruct is a state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data. It's tuned for instruction following with safety measures and has a strong reasoning property. It's lightweight with a model size of 3.8 billion parameters and can support a context length (in tokens) of 4K and 128K.
  created_date: Unknown
  url: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
  model card: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct
  modality: Text; Text
  analysis: The model was evaluated against benchmarks testing common sense, language understanding, math, code, long context, and logical reasoning. The results indicate a robust performance and it outperformed similar models with less than 13 billion parameters.
  size: 3.8B parameters
  dependencies: [Phi-3 datasets, Microsoft's original instruction-tuned Phi-3-Mini]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The model underwent a post-training process, which incorporates both supervised fine-tuning and direct preference optimization for the instruction following and safety measures.
  access: Open
  license: Unknown
  intended_uses: The model is intended for broad commercial and research uses in English. It's suitable for general purpose AI systems and applications requiring memory/compute constrained environments, latency bound scenarios, and strong reasoning (especially math and logic).
  prohibited_uses: The models are not designed to be used for all downstream purposes and should not be used in high-risk scenarios without proper evaluation of accuracy, safety, and fairness considerations. Developers must also adhere to any laws or regulations relevant to their use case.
  monitoring: Unknown
  feedback: Feedback on the model is welcomed and can be given through the Phi-3 model family community. The developers appreciate all feedback and will continue to welcome it.
- type: model
  name: Phi-3-Mini-128K-Instruct
  organization: Microsoft
  description: The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets. It provides robust and state-of-the-art performance among models with fewer than 13 billion parameters, especially in areas that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning. The model underwent a post-training process that involved supervised fine-tuning and direct preference optimization.
  created_date: 2024-06-22
  url: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct
  model card: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct
  modality: Text
  analysis: The model demonstrated robust performance against benchmarks that test common sense, language understanding, mathematics, coding, long-term context, and logical reasoning. It shows improvements in instruction following, structure output, reasoning, long-context understanding in both internal and public benchmarks.
  size: 3.8B parameters (dense)
  dependencies: [Phi-3 datasets]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: Underwent a post-training process that involved supervised fine-tuning and direct preference optimization for enhancing its ability to follow instructions and adhere to safety measures.
  access: Open
  license: Unknown 
  intended_uses: Designed for commercial and research use, especially in memory/compute constrained environments, latency-bound scenarios, and applications requiring strong reasoning such as code, math, and logic. Can be used as a building block for generative AI-powered features.
  prohibited_uses: Not specifically designed or evaluated for all downstream purposes. Developers should consider the common limitations of language models and should evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.
  monitoring: Measures to monitor downstream uses of the model are not mentioned.
  feedback: Encourages feedback from the community, but no specific method is provided. Users are most likely expected to give feedback via the Phi-3 Portal, Phi-3 Microsoft Blog or Azure AI Studio.
