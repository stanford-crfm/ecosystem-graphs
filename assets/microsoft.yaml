---
- type: model
  name: VLMo
  organization: Microsoft
  description: VLMo is a model for text-to-image generation
  created_date:
    explanation: The date the model paper was released
    value: 2021-11-03
  url: https://arxiv.org/abs/2111.02358
  model_card: none
  modality: text; image
  analysis: ''
  size: 562M parameters (dense)
  dependencies:
    - Conceptual Captions
    - SBU Captions
    - COCO
    - Visual Genome
    - Wikipedia
    - BooksCorpus
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access:
    explanation: Microsoft does not provide VLMo to external researchers. One author
      commented that code would be pushed "soon" in [[November of 2021]](https://github.com/microsoft/unilm/issues/532),
      but the repository does not contain relevant changes.
    value: closed
  license: none
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: T-ULRv5
  organization: Microsoft
  description: T-ULRv5 is a language model trained with two unique training objectives
  created_date:
    explanation: Date model blog post was released
    value: 2022-09-28
  url: https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/
  model_card: ''
  modality: text; text
  analysis: ''
  size: 2.2B parameters (dense)
  dependencies: []
  training_emissions: ''
  training_time: Less than two weeks
  training_hardware: 256 A100
  quality_control: ''
  access:
    explanation: Manual approval through early access request form required.
    value: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Turing NLR-v5
  organization: Microsoft
  description: ''
  created_date:
    explanation: Date model blog post was released
    value: 2021-12-02
  url: https://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/?OCID=msr_blog_TNLRV5_tw
  model_card: ''
  modality: text; text
  analysis: ''
  size: 5B parameters (dense)
  dependencies: []
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access:
    explanation: Manual approval through early access request form required.
    value: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Megatron-Turing NLG
  organization: Microsoft, NVIDIA
  description: "Megatron-Turing NLG is a 530B parameter autoregressive language\
    \ model.\n"
  created_date:
    explanation: "The date of the Microsoft Research blog announcing MT-NLG [[Microsoft\
      \ Research Blog]](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/).\n"
    value: 2022-01-28
  url: https://arxiv.org/abs/2201.11990
  model_card: ''
  modality: text; text
  analysis: ''
  size: 530B parameters (dense)
  dependencies: [The Pile]
  training_emissions: ''
  training_time: ''
  training_hardware: 4480 A100s (560 x 8)
  quality_control: ''
  access:
    explanation: Megatron-Turing NLG can be accessed through the [[Turing Academic
      Program]](https://www.microsoft.com/en-us/research/collaboration/microsoft-turing-academic-program/)
    value: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: VALL-E
  organization: Microsoft
  description: Vall-E is a neural code model for text-to-speech synthesis
  created_date:
    explanation: The date the [[model paper]](https://arxiv.org/abs/2301.02111)
      was released
    value: 2023-01-05
  url: https://valle-demo.github.io/
  model_card: none
  modality: text; audio
  analysis: ''
  size: unknown
  dependencies: []
  training_emissions: ''
  training_time: ''
  training_hardware: 16 V100 32GB GPUs
  quality_control: ''
  access:
    explanation: Microsoft does not provide public access to VALL-E
    value: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: GitHub CoPilot
  organization: Microsoft
  description: "GitHub CoPilot is a coding pair programmer assisting programmers\
    \ as they write code.\n"
  created_date:
    explanation: "Date of the blog post introducing CoPilot [[GitHub Blog Post]]\
      \ (https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/).\n"
    value: 2021-06-29
  url: https://copilot.github.com/
  dependencies: [Codex]
  adaptation: unknown
  output_space: Code completions
  quality_control: "GitHub is working on a filter to detect and suppress code generations\
    \ that are verbatim from the training set [[GitHub Research Recitation]] (https://docs.github.com/en/github/copilot/research-recitation).\
    \ According to the FAQ, GitHub implemented a simple filter that blocks emails\
    \ in standard formats to protect personally identifiable data that may be present\
    \ in the training data [[GitHub CoPilot]](https://copilot.github.com/).\n"
  access:
    explanation: "The feature is available to developers in a restricted technical\
      \ preview [[GitHub CoPilot]](https://copilot.github.com/).\n"
    value: limited
  license: unknown
  terms_of_service: "https://docs.github.com/en/site-policy/github-terms/github-terms-of-service\n"
  intended_uses: "GitHub CoPilot is intended to be used as a coding assistant.\n"
  prohibited_uses: "Access to GPT-3 is governed by GitHub Acceptable Use Policies\
    \ and Terms of Service, both of which list a set of prohibited uses [[Use Policies]]\
    \ (https://docs.github.com/en/site-policy/acceptable-use-policies/github-acceptable-use-policies)\
    \ [[Terms of Service]] (https://docs.github.com/en/site-policy/github-terms/github-terms-of-service).\n"
  monitoring: "value: unknown explanation: >\n  There may be internal monitoring\
    \ mechanisms unknown to the public.\n"
  feedback: "Feedback can be provided in the CoPilot feedback project [[CoPilot\
    \ feedback]] (https://github.com/github/feedback/discussions/categories/copilot-feedback).\n"
  monthly_active_users: "GitHub Copilot reportedly has over 1 million sign-ups [[Tweet\
    \ Source]](https://twitter.com/sama/status/1539737789310259200?s=21&t=YPaYd0ZueJzrR6rLslUqzg).\n"
  user_distribution: unknown
  failures: unknown
- type: model
  name: BioGPT
  organization: Microsoft
  description: ''
  created_date: 2022-09-24
  url: https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9&login=true
  model_card: ''
  modality: text; text
  analysis: ''
  size: 1.5B parameters (dense)
  dependencies: [PubMed]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: Bing Search
  organization: Microsoft
  description: AI-powered Bing search engine and Edge browser, available in preview
    now at Bing.com, to deliver better search, more complete answers, a new chat
    experience and the ability to generate content. We think of these tools as an
    AI copilot for the web.
  created_date: 2023-02-07
  url: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/
  dependencies: [ChatGPT API]
  adaptation: unknown
  output_space: Search results
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Search engine
  prohibited_uses: ''
  monitoring: ''
  feedback: "Feedback can be submitted at [bing.com](bing.com).\n"
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: KOSMOS-1
  organization: Microsoft
  description: KOSMOS-1 is a multimodal language model that is capable of perceiving
    multimodal input, following instructions, and performing in-context learning
    for not only language tasks but also multimodal tasks.
  created_date: 2023-03-01
  url: https://arxiv.org/pdf/2302.14045.pdf
  model_card: ''
  modality: image, text; image, text
  analysis: ''
  size: 1.6B parameters (dense)
  dependencies:
    - The Pile
    - CommonCrawl
    - LAION-2B-en
    - LAION-400M
    - COYO-700M
    - Conceptual Captions
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Prometheus
  organization: Microsoft
  description: In the context of Bing, we have developed a proprietary way of working
    with the OpenAI model that allows us to best leverage its power. We call this
    collection of capabilities and techniques the Prometheus model. This combination
    gives you more relevant, timely and targeted results, with improved safety.
  created_date: 2023-02-07
  url: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/
  model_card: ''
  modality: unknown
  analysis: ''
  size: unknown
  dependencies: []
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Florence
  organization: Microsoft
  description: ''
  created_date: 2022-11-23
  url: https://arxiv.org/abs/2111.11432
  model_card: ''
  modality: text; image
  analysis: ''
  size: 900M parameters (dense)
  dependencies: [FLD-900M]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: FLD-900M
  organization: Microsoft
  description: ''
  created_date: 2022-11-23
  url: https://arxiv.org/abs/2111.11432
  datasheet: ''
  modality: image, text
  size: 900M image-text pairs
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: Azure Cognitive Services for Vision
  organization: Microsoft
  description: Cost-effective, production-ready computer vision services in Azure
    Cognitive Service for Vision. The improved Vision Services enables developers
    to create cutting-edge, market-ready, responsible computer vision applications
    across various industries.
  created_date: 2023-03-07
  url: https://azure.microsoft.com/en-us/blog/announcing-a-renaissance-in-computer-vision-ai-with-microsofts-florence-foundation-model/?utm_content=buffer16fa0&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
  dependencies: [Florence]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license in the Microsoft Terms of Use govern the license\
      \ rules for Azure services as outlined in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://azure.microsoft.com/en-us/support/legal/
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: VisualChatGPT
  organization: Microsoft
  description: ''
  created_date: 2023-03-08
  url: https://arxiv.org/pdf/2303.04671.pdf
  model_card: ''
  modality: text; image, text
  analysis: ''
  size: unknown
  dependencies: [OpenAI API]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: none
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: Microsoft 365 Copilot
  organization: Microsoft
  description: It combines the power of language models with your data in the Microsoft
    Graph and the Microsoft 365 apps to turn your words into the most powerful productivity
    tool on the planet.
  created_date: 2023-03-16
  url: https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
  dependencies: [GPT-4 API]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Business Chat
  organization: Microsoft
  description: Business Chat works across the langugae model, the Microsoft 365
    apps, and your data — your calendar, emails, chats, documents, meetings and
    contacts — to do things you’ve never been able to do before. You can give it
    natural language prompts like “Tell my team how we updated the product strategy,”
    and it will generate a status update based on the morning’s meetings, emails
    and chat threads.
  created_date: 2023-03-16
  url: https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Excel
  organization: Microsoft
  description: Microsoft Excel is the industry leading spreadsheet software program,
    a powerful data visualization and analysis tool.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/excel
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Outlook
  organization: Microsoft
  description: Microsoft Outlook is a personal information manager software system
    from Microsoft, available as a part of the Microsoft Office and Microsoft 365
    software suites.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/outlook/email-and-calendar-software-microsoft-outlook
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Power Platform
  organization: Microsoft
  description: Microsoft Power Platform is a line of business intelligence, app
    development, and app connectivity software applications.
  created_date: unknown
  url: https://powerplatform.microsoft.com/en-us/
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft PowerPoint
  organization: Microsoft
  description: Microsoft PowerPoint empowers you to create clean slideshow presentations
    and intricate pitch decks and gives you a powerful presentation maker.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/powerpoint
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Teams
  organization: Microsoft
  description: Microsoft Teams is a proprietary business communication platform
    developed by Microsoft, as part of the Microsoft 365 family of products.
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-teams/group-chat-software
  dependencies: [Microsoft 365 Copilot, Microsoft Business Chat]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Word
  organization: Microsoft
  description: Microsoft Word is a word processing software developed by Microsoft
  created_date: unknown
  url: https://www.microsoft.com/en-us/microsoft-365/word
  dependencies: [Microsoft 365 Copilot]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Microsoft Inside Look
  organization: Microsoft
  description: "Inside look is a Microsoft Office feature, composing document insights\
    \ highlighting key points, expected time to read, and popularity among others.\n"
  created_date: unknown
  url: https://support.microsoft.com/en-us/office/see-file-insights-before-you-open-a-file-87a23bbc-a516-42e2-a7b6-0ecb8259e026
  dependencies: []
  adaptation: unknown
  output_space: Document level insights for users.
  quality_control: unknown
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Providing document insights to users.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: application
  name: Microsoft Suggested Replies
  organization: Microsoft
  description: "Suggested replies is a Microsoft Outlook feature that suggests responses\
    \ to emails, available in: English, Spanish, Italian, French, German, Portuguese\
    \ Chinese Simplified, Chinese Traditional, Swedish, Russian, Korean, Czech,\
    \ Hungarian, Arabic, Hebrew, Thai, Turkish, Japanese, Dutch, Norwegian, Danish,\
    \ and Polish.\n"
  created_date: unknown
  url: https://support.microsoft.com/en-us/office/use-suggested-replies-in-outlook-19316194-0434-43ba-a742-6b5890157379
  dependencies: []
  adaptation: unknown
  output_space: Suggested emails.
  quality_control: unknown
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Suggesting email replies.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: application
  name: Microsoft Security Copilot
  organization: Microsoft
  description: "Microsoft Security Copilot is an AI-powered security analysis tool\
    \ that enables analysts to respond to threats quickly, process signals at machine\
    \ speed, and assess risk exposure in minutes.\n"
  created_date:
    explanation: The date Security Copilot was announced in the [[Microsoft blog
      post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: 2023-03-28
  url: https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/
  dependencies: [GPT-4, Microsoft security-specific model]
  adaptation:
    explanation: See [[blog post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: Security Copilot combines OpenAI's GPT-4 generative AI with a security-specific
      model from Microsoft. This security-specific model in turn incorporates a
      growing set of security-specific skills and is informed by Microsoft's unique
      global threat intelligence and more than 65 trillion daily signals.
  output_space:
    explanation: See [[product demo]](https://www.microsoft.com/en-us/security/business/ai-machine-learning/microsoft-security-copilot).
    value: Actionable responses to security-related questions (text and image).
      Security event, incident or threat reports (PowerPoint slide).
  quality_control:
    explanation: See [[blog post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: Security Copilot employs a closed-loop learning system that learns from
      user interactions and feedback, enabling it to provide more coherent, relevant,
      and useful answers that continually improve over time. Security Copilot is
      committed to delivering safe, secure, and responsible AI solutions, ensuring
      that customers' data and AI models are protected with enterprise compliance
      and security controls. Customer data is owned and controlled by them, and
      not used to train AI models for anyone outside their organization.
  access: limited
  license:
    explanation: "Software license as described in the Terms of Service document.\n"
    value: custom
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  intended_uses: Security Copilot is designed to enhance the capabilities of cybersecurity
    professionals. It leverages machine speed and scale to accelerate response to
    security incidents, discover and process threat signals, and assess risk exposure
    within minutes.
  prohibited_uses: unknown
  monitoring: ''
  feedback: unknown
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: model
  name: UniLM
  organization: Microsoft
  description: UniLM is a unified language model that can be fine-tuned for both
    natural language understanding and generation tasks.
  created_date: 2019-10-01
  url: https://proceedings.neurips.cc/paper_files/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf
  model_card: ''
  modality: text; text
  analysis: Evaluated on GLUE, SQuAD 2.0, and CoQA benchmarks.
  size: 340M parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: 10,000 steps in 7 hours
  training_hardware: 8 NVIDIA Tesla V100 32GB GPUs
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Docugami
  organization: Microsoft
  description: Docugami is a LLM focused on writing business documents and data
    using generative AI.
  created_date: 2021-04-12
  url: https://www.docugami.com/generative-ai
  model_card: ''
  modality: text; text
  analysis: ''
  size: 20B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: ''
  quality_control: ''
  access: limited
  license: ''
  intended_uses: analyzing, writing, and connecting business documents and data
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: BEiT-3
  organization: Microsoft
  description: BEiT-3 is a general-purpose multimodal foundation model for vision
    and vision-language tasks.
  created_date: 2022-08-31
  url: https://arxiv.org/pdf/2208.10442.pdf
  model_card: ''
  modality: image, text; image, text
  analysis: Evaluated on a range of standardized vision benchmarks, and achieves
    state of the art performance on all experimentally.
  size: 1.9B parameters (dense)
  dependencies: [Multiway Transformer network]
  training_emissions: unknown
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: WizardLM
  organization: Microsoft
  description: Starting with an initial set of instructions, we use our proposed
    Evol-Instruct to rewrite them step by step into more complex instructions. Then,
    we mix all generated instruction data to fine-tune LLaMA. We call the resulting
    model WizardLM.
  created_date: 2023-04-24
  url: https://arxiv.org/pdf/2304.12244v1.pdf
  model_card: https://huggingface.co/WizardLM/WizardLM-13B-1.0
  modality: text; text
  analysis: Reports results on standard LLM benchmarks in comparison to other LLMs
    and test sets.
  size: 7B parameters (dense)
  dependencies: [LLaMA, Evol-Instruct, Alpaca dataset]
  training_emissions: ''
  training_time: 70 hours on 3 epochs
  training_hardware: 8 V100 GPUs
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: Creating large amounts of instruction data, particularly with high
    complexity
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/datasets/WizardLM/evol_instruct_70k/discussions
- type: model
  name: WizardCoder
  organization: Microsoft
  description:
    explanation: WizardCoder refers to both a set of models fine-tuned on StarCoder
      and a set of models fined-tuned on Code LLaMA. This node refers to the set
      fine-tuned on StarCoder.
    value: WizardCoder empowers Code LLMs with complex instruction fine-tuning,
      by adapting the Evol-Instruct method to the domain of code.
  created_date: 2023-08-26
  url: https://arxiv.org/pdf/2306.08568.pdf
  model_card: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0
  modality:
    explanation: text; text
    value: text; text
  analysis: Evaluated on four prominent code generation benchmarks HumanEval, HumanEval+,
    MBPP, and DS100.
  size: 34B parameters (dense)
  dependencies: [Evol-Instruct, Alpaca dataset, StarCoder]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license:
    explanation: Model license can be found at https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/MODEL_WEIGHTS_LICENSE.
      Code license is under Apache 2.0
    value: BigCode Open Rail-M
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions
- type: model
  name: Florence-2
  organization: Microsoft
  description: WizardCoder empowers Code LLMs with complex instruction fine-tuning,
    by adapting the Evol-Instruct method to the domain of code.
  created_date: 2023-11-10
  url: https://arxiv.org/pdf/2311.06242.pdf
  model_card: none
  modality: image, text; text
  analysis: Evaluated on standard image processing benchmarks
  size: 771M parameters (dense)
  dependencies: [FLD-5B]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: dataset
  name: FLD-5B
  organization: Microsoft
  description: FLD-5B is the dataset that powers Florence-2
  created_date: 2023-11-10
  url: https://arxiv.org/pdf/2311.06242.pdf
  datasheet: ''
  modality: image, text
  size: 1.3B image-text annotations
  sample: []
  analysis: FLD-5B evaluated in comparison to datasets that power other large-scale
    image models on standard image benchmarks.
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: OpenOrca
  organization: Microsoft
  description: The OpenOrca dataset is a collection of augmented FLAN Collection
    data. Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is
    tabularized in alignment with the distributions presented in the ORCA paper
    and currently represents a partial completion of the full intended dataset,
    with ongoing generation to expand its scope.
  created_date: 2023-06-05
  url: https://huggingface.co/datasets/Open-Orca/OpenOrca
  datasheet: https://huggingface.co/datasets/Open-Orca/OpenOrca
  modality: text
  size: 4.5M text queries
  sample: []
  analysis: Models trained on OpenOrca compared to GPT-series on language benchmarks.
  dependencies: [GPT-3.5, GPT-4, Flan Collection]
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license: MIT
  intended_uses: training and evaluation in the field of natural language processing.
  prohibited_uses: none
  monitoring: ''
  feedback: none
- type: model
  name: LlongOrca
  organization: Microsoft
  description: LlongOrca is an attempt to make OpenOrca able to function in a Llong
    context.
  created_date: 2023-08-01
  url: https://huggingface.co/Open-Orca/LlongOrca-7B-16k
  model_card: https://huggingface.co/Open-Orca/LlongOrca-7B-16k
  modality: text; text
  analysis: LlongOrca evaluated on BigBench-Hard and AGIEval results.
  size: 7B parameters (dense)
  dependencies: [OpenOrca, LLongMA-2]
  training_emissions: unknown
  training_time: 37 hours
  training_hardware: 8x A6000-48GB (first-gen) GPUs
  quality_control: ''
  access: open
  license: LLaMA 2
  intended_uses: training and evaluation in the field of natural language processing.
  prohibited_uses: none
  monitoring: ''
  feedback: https://huggingface.co/Open-Orca/LlongOrca-7B-16k/discussions
- type: model
  name: Phi-1.5
  organization: Microsoft
  description: Phi-1.5 is a large language transformer model.
  created_date: 2023-09-11
  url: https://arxiv.org/pdf/2309.05463.pdf
  model_card: https://huggingface.co/microsoft/phi-1_5
  modality: text; text
  analysis: Evaluated on common sense reasoning, language understanding, and multi-step
    reasoning compared to other SOTA language models.
  size: 1.3B parameters (dense)
  dependencies: [phi-1]
  training_emissions: unknown
  training_time: 8 days
  training_hardware: 32 A100-40G GPUs
  quality_control: generic web-crawl data is removed from dataset.
  access: open
  license:
    explanation: can be found via the license tab at top of https://huggingface.co/microsoft/phi-1_5
    value: MIT
  intended_uses: Phi-1.5 is best suited for answering prompts using the QA format,
    the chat format, and the code format.
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/microsoft/phi-1_5/discussions
- type: model
  name: Orca 2
  organization: Microsoft
  description: Orca 2 is a finetuned version of LLAMA-2 for research purposes.
  created_date: 2023-11-21
  url: https://arxiv.org/pdf/2311.11045.pdf
  model_card: https://huggingface.co/microsoft/Orca-2-13b
  modality: text; text
  analysis: Orca 2 has been evaluated on a large number of tasks ranging from reasoning
    to grounding and safety.
  size: 13B parameters (dense)
  dependencies: [LLaMA 2]
  training_emissions: unknown
  training_time: 80 hours
  training_hardware: 32 NVIDIA A100 80GB GPUs
  quality_control: ''
  access: open
  license:
    explanation: can be found at https://huggingface.co/microsoft/Orca-2-13b/blob/main/LICENSE
    value: custom
  intended_uses: Orca 2 is built for research purposes only. The main purpose is
    to allow the research community to assess its abilities and to provide a foundation
    for building better frontier models.
  prohibited_uses: Any purposes other than research.
  monitoring: unknown
  feedback: https://huggingface.co/microsoft/Orca-2-13b/discussions
- type: model
  name: Phi-3
  organization: Microsoft
  description: Phi-3 is a 14 billion-parameter, lightweight, state-of-the-art open
    model trained using the Phi-3 datasets.
  created_date: 2024-05-21
  url: https://arxiv.org/abs/2404.14219
  model_card: https://huggingface.co/microsoft/Phi-3-medium-128k-instruct
  modality: text; text
  analysis: The model has been evaluated against benchmarks that test common sense,
    language understanding, mathematics, coding, long-term context, and logical
    reasoning. The Phi-3 Medium-128K-Instruct demonstrated robust and state-of-the-art
    performance.
  size: 14B parameters
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: The model underwent post-training processes viz. supervised fine-tuning
    and direct preference optimization to increase its capability in following instructions
    and aligning to safety measures.
  access: open
  license: MIT
  intended_uses: The model's primary use cases are for commercial and research purposes
    that require capable reasoning in memory or compute constrained environments
    and latency-bound scenarios. It can also serve as a building block for generative
    AI-powered features.
  prohibited_uses: The model should not be used for high-risk scenarios without
    adequate evaluation and mitigation techniques for accuracy, safety, and fairness.
  monitoring: Issues like allocation, high-risk scenarios, misinformation, generation
    of harmful content and misuse should be monitored and addressed.
  feedback: https://huggingface.co/microsoft/Phi-3-medium-128k-instruct/discussions
- type: model
  name: Aurora
  organization: Microsoft
  description: Aurora is a large-scale foundation model of the atmosphere trained
    on over a million hours of diverse weather and climate data.
  created_date: 2024-05-28
  url: https://arxiv.org/pdf/2405.13063
  model_card: none
  modality: text; climate forecasts
  analysis: Evaluated by comparing climate predictions to actual happened events.
  size: 1.3B parameters
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: 32 A100 GPUs
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: model
  name: Prov-GigaPath
  organization: Microsoft
  description: Prov-GigaPath is a whole-slide pathology foundation model pretrained
    on 1.3 billion 256 × 256 pathology image tiles.
  created_date: 2024-05-22
  url: https://www.nature.com/articles/s41586-024-07441-w
  model_card: none
  modality: image; embeddings
  analysis: Evaluated on a digital pathology benchmark comprising 9 cancer subtyping
    tasks and 17 pathomics tasks, with Prov-GigaPath demonstrating SoTA performance
    in 25 out of 26 tasks.
  size: unknown
  dependencies: [GigaPath]
  training_emissions: unknown
  training_time: 2 days
  training_hardware: 4 80GB A100 GPUs
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: model
  name: Phi-3.5-MoE
  organization: Microsoft
  description: Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon
    datasets used for Phi-3 - synthetic data and filtered publicly available documents,
    with a focus on very high-quality, reasoning dense data. It supports multilingual
    and has a 128K context length in tokens. The model underwent a rigorous enhancement
    process, incorporating supervised fine-tuning, proximal policy optimization,
    and direct preference optimization to ensure instruction adherence and robust
    safety measures.
  created_date: 2024-09-08
  url: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct
  model_card: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct
  modality: text; text
  analysis: The model was evaluated across a variety of public benchmarks, comparing
    with a set of models including Mistral-Nemo-12B-instruct-2407, Llama-3.1-8B-instruct,
    Gemma-2-9b-It, Gemini-1.5-Flash, and GPT-4o-mini-2024-07-18. It achieved a similar
    level of language understanding and math as much larger models. It also displayed
    superior performance in reasoning capability, even with only 6.6B active parameters.
    It was also evaluated for multilingual tasks.
  size: 61B parameters (sparse); 6.6B active parameters
  dependencies: [Phi-3 dataset]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The model was enhanced through supervised fine-tuning, proximal
    policy optimization, and direct preference optimization processes for safety
    measures.
  access: open
  license: MIT
  intended_uses: The model is intended for commercial and research use in multiple
    languages. It is designed to accelerate research on language and multimodal
    models, and for use as a building block for generative AI powered features.
    It is suitable for general purpose AI systems and applications which require
    memory/computed constrained environments, latency bound scenarios, and strong
    reasoning.
  prohibited_uses: The model should not be used for downstream purposes it was not
    specifically designed or evaluated for. Developers should evaluate and mitigate
    for accuracy, safety, and fariness before using within a specific downstream
    use case, particularly for high risk scenarios.
  monitoring: Unknown
  feedback: Unknown
- type: model
  name: Phi-4
  organization: Microsoft Research
  description: "phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets."
  created_date: 2024-12-12
  url: https://huggingface.co/microsoft/phi-4
  model_card: https://huggingface.co/microsoft/phi-4
  modality:
    explanation: "Inputs: Text,... Outputs: Generated text in response to input"
    value: text; text
  analysis: "We evaluated phi-4 using OpenAI’s SimpleEval and our own internal benchmarks to understand the model’s capabilities."
  size:
    explanation: "14B parameters, dense decoder-only Transformer model"
    value: 14B parameters
  dependencies: [Phi-3]
  training_emissions: unknown
  training_time:
    explanation: "Training time 21 days"
    value: 21 days
  training_hardware:
    explanation: "GPUs 1920 H100-80G"
    value: 1920 H100-80G
  quality_control: "adopted a robust safety post-training approach," and "collaborated with the independent AI Red Team (AIRT) at Microsoft to assess safety risks"
  access:
    explanation: "an open model"
    value: open
  license:
    explanation: "License MIT"
    value: MIT
  intended_uses: "designed to accelerate research on language models, for use as a building block for generative AI powered features."
  prohibited_uses: "not specifically designed or evaluated for all downstream purposes," and "should consider common limitations of language models"
  monitoring: "Developers should...build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information"
  feedback: Developers should "build feedback mechanisms and pipelines"

