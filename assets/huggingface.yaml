---
- type: model
  name: CodeParrot
  organization: HuggingFace
  description: CodeParrot is an autoregressive language model trained on code
  created_date:
    explanation: The date the model was announced
    value: 2021-12-06
  url: https://twitter.com/lvwerra/status/1467933794699259908
  model_card: none
  modality: text; code, text
  analysis: none
  size: 1B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: 16 x A100 (40GB)
  quality_control: none
  access: open
  license:
    explanation: No license is explicitly provided for this model.
    value: none
  intended_uses: none
  prohibited_uses: none
  monitoring: none
  feedback: none
- type: model
  name: Zephyr
  organization: HuggingFace
  description: Zephyr is a series of language models that are trained to act as
    helpful assistants.
  created_date: 2023-10-11
  url: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha
  model_card: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha
  modality: text; text
  analysis: Evaluated on loss, rewards, logps, and logits rejected and chosen.
  size: 7B parameters (dense)
  dependencies: [Mistral]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: none
  access: open
  license: MIT
  intended_uses: Educational and research purposes
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/discussions
- type: model
  name: IDEFICS
  organization: HuggingFace
  description: IDEFICS is an open-access visual language model, based on Flamingo.
  created_date: 2023-08-22
  url: https://huggingface.co/blog/idefics
  model_card: https://huggingface.co/HuggingFaceM4/idefics-80b-instruct
  modality: image, text; text
  analysis: Evaluated in comparison to Flamingo and OpenFlamingo on standard benchmarks.
  size: 80B parameters (dense)
  dependencies: [OBELICS, Wikipedia, LAION-5B, PMD]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: none
  access: open
  license:
    explanation: Can be found at https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#license
    value: custom
  intended_uses: Educational and research purposes
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/HuggingFaceM4/idefics-80b-instruct/discussions
- type: dataset
  name: OBELICS
  organization: HuggingFace
  description: OBELICS is a dataset consisting of 141 million interleaved image-text
    documents scraped from the web and contains 353 million images.
  created_date: 2023-08-22
  url: https://huggingface.co/blog/idefics
  datasheet: https://huggingface.co/datasets/HuggingFaceM4/OBELICS
  modality: image, text
  size: 115B tokens
  sample: []
  analysis: Subset of training dataset evaluated for bias using Data Measurements
    Tool.
  dependencies: []
  included: ''
  excluded: All images for which creators explicitly requested opt-out of AI training.
  quality_control: Sexual and violent content still present in OBELICS even after
    filtering.
  access: open
  license: CC-BY-4.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/datasets/HuggingFaceM4/OBELICS/discussions
- type: model
  name: FinGPT
  organization: University of Turku, HuggingFace, National Library of Finland
  description: FinGPT is a series of Finnish LLMs trained from scratch.
  created_date: 2023-11-03
  url: https://arxiv.org/pdf/2311.05640.pdf
  model_card: https://huggingface.co/TurkuNLP/gpt3-finnish-13B
  modality: text; text
  analysis: Evaluated on in-house benchmark, FIN-bench, adapted from BIG-bench for
    Finnish.
  size: 13B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a
    single 64-core AMD Trento CPU and 512GB of memory.
  quality_control: unknown
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/TurkuNLP/gpt3-finnish-13B/discussions
- type: model
  name: BLUUMI
  organization: University of Turku, HuggingFace, National Library of Finland
  description: BLUUMI is a multilingual fine-tuned version of BLOOM.
  created_date: 2023-11-03
  url: https://arxiv.org/pdf/2311.05640.pdf
  model_card: https://huggingface.co/TurkuNLP/bloom-finnish-176b
  modality: text; text
  analysis: Evaluated on in-house benchmark, FIN-bench, adapted from BIG-bench for
    Finnish.
  size: 176B parameters (dense)
  dependencies: [BLOOM]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a
    single 64-core AMD Trento CPU and 512GB of memory.
  quality_control: unknown
  access: open
  license:
    explanation: Model card indicates same as license for BLOOM.
    value: BigScience RAIL v1.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/TurkuNLP/bloom-finnish-176b/discussions
- type: dataset
  name: Cosmopedia v0.1
  organization: Hugging Face
  description: Cosmopedia is a dataset of synthetic textbooks, blogposts, stories,
    posts, and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. The dataset
    contains over 30 million files and 25 billion tokens, making it the largest
    open synthetic dataset to date. It covers a variety of topics, mapping worldwide
    knowledge from Web datasets like RefinedWeb and RedPajama, to generate synthetic
    content.
  created_date: 2024-02-22
  url: none
  datasheet: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia
  modality: text
  size: 25B tokens
  sample: []
  analysis: Some seed samples were used in different prompt styles and audiences.
    Less than 1% of files are duplicates after running MinHash deduplication. Contaminated
    samples were removed from each dataset split.
  dependencies: [Mixtral]
  included: ''
  excluded: unknown
  quality_control: Measures were taken to reduce redundancy and ensure diversity
    in generated content. A decontamination pipeline was implemented to avoid benchmark
    contamination.
  access: open
  license: unknown
  intended_uses: ''
  prohibited_uses: unknown
  monitoring: ''
  feedback: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia/discussions
- type: model
  name: Idefics2
  organization: Hugging Face
  description: Idefics2 is a general multimodal model that takes as input arbitrary
    sequences of text and images, generating text responses. It has the capability
    to describe visual content, answer questions about images, perform basic arithmetic
    operations, create stories grounded in multiple images, and extract information
    from documents.
  created_date: 2024-04-15
  url: https://huggingface.co/blog/idefics2
  model_card: https://huggingface.co/HuggingFaceM4/idefics2-8b
  modality: image, text; text
  analysis: The performance of Idefics2 has been evaluated on numerous benchmarks.
    It is top of its class size and competes with much larger models such as LLava-Next-34B
    and MM1-30B-chat.
  size: 8B parameters
  dependencies: [The Cauldron]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: The quality of the model has been ensured by training it on a
    mixture of openly available datasets and enhancing its OCR capabilities. Further
    improvements include manipulating images in their native resolutions and aspect
    ratios, better pre-trained backbones, and allowing for sub-image splitting.
  access: open
  license: Apache 2.0
  intended_uses: The model can be used for answering questions about images, describing
    visual content, creating stories grounded in multiple images, extracting information
    from documents, and performing basic arithmetic operations.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: https://huggingface.co/HuggingFaceM4/idefics2-8b/discussions
- type: dataset
  name: The Cauldron
  organization: Hugging Face
  description: The Cauldron is an open compilation of 50 manually-curated datasets
    formatted for multi-turn conversations.
  created_date: 2024-04-15
  url: https://huggingface.co/blog/idefics2
  datasheet: https://huggingface.co/datasets/HuggingFaceM4/the_cauldron
  modality: image, text
  size: 50 vision-language datasets
  sample: []
  analysis: none
  dependencies:
    explanation: These are the datasets with the most tokens included; the full
      list of all 50 datasets can be found at https://huggingface.co/datasets/HuggingFaceM4/the_cauldron
    value: [LNarratives, Rendered Text, WebSight, DaTikz]
  included: ''
  excluded: ''
  quality_control: unknown
  access: open
  license: CC BY 4.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/discussions
- type: model
  name: all-MiniLM-L6-v2
  organization: Hugging Face Community and Googleâ€™s Flax, JAX, and Cloud team
  description: A sentence-transformers model that maps sentences & paragraphs to a 384-dimensional dense vector space. It can be used for tasks like clustering or semantic search. The model was fine-tuned using a self-supervised contrastive learning objective, developed during a Hugging Face Community week on a large sentence level dataset.
  created_date: Unknown
  url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
  model card: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
  modality: Text; Vector
  analysis: Automated model evaluations can be found at the Sentence Embeddings Benchmark website: https://seb.sbert.net
  size: Unknown
  dependencies: Used the pretrained nreimers/MiniLM-L6-H384-uncased model and fine-tuning was done on the concatenation of multiple datasets totaling over one billion sentence pairs. Specific dataset references are provided but could not be compiled here due to space constraints.
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: 7 TPUs v3-8
  quality_control: Cross entropy loss was applied to simulate a contrastive objective during model fine-tuning. Details of all hyperparameters used during training are accessible in the train_script.py repository.
  access: Open
  license: Unknown
  intended_uses: The model is intended to encode sentences and short paragraphs. The resultant vector, capturing semantic information of the original input, can be used for information retrieval, clustering, or sentence similarity tasks.
  prohibited_uses: Not suitable for input text longer than 256 word pieces, as such inputs will be truncated.
  monitoring: Unknown
  feedback: Unknown
- type: model
  name: Qwen2-57B-A14B
  organization: Hugging Face
  description: Qwen2 is a series of large language models, with the Qwen2-57B-A14B being a language model based on the Transformer architecture. It includes various enhancements like SwiGLU activation, QKV attention bias, group query attention, and an improved tokenizer for multiple natural languages and codes. It is a Mixture of Experts model with 57 billion parameters, 14 billion of which are activated in each pass. The model shows impressive performance in many benchmarks targeting language understanding and generation, coding, mathematics, reasoning, multilingual capability, etc.
  created_date: 2024 (Exact month and date unknown)
  url: https://hf.co/Qwen/Qwen2-57B-A14B
  model card: https://hf.co/Qwen/Qwen2-57B-A14B
  modality: Text; Text
  analysis: The evaluation of the model mainly focuses on natural language understanding, general question answering, coding, mathematics, reasoning, and multilingual capabilities. The datasets for evaluation include English tasks, coding tasks, math tasks, Chinese tasks, and multilingual tasks. 
  size: 57B parameters (Mixture of Experts model with 14B activated parameters per pass)
  dependencies: It directly relies on the Transformer architecture, SwiGLU activation, and the improved tokenizer in the Hugging Face transformers (version >=4.40.0) dependency.
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: Unknown
  access: Access details not provided.
  license: Apache 2.0  
  intended_uses: The model series is intended to be used for tasks related to language understanding, general question answering, coding, mathematics, reasoning, multilingual capabilities, etc.
  prohibited_uses: The creators advise against using the base language models for text generation, suggesting post-training methods like SFT, RLHF, continued pretraining, etc., instead.
  monitoring: Unknown
  feedback: Problem reports are likely expected through the Hugging Face platform, although specific instructions are not provided.
