---
- type: model
  name: CodeParrot
  organization: HuggingFace
  description: CodeParrot is an autoregressive language model trained on code
  created_date:
    explanation: The date the model was announced
    value: 2021-12-06
  url: https://twitter.com/lvwerra/status/1467933794699259908
  model_card: none
  modality: text; code, text
  analysis: none
  size: 1B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: 16 x A100 (40GB)
  quality_control: none
  access: open
  license:
    explanation: No license is explicitly provided for this model.
    value: none
  intended_uses: none
  prohibited_uses: none
  monitoring: none
  feedback: none
- type: model
  name: Zephyr
  organization: HuggingFace
  description: Zephyr is a series of language models that are trained to act as
    helpful assistants.
  created_date: 2023-10-11
  url: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha
  model_card: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha
  modality: text; text
  analysis: Evaluated on loss, rewards, logps, and logits rejected and chosen.
  size: 7B parameters (dense)
  dependencies: [Mistral]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: none
  access: open
  license: MIT
  intended_uses: Educational and research purposes
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/discussions
- type: model
  name: IDEFICS
  organization: HuggingFace
  description: IDEFICS is an open-access visual language model, based on Flamingo.
  created_date: 2023-08-22
  url: https://huggingface.co/blog/idefics
  model_card: https://huggingface.co/HuggingFaceM4/idefics-80b-instruct
  modality: image, text; text
  analysis: Evaluated in comparison to Flamingo and OpenFlamingo on standard benchmarks.
  size: 80B parameters (dense)
  dependencies: [OBELICS, Wikipedia, LAION-5B, PMD]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: none
  access: open
  license:
    explanation: Can be found at https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#license
    value: custom
  intended_uses: Educational and research purposes
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/HuggingFaceM4/idefics-80b-instruct/discussions
- type: dataset
  name: OBELICS
  organization: HuggingFace
  description: OBELICS is a dataset consisting of 141 million interleaved image-text
    documents scraped from the web and contains 353 million images.
  created_date: 2023-08-22
  url: https://huggingface.co/blog/idefics
  datasheet: https://huggingface.co/datasets/HuggingFaceM4/OBELICS
  modality: image, text
  size: 115B tokens
  sample: []
  analysis: Subset of training dataset evaluated for bias using Data Measurements
    Tool.
  dependencies: []
  included: ''
  excluded: All images for which creators explicitly requested opt-out of AI training.
  quality_control: Sexual and violent content still present in OBELICS even after
    filtering.
  access: open
  license: CC-BY-4.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/datasets/HuggingFaceM4/OBELICS/discussions
- type: model
  name: FinGPT
  organization: University of Turku, HuggingFace, National Library of Finland
  description: FinGPT is a series of Finnish LLMs trained from scratch.
  created_date: 2023-11-03
  url: https://arxiv.org/pdf/2311.05640.pdf
  model_card: https://huggingface.co/TurkuNLP/gpt3-finnish-13B
  modality: text; text
  analysis: Evaluated on in-house benchmark, FIN-bench, adapted from BIG-bench for
    Finnish.
  size: 13B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a
    single 64-core AMD Trento CPU and 512GB of memory.
  quality_control: unknown
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/TurkuNLP/gpt3-finnish-13B/discussions
- type: model
  name: BLUUMI
  organization: University of Turku, HuggingFace, National Library of Finland
  description: BLUUMI is a multilingual fine-tuned version of BLOOM.
  created_date: 2023-11-03
  url: https://arxiv.org/pdf/2311.05640.pdf
  model_card: https://huggingface.co/TurkuNLP/bloom-finnish-176b
  modality: text; text
  analysis: Evaluated on in-house benchmark, FIN-bench, adapted from BIG-bench for
    Finnish.
  size: 176B parameters (dense)
  dependencies: [BLOOM]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a
    single 64-core AMD Trento CPU and 512GB of memory.
  quality_control: unknown
  access: open
  license:
    explanation: Model card indicates same as license for BLOOM.
    value: BigScience RAIL v1.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/TurkuNLP/bloom-finnish-176b/discussions
- type: dataset
  name: Cosmopedia v0.1
  organization: Hugging Face
  description: Cosmopedia is a dataset of synthetic textbooks, blogposts, stories, posts, and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. The dataset contains over 30 million files and 25 billion tokens, making it the largest open synthetic dataset to date. It covers a variety of topics, mapping worldwide knowledge from Web datasets like RefinedWeb and RedPajama, to generate synthetic content.
  created_date: 2024-02-22
  url: none
  datasheet: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia
  modality: text
  size: 25B tokens
  sample: []
  analysis: Some seed samples were used in different prompt styles and audiences. Less than 1% of files are duplicates after running MinHash deduplication. Contaminated samples were removed from each dataset split.
  dependencies: [Mixtral]
  included: ''
  excluded: unknown
  quality_control: Measures were taken to reduce redundancy and ensure diversity in generated content. A decontamination pipeline was implemented to avoid benchmark contamination.
  access: open
  license: unknown
  intended_uses: ''
  prohibited_uses: unknown
  monitoring: ''
  feedback: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia/discussions

