---
- type: model
  name: GOAT
  organization: National University of Singapore
  description: GOAT is a fine-tuned LLaMA model which uses the tokenization of numbers
    to significantly outperform benchmark standards on a range of arithmetic tasks.
  created_date: 2023-05-23
  url: https://arxiv.org/pdf/2305.14201.pdf
  model_card: none
  modality: text; text
  analysis: Performance assessed on BIG-bench arithmetic sub-task, and various elementary
    arithmetic tasks.
  size: 7B parameters (dense)
  dependencies: [LLaMA, GOAT dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 24 GB VRAM GPU
  quality_control: Number data is randomly generated from log space to reduce likelihood
    of redundancy and range of magnitudes.
  access: open
  license: Apache 2.0
  intended_uses: Integration into other instruction-tuned LLMs to further enhance
    arithmetic reasoning abilities in solving math word problems.
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: OpenMoE
  organization: National University of Singapore, University of Edinburgh, ETH Zurich
  description: OpenMoE is a series of fully open-sourced and reproducible decoder-only
    MoE LLMs.
  created_date: 2024-01-12
  url: https://github.com/XueFuzhao/OpenMoE
  model_card: https://huggingface.co/OrionZheng/openmoe-base
  modality: text; text
  analysis: Evaluated on relatively simple established benchmarks.
  size: 34B parameters (dense)
  dependencies: [RedPajama, The Stack]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/OrionZheng/openmoe-base/discussions
- type: model
  name: Whisper Medusa
  organization: Aiola
  description: Whisper Medusa is an advanced speech transcription and translation model that processes audio through encoding and decoding stages. This model builds on the Whisper by predicting multiple tokens per iteration, significantly improving speed with a small degradation in WER (Word Error Rate).
  created_date: Unknown
  url: https://huggingface.co/aiola/whisper-medusa-v1
  model_card: https://huggingface.co/aiola/whisper-medusa-v1
  modality: Auditory; Text
  analysis: The model was trained and evaluated on the LibriSpeech dataset, demonstrating speed improvements.
  size: Unknown
  dependencies: ["LibriSpeech"]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: Unknown
  access: Unknown
  license: Unknown
  intended_uses: It is intended to perform audio translation specifically optimized for the English language.
  prohibited_uses: As the Medusa model heads were optimized for English language audio, performance may not be optimal for other languages.
  monitoring: Unknown
  feedback: Unknown
