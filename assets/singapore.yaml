---
- type: model
  name: GOAT
  organization: National University of Singapore
  description: GOAT is a fine-tuned LLaMA model which uses the tokenization of numbers
    to significantly outperform benchmark standards on a range of arithmetic tasks.
  created_date: 2023-05-23
  url: https://arxiv.org/pdf/2305.14201.pdf
  model_card: none
  modality: text; text
  analysis: Performance assessed on BIG-bench arithmetic sub-task, and various elementary
    arithmetic tasks.
  size: 7B parameters (dense)
  dependencies: [LLaMA, GOAT dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 24 GB VRAM GPU
  quality_control: Number data is randomly generated from log space to reduce likelihood
    of redundancy and range of magnitudes.
  access: open
  license: Apache 2.0
  intended_uses: Integration into other instruction-tuned LLMs to further enhance
    arithmetic reasoning abilities in solving math word problems.
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: OpenMoE
  organization: National University of Singapore, University of Edinburgh, ETH Zurich
  description: OpenMoE is a series of fully open-sourced and reproducible decoder-only
    MoE LLMs.
  created_date: 2024-01-12
  url: https://github.com/XueFuzhao/OpenMoE
  model_card: https://huggingface.co/OrionZheng/openmoe-base
  modality: text; text
  analysis: Evaluated on relatively simple established benchmarks.
  size: 34B parameters (dense)
  dependencies: [RedPajama, The Stack]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/OrionZheng/openmoe-base/discussions
- type: model
  name: Whisper Medusa
  organization: Aiola
  description: Whisper is an advanced encoder-decoder model for speech transcription and translation. It processes audio through encoding and decoding stages. Various optimization strategies like Faster-Whisper and Speculative Decoding have been proposed to enhance performance. The Medusa model builds on Whisper by predicting multiple tokens per iteration, which significantly improves speed with a small degradation in WER (Word Error Rate).
  created_date: Unknown
  url: https://huggingface.co/aiola/whisper-medusa-v1
  model_card: https://huggingface.co/aiola/whisper-medusa-v1
  modality: Audio; Text
  analysis: The model was trained and evaluated on the LibriSpeech dataset which resulted in significant speed improvements.
  size: Unknown
  dependencies: [LibriSpeech dataset, Whisper ASR model]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The model's quality is controlled by evaluating its performance in terms of speed improvement and degradation in WER on the LibriSpeech dataset.
  access: Open
  license: Unknown
  intended_uses: The model is intended for use in audio transcription and translation. It is specifically optimized for English audio translation.
  prohibited_uses: Usage of non-English audio is discouraged as the model's Medusa heads were specifically optimized for English.
  monitoring: Unknown
  feedback: Feedback on model issues can likely be reported through the Whisper Medusa GitHub repository.
