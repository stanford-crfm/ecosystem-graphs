- type: dataset
  name: LAION-400M
  organization: LAION
  description: LAION-400M is a dataset with CLIP-filtered 400 million image-text pairs,
    their CLIP embeddings and kNN indices that allow efficient similarity search.
    This dataset is entirely openly, freely accessible.
  created_date:
    explanation: The date the [[blog post]](https://laion.ai/blog/laion-400-open-dataset/)
      was released.
    value: 2021-08-20
  url: https://laion.ai/blog/laion-400-open-dataset/
  datasheet: https://laion.ai/blog/laion-400-open-dataset/
  modality: {}
  size: 400M image-text pairs
  sample: []
  analysis: ''
  dependencies:
  - CLIP
  - CommonCrawl
  included: ''
  excluded:
    explanation: See [[Dataset and Methods]](https://arxiv.org/pdf/2111.02114.pdf#section.2)
    value: 'Authors apply the following filtering conditions on the WAT files downloaded
      from Common Crawl: "All samples with less than 5 character alt-text length or
      less than 5 KB image size are dropped. Duplicate removal is performed with bloom
      filter based on URL and alt-text. We use CLIP to compute embeddings of the image
      and alt-text. Then we compute the cosine similarity of both embeddings and drop
      all samples with cosine similarity below 0.3. This threshold was selected based
      on human inspections. We use the CLIP embeddings of images and texts to filter
      out illegal contents."

      '
  quality_control:
    explanation: See [[Dataset and Methods]](https://arxiv.org/pdf/2111.02114.pdf#section.2)
    value: The authors use  CLIP embeddings of images and texts to filter out illegal
      contents. They also use CLIP to tag image-text pairs as NSFW. They note that
      less than 1% of images were detected as NSFW, which can be filtered out by an
      user with NSFW tag.
  access:
    explanation: The dataset index is available from [[dataset blog post]](https://laion.ai/blog/laion-400-open-dataset/)
    value: open
  license:
    explanation: The license is listed on the [[dataset blog post]](https://laion.ai/blog/laion-400-open-dataset/)
    value: CC BY 4.0
  intended_uses: The authors recommend using the dataset "for research purposes" and
    warn that "this large-scale dataset is non-curated. It was built for research
    purposes to enable testing model training on larger scale for broad researcher
    and other interested communities, and is not meant for any real-world production
    or application."
  prohibited_uses: No uses are explicitly prohibited by the license. Users are warned
    from using LAION-400M for any real-world production or application.
  monitoring: ''
  feedback: ''
- type: dataset
  name: LAION-5B
  organization: LAION
  description: LAION is a dataset of 5 billion image-text pairs from the Internet
  created_date:
    explanation: The date the [[blog post]](https://laion.ai/blog/laion-5b/) was released.
      Note that the dataset was already compiled earlier, e.g. the Stable Diffusion
      model released earlier was trained on a subset of LAION-5B
    value: 2022-12-12
  url: https://laion.ai/blog/laion-5b/
  datasheet: https://laion.ai/blog/laion-5b/
  modality: image, text
  size: 5B image-text pairs
  sample: []
  analysis: ''
  dependencies:
  - CLIP
  - mCLIP
  - CommonCrawl
  included: ''
  excluded:
    explanation: See [[dataset blog post]](https://laion.ai/blog/laion-5b/)
    value: All samples with less than 5 characters alt-text length or less than 5
      KB image size are dropped. All images with the too big resolution, potentially
      DOS bombs, were dropped before attempting to process them. Duplicate removal
      is performed with a bloom filter based on URL. Future runs would include more
      variate deduplication rules, such as URL + language for the multilanguage dataset.
      We use CLIP respectively MCLIP to compute embeddings of the image and alt-text.
      Then we compute the cosine similarity of both embeddings and drop all samples
      with cosine similarity below 0.28 for the English language ( with CLIP B/32)
      and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected
      based on human inspection of the test results. We use the CLIP embeddings of
      images and texts to filter out to the possible extent the illegal content.
  quality_control: ''
  access:
    explanation: The dataset index is available from [[dataset blog post]](https://laion.ai/blog/laion-5b/)
    value: open
  license:
    explanation: The license is listed on the [[dataset blog post]](https://laion.ai/blog/laion-5b/)
    value: CC BY 4.0
  intended_uses: The authors recommend using the dataset "for research purposes" and
    "do not recommend using it for creating ready-to-go industrial products, as the
    basic research about general properties and safety of such large-scale models,
    which we would like to encourage with this release, is still in progress"
  prohibited_uses: No uses are explicitly prohibited by the license. Users are warned
    from using LAION-5B for non-research purposes.
  monitoring: ''
  feedback: ''
- type: dataset
  name: LAION-2B-en
  organization: LAION
  description: LAION-2B-en is a subset of the LAION-5B dataset and contains 2.3 billion
    English image-text pairs.
  created_date:
    explanation: The date the [[blog post]](https://laion.ai/blog/laion-5b/) was released.
      Note that the dataset was already compiled earlier, e.g. the Stable Diffusion
      model released earlier was trained on a subset of LAION-5B
    value: 2022-12-12
  url: https://arxiv.org/pdf/2210.08402.pdf
  datasheet: https://laion.ai/blog/laion-5b/
  modality: image, text
  size: 2.32B image-text pairs
  sample: []
  analysis: ''
  dependencies:
  - CLIP
  - LAION-5B
  included: ''
  excluded:
    explanation: See [[dataset blog post]](https://laion.ai/blog/laion-5b/)
    value: All samples with less than 5 characters alt-text length or less than 5
      KB image size are dropped. All images with the too big resolution, potentially
      DOS bombs, were dropped before attempting to process them. Duplicate removal
      is performed with a bloom filter based on URL. Future runs would include more
      variate deduplication rules, such as URL + language for the multilanguage dataset.
      We use CLIP respectively MCLIP to compute embeddings of the image and alt-text.
      Then we compute the cosine similarity of both embeddings and drop all samples
      with cosine similarity below 0.28 for the English language ( with CLIP B/32)
      and 0.26 for the multilingual dataset (MCLIP). These thresholds were selected
      based on human inspection of the test results. We use the CLIP embeddings of
      images and texts to filter out to the possible extent the illegal content.
  quality_control: ''
  access:
    explanation: The dataset index is available from [[dataset blog post]](https://laion.ai/blog/laion-5b/)
    value: open
  license:
    explanation: The license is listed on the [[dataset blog post]](https://laion.ai/blog/laion-5b/)
    value: CC BY 4.0
  intended_uses: The authors recommend using the dataset "for research purposes" and
    "do not recommend using it for creating ready-to-go industrial products, as the
    basic research about general properties and safety of such large-scale models,
    which we would like to encourage with this release, is still in progress"
  prohibited_uses: No uses are explicitly prohibited by the license. Users are warned
    from using LAION-2B-en for non-research purposes.
  monitoring: ''
  feedback: ''
- type: model
  name: OpenFlamingo
  organization: LAION
  description: An open-source reproduction of DeepMind's Flamingo model. At its core,
    OpenFlamingo is a framework that enables training and evaluation of large multimodal
    models (LMMs).
  created_date: 2023-03-28
  url: https://laion.ai/blog/open-flamingo/
  model_card: https://github.com/mlfoundations/open_flamingo/blob/main/MODEL_CARD.md
  modality:
    explanation: image, text; text
    value: image, text; image, text
  analysis: Evaluated on COCO captioning and VQAv2 vision-language tasks.
  size: 9B parameters (dense)
  dependencies:
  - LLaMA
  - CLIP
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: MIT
  intended_uses: academic research purposes
  prohibited_uses: commercial use
  monitoring: ''
  feedback: ''
