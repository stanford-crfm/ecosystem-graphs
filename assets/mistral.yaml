---
- type: model
  name: Mistral
  organization: Mistral AI
  description: Mistral is a compact language model.
  created_date: 2023-09-27
  url: https://mistral.ai/news/announcing-mistral-7b/
  model_card: https://huggingface.co/mistralai/Mistral-7B-v0.1
  modality: text; text
  analysis: Evaluated in comparison to LLaMA series models on standard language
    benchmarks.
  size: 7.3B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions
- type: model
  name: Mistral Large
  organization: Mistral AI
  description: Mistral Large is Mistral AIâ€™s new cutting-edge text generation model.
  created_date: 2024-02-26
  url: https://mistral.ai/news/mistral-large/
  model_card: none
  modality: text; text
  analysis: Evaluated on commonly used benchmarks in comparison to the current LLM
    leaders.
  size: unknown
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: application
  name: Le Chat
  organization: Mistral AI
  description: Le Chat is a first demonstration of what can be built with Mistral
    models and what can deployed in the business environment.
  created_date: 2024-02-26
  url: https://mistral.ai/news/le-chat-mistral/
  dependencies: [Mistral, Mistral Large]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license: unknown
  terms_of_service: https://mistral.ai/terms/#terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: model
  name: Codestral
  organization: Mistral AI
  description: Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. Mastering code and English, it can be used to design advanced AI applications for software developers. It is fluent in 80+ programming languages.
  created_date: 2024-05-29
  url: https://mistral.ai/news/codestral/
  model_card: none
  modality: text; code
  analysis: Performance of Codestral is evaluated in Python, SQL, and additional languages, C++, bash, Java, PHP, Typescript, and C#. Fill-in-the-middle performance is assessed using HumanEval pass@1 in Python, JavaScript, and Java.
  size: 22B parameters
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Mistral AI Non-Production License
  intended_uses: Helps developers write and interact with code, design advanced AI applications for software developers, integrated into LlamaIndex and LangChain for building applications, integrated in VSCode and JetBrains environments for code generation and interactive conversation.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: none
- type: model
  name: Biggie-SmoLlm-0.15B-Base
  organization: nisten
  description: Biggie-SmoLlm-0.15B-Base is a unique model developed through semi-automated continuous merging techniques to enhance coherence. The model potentially has application in tasks related to natural language understanding and generation, as implied by the testing prompt targeted at generating a coherent response on building cities on Mars using Aldrin-Cycler orbits.
  created_date: Unknown
  url: https://huggingface.co/nisten/Biggie-SmoLlm-0.15B-Base
  model_card: https://huggingface.co/nisten/Biggie-SmoLlm-0.15B-Base
  modality: text to text
  analysis: The model has shown coherence for the first 100 tokens even in default settings, implying a robust capacity for semantically meaningful text generation. However, the exact evaluations or benchmark tests that were performed are not specified.
  size: 0.15B parameters
  dependencies: [Unknown]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The quality of this model seems to be primarily controlled through continuous merging techniques. However, specific measures taken to ensure quality, safety, and mitigate harms are not discussed.
  access: Unknown
  license: Unknown
  intended_uses: This model is likely intended to aid in language-related tasks. However, specific intended uses have not been fully elaborated by the creator.
  prohibited_uses: No explicit prohibited uses have been mentioned by the creator. However, general responsible AI practices would advise against using the model for tasks which it is not trained for, or for generating harmful, misleading or inappropriate content.
  monitoring: Details on any monitoring mechanisms for downstream uses of this model are not mentioned in the available description.
  feedback: Unknown
  
