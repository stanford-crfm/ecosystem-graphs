---
- type: model
  name: Mistral
  organization: Mistral AI
  description: Mistral is a compact language model.
  created_date: 2023-09-27
  url: https://mistral.ai/news/announcing-mistral-7b/
  model_card: https://huggingface.co/mistralai/Mistral-7B-v0.1
  modality: text; text
  analysis: Evaluated in comparison to LLaMA series models on standard language
    benchmarks.
  size: 7.3B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: none
  feedback: https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions
- type: model
  name: Mistral Large
  organization: Mistral AI
  description: Mistral Large is Mistral AIâ€™s new cutting-edge text generation model.
  created_date: 2024-02-26
  url: https://mistral.ai/news/mistral-large/
  model_card: none
  modality: text; text
  analysis: Evaluated on commonly used benchmarks in comparison to the current LLM
    leaders.
  size: unknown
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: application
  name: Le Chat
  organization: Mistral AI
  description: Le Chat is a first demonstration of what can be built with Mistral
    models and what can deployed in the business environment.
  created_date: 2024-02-26
  url: https://mistral.ai/news/le-chat-mistral/
  dependencies: [Mistral, Mistral Large]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license: unknown
  terms_of_service: https://mistral.ai/terms/#terms-of-use
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: model
  name: Codestral
  organization: Mistral AI
  description: Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. Mastering code and English, it can be used to design advanced AI applications for software developers. It is fluent in 80+ programming languages.
  created_date: 2024-05-29
  url: https://mistral.ai/news/codestral/
  model_card: none
  modality: text; code
  analysis: Performance of Codestral is evaluated in Python, SQL, and additional languages, C++, bash, Java, PHP, Typescript, and C#. Fill-in-the-middle performance is assessed using HumanEval pass@1 in Python, JavaScript, and Java.
  size: 22B parameters
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Mistral AI Non-Production License
  intended_uses: Helps developers write and interact with code, design advanced AI applications for software developers, integrated into LlamaIndex and LangChain for building applications, integrated in VSCode and JetBrains environments for code generation and interactive conversation.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: none
- type: model
  name: Pixtral 12B
  organization: Mistral AI
  description: Pixtral 12B is a Mixture of Experts model with a 400M parameter vision encoder and a 12B parameter multimodal decoder. The model has strong performance on multimodal tasks and maintains state-of-the-art performance on text-only benchmarks. It supports variable image sizes and aspect ratios as well as multiple images in the long context window of 128k tokens.
  created_date: 2024-09-17 
  url: https://mistral.ai/news/pixtral-12b/
  model_card: 
  modality:
    explanation: "Natively multimodal, trained with interleaved image and text data" 
    value: Image; text
  analysis: Pixtral 12B was evaluated on a range of open and closed models using the same evaluation harness. It performed well on multimodal reasoning without compromising on text capabilities such as instruction following, coding, and math.
  size:
    explanation: "12B parameter multimodal decoder based on Mistral Nemo" 
    value: 12B parameters
  dependencies: [Mistral Nemo 12B]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The model was evaluated through the same evaluation harness as open and closed models. It performed well on a variety of tasks and outperformed many similar models.
  access: ''
  license:
    explanation: "License: Apache 2.0"
    value: Apache 2.0
  intended_uses: The model can be used for tasks involving multimodal reasoning, chart and figure understanding, document question answering, and instruction following. It can also be used as a drop-in replacement for Mistral Nemo 12B. 
  prohibited_uses: Unknown
  monitoring: Unknown
  feedback: Unknown

