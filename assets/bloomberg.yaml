---

- type: dataset
  name: FinPile
  # General
  organization: Bloomberg
  description: A comprehensive dataset consisting of a range of English financial
    documents including news, filings, press releases, web-scraped financial documents,
    and social media drawn from the Bloomberg archives that was used to train the
    BloombergGPT model.
  created_date:
    value: 2023-03-30
    explanation: The date the model was announced in the [[Bloomberg article]](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/).
  url: https://arxiv.org/pdf/2303.17564.pdf#section.2
  datasheet:
    value: https://arxiv.org/pdf/2303.17564.pdf#section.2
    explanation: Section 2 of the BloombergGPT paper.
  modality: text (English)
  size:
    value: 363B tokens
    explanation: See [[Financial Datasets]](https://arxiv.org/pdf/2303.17564.pdf#subsection.2.1)
  sample: []
  analysis: ''
  # Construction
  dependencies: []
  license: unknown
  included:
    value: |
      FinPile consists of English financial documents. Authors utilize the The Bloomberg
      Terminal, which is an extensive collection of curated and maintained documents,
      to create the FinPile dataset. Each document in FinPile is time-stamped, with
      dates ranging from 2007-03-01 to 2022-07-31.
      Types of data included are given below:
        1. Web (298B tokens) - Inclues Bloomberg's web crawl focused on high-quality
      websites that have financially relevant information. This makes up the majority
      of FinPile.
        2. News (38B tokens) - Includes all news sources relevant to the financial
      community, excluding news articles written by Bloomberg journalists. Overall,
      there are hundreds of English news sources in FinPile including "Bloomberg
      Transcripts", which are transcripts of Bloomberg TV news.
        3. Filings (14B tokens) - Includes financial statements prepared by (public)
      companies and made available to the general public.  In the dataset, a majority
      of the filings come from EDGAR, which is the SEC's online database.
        4. Press (9B tokens) - Includes press releases typically issued by companies
      that are financially relevant.
        5. Bloomberg (5B tokens) - Includes Bloomberg authored news and other documents
      such as opinions and analyses. The largest sources are “Bloomberg News” and
      “Bloomberg First Word”, the Bloomberg-authored wire of real-time news.
    explanation: See [[Financial Datasets]](https://arxiv.org/pdf/2303.17564.pdf#subsection.2.1)
  excluded: ''
  quality_control: ''
  # Downstream
  access:
    value: closed
    explanation: See [[Openness]](https://arxiv.org/pdf/2303.17564.pdf#subsection.8.2)
  intended_uses: Used to train the BloombergGPT model.
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: BloombergGPT
  # General
  organization: Bloomberg
  description: BloombergGPT is a 50 billion parameter large language model that
    is specifically trained on a wide range of financial data to support a diverse
    set of natural language processing tasks within the financial industry.
  created_date:
    value: 2023-03-30
    explanation: The date the model was announced in the [[Bloomberg article]](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/).
  url: https://arxiv.org/abs/2303.17564
  model_card: none
  modality: text (English)
  analysis:
    value: >
      Authors evaluate the performance of BloombergGPT on two broad categories of
      tasks, finance-specific and general purpose, on several standard benchmarks.
      They compare BloombergGPT to the three closest models: GPT-NeoX, OPT-66B and
      BLOOM-176B. They also report results from the original GPT-3 whenever externally
      available. They conclude "We achieve strong results on general LLM benchmarks
      and outperform comparable models on financial tasks. We attribute this, in
      decreasing order of impact, to 1. a well-curated internal dataset, 2. our
      unique choice in tokenizer, and 3. an up-to-date architecture."
    explanation: See [[Evaluation]](https://arxiv.org/pdf/2303.17564.pdf#section.5)
  size: 50B parameters (dense model)
  # Construction
  dependencies: [FinPile, The Pile, C4, Wikipedia]
  training_emissions: unknown
  training_time:
    value: 53 days
    explanation: See [[Training Run]](https://arxiv.org/pdf/2303.17564.pdf#section.4)
  training_hardware:
    value: 64 Amazon EC2 p4d.24xlarge instances each with 8 NVIDIA 40GB A100 GPUs
      (i.e. total 512 A100 GPUs)
    explanation: See [[Training Configuration]](https://arxiv.org/pdf/2303.17564.pdf#subsection.3.3)
  quality_control:
    value: |
      Authors state the following:
      - "To provide natural language applications to the financial community, we
        have developed a rigorous risk and testing assessment process. This process
        includes careful annotation guidelines Tseng et al. (2020), pre-launch review
        at multiple levels by the central risk and compliance organizations, and
        by the product leaders (e.g., the newsroom) as applicable, and post-launch
        monitoring. Moreover, we conduct our research, development, and deployment
        of NLP and AI systems in accordance with all applicable regulations."
      - "Similarly, toxicity and bias are areas where, as a company, we take extraordinary
        care with any content we produce, whether from humans or machines. Since
        the measurement of toxicity and bias in our model depends on its application
        areas, quantifying the potential for the generation of harmful language
        remains an open question. We are particularly interested in studying whether
        FinPile, which is cleaner and contains fewer examples of overtly biased
        or toxic language (e.g., Press Releases), reduces the proclivity of the
        model to generate inappropriate content."
    explanation: See [[Ethics, Limitations, and Implications]](https://arxiv.org/pdf/2303.17564.pdf#section.8)
  # Downstream
  access:
    value: closed
    explanation: See [[Openness]](https://arxiv.org/pdf/2303.17564.pdf#subsection.8.2)
  license: unknown
  intended_uses:
    value: >
      "This model will assist Bloomberg in improving existing financial NLP
      tasks, such as sentiment analysis, named entity recognition, news classification,
      and question answering, among others. Furthermore, BloombergGPT will unlock
      new opportunities for marshalling the vast quantities of data available on
      the Bloomberg Terminal to better help the firm's customers, while bringing
      the full potential of AI to the financial domain."
    explanation: See [[Bloomberg article]](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/)
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
