---

# Datasets
- type: dataset
  name: ROOTS
  # General
  organization: BigScience
  description: The Responsible Open-science Open-collaboration Text Sources (ROOTS)
    corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter
    BigScience Large Open-science Open-access Multilingual (BLOOM) language model.
  created_date:
    value: 2022-06-06
  url: https://openreview.net/forum?id=UoEw6KigkUn
  datasheet: https://huggingface.co/spaces/bigscience-data/bigscience-corpus
  modality: Text (46 languages) and Code (13 programming languages)
  size: 1.6TB
  sample: [https://huggingface.co/spaces/bigscience-data/roots-search]
  analysis: https://huggingface.co/spaces/bigscience-data/roots-search
  # Construction
  dependencies: []
  license:
    value: Several
    explanation: Each constituent subset of the dataset will be released under the
      license that applies to it. ([See individual dataset page for specific license
      information](https://hf.co/bigscience-data))
  included: See section 2 of the paper.
  excluded: Unknown
  quality_control: Data collection involved merging and deduplicating searches to
    remove menus, HTML tags. Further, a quality improvement pipeline was implemented.
  # Downstream
  access:
    value: Open
    explanation: While not all of the ROOTS corpus is publicly accessible, the majority
      is through [this form](https://docs.google.com/forms/d/e/1FAIpQLSdq50O1x4dkdGI4dwsmchFuNI0KCWEDiKUYxvd0r0_sl6FfAQ/viewform?usp=send_form)
  intended_uses: To empower large-scale monolingual and multilingual modeling projects
    with both the data and the processing tools, as well as stimulate research around
    this large multilingual corpus.
  prohibited_uses: None
  monitoring: None
  feedback: https://huggingface.co/spaces/bigscience-data/roots-search/discussions

- type: dataset
  name: P3
  # General
  organization: BigScience
  description: The Public Pool of Prompts (P3) are prompts written in an unified
    format use to train T0++.
  created_date:
    value: 2022-10-15
  url: https://arxiv.org/pdf/2110.08207.pdf
  datasheet: https://huggingface.co/datasets/bigscience/P3
  modality: Text (English)
  size: 2000 prompts
  sample: [https://huggingface.co/datasets/bigscience/P3/viewer]
  analysis: See the paper.
  # Construction
  dependencies: []
  license: Apache 2.0
  included: The Public Pool of Prompts relies on the Hugging Face Dataset library.
    Any public dataset in the Datasets library can be prompted. We select the datasets
    that have at least one subset in English and excluded datasets containing (predominantly)
    non-natural language examples.
  excluded: We conservatively decided not to prompt datasets that contain potentially
    harmful content (for instance, datasets built on social media content).
  quality_control: Data collection involved merging and deduplicating searches to
    remove menus, HTML tags. Further, a quality improvement pipeline was implemented.
  # Downstream
  access: Open
  intended_uses: Multitask finetuning of language models.
  prohibited_uses: None
  monitoring: None
  feedback: Point of Contact is [Victor Sanh](https://huggingface.co/datasets/bigscience/P3)

- type: dataset
  name: xP3
  # General
  organization: BigScience
  description: xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts
    and datasets across 46 of languages & 16 NLP tasks. It is used for the training
    of BLOOMZ and mT0, multilingual language models capable of following human instructions
    in dozens of languages zero-shot.
  created_date:
    value: 2022-11-03
  url: https://arxiv.org/abs/2211.01786
  datasheet: https://huggingface.co/datasets/bigscience/xP3
  modality: Text (46 languages) and Code (13 programming languages)
  size: 9.4GB
  sample: [https://huggingface.co/datasets/bigscience/xP3/viewer]
  analysis: See the paper.
  # Construction
  dependencies: [P3]
  license: Apache 2.0
  included: xP3 adds 28 multilingual datasets to P3 based on the P3 task taxonomy.
  excluded: We conservatively decided not to prompt datasets that contain potentially
    harmful content (for instance, datasets built on social media content).
  quality_control: Data collection involved merging and deduplicating searches to
    remove menus, HTML tags. Further, a quality improvement pipeline was implemented.
  # Downstream
  access: Open
  intended_uses: Multitask finetuning of language models.
  prohibited_uses: None
  monitoring: None
  feedback: Point of Contact is [Niklas Muennighoff](https://huggingface.co/datasets/bigscience/xP3)


- type: model
  name: T0++
  organization: BigScience
  description: T0++ is an multitask fine-tuned language model based on T5.
  created_date:
    value: 2021-10-15
    explanation: The date the paper was posted to arXiv.
  url: https://arxiv.org/pdf/2110.08207.pdf
  model_card: https://huggingface.co/bigscience/T0pp
  modality: Text (English)
  size: 11B parameters (dense model)
  analysis: ''
  dependencies: [T5, P3]
  training_emissions:
    value: 0.9 tCO2e
    explanation: Sourced from estimates in https://arxiv.org/pdf/2110.08207.pdf
  training_time:
    value: 27 hours
    explanation: Sourced from estimates in https://arxiv.org/pdf/2110.08207.pdf
  training_hardware: Jean Zay (v3-512)
  quality_control: https://arxiv.org/pdf/2110.08207.pdf
  access: Open
  license:
    value: Apache 2.0
  intended_uses: You can use the models to perform inference on tasks by specifying
    your query in natural language, and the models will generate a prediction.
  prohibited_uses: None
  monitoring: None
  feedback: https://huggingface.co/bigscience/T0pp/discussions

- type: model
  name: BLOOM
  organization: BigScience
  description: BLOOM is an autoregressive multilingual language model.
  created_date:
    value: 2022-07-12
    explanation: The date the model was released
  url: https://arxiv.org/abs/2211.05100
  model_card: ''
  modality: Text (46 languages) and Code (13 programming languages)
  size: 176B parameters (dense model)
  analysis: ''
  dependencies: [ROOTS]
  training_emissions:
    value: 25 tCO2e
    explanation: Sourced from estimates in https://arxiv.org/abs/2211.02001
  training_time:
    value: 7039 petaflop/s-days
    explanation: 1082990 A100 hours at 156 TFLOP/s maximum utilization
  training_hardware: Jean Zay (48 * 8xA100 80GB nodes)
  quality_control: ''
  access: Open
  license:
    value: BigScience RAIL v1.0
    explanation: Model is licensed under https://huggingface.co/spaces/bigscience/license
  intended_uses: This model is being created in order to enable public research
    on large language models (LLMs). LLMs are intended to be used for language generation
    or as a pretrained base model that can be further fine-tuned for specific tasks.
    Use cases below are not exhaustive.
  prohibited_uses: Using the model in high-stakes settings is out of scope for this
    model (e.g. biomedical/political/legal/finance domains, evaluating or scoring
    individuals). The model is not designed for critical decisions nor uses with
    any material consequences on an individual's livelihood or wellbeing. The model
    outputs content that appears factual but may not be correct. Misuse. Intentionally
    using the model for harm, violating human rights, or other kinds of malicious
    activities, is a misuse of this model (e.g. spam generation, disinformation,
    disparagement, deception, surveillance).
  monitoring: None
  feedback: https://huggingface.co/bigscience/bloom/discussions

- type: model
  name: mT0
  organization: BigScience
  description: mT0 is an multitask fine-tuned multilingual language model based
    on mT5.
  created_date:
    value: 2021-10-15
    explanation: The date the model was released
  url: https://arxiv.org/pdf/2110.08207.pdf
  model_card: https://huggingface.co/bigscience/T0pp
  modality: Text (English)
  size: 13B parameters (dense model)
  analysis: https://huggingface.co/bigscience/bloomz#evaluation
  dependencies: [mT5, xP3]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Jean Zay (288 A100 80GB GPUs with 8 GPUs per node (36 nodes)
    using NVLink 4 inter-gpu connects, 4 OmniPath links)
  quality_control: https://arxiv.org/pdf/2211.01786.pdf
  access: Open
  license:
    value: BigScience RAIL v1.0
    explanation: Model is licensed under https://huggingface.co/spaces/bigscience/license
  intended_uses: We recommend using the model to perform tasks expressed in natural
    language.
  prohibited_uses: None
  monitoring: None
  feedback: https://huggingface.co/bigscience/bloomz/discussions

- type: model
  name: BLOOMZ
  organization: BigScience
  description: BLOOMZ is an multitask fine-tuned autoregressive multilingual language
    model.
  created_date:
    value: 2022-11-03
  url: https://arxiv.org/pdf/2211.01786.pdf
  model_card: https://huggingface.co/bigscience/bloomz
  modality: Text (46 languages) and Code (13 programming languages)
  size: 176B parameters (dense model)
  analysis: https://huggingface.co/bigscience/bloomz#evaluation
  dependencies: [BLOOM, xP3]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Jean Zay (288 A100 80GB GPUs with 8 GPUs per node (36 nodes)
    using NVLink 4 inter-gpu connects, 4 OmniPath links)
  quality_control: https://arxiv.org/pdf/2211.01786.pdf
  access: Open
  license:
    value: BigScience RAIL v1.0
    explanation: Model is licensed under https://huggingface.co/spaces/bigscience/license
  intended_uses: We recommend using the model to perform tasks expressed in natural
    language.
  prohibited_uses: None
  monitoring: None
  feedback: https://huggingface.co/bigscience/bloomz/discussions
