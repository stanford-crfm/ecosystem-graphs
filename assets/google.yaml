---
- type: dataset
  name: C4
  organization: Google
  description: The Colossal Clean Crawled Corpus (C4) is a processed version of
    Common Crawl to facilitate transfer learning in NLP.
  created_date:
    explanation: The date the T5 paper was made public.
    value: 2019-10-23
  url: https://arxiv.org/abs/1910.10683
  datasheet: https://huggingface.co/datasets/c4
  modality: text
  size: 750GB
  sample: [https://huggingface.co/datasets/c4/viewer/en/train]
  analysis: https://arxiv.org/abs/2104.08758
  dependencies: [CommonCrawl]
  included: none
  excluded: "Data was filtered for English using langdetect. Further, data was filtered\
    \ to end in terminal punctuation, to remove short pages (less than 5 sentences),\
    \ and to remove \"Dirty, Naughty, Obscene or Otherwise Bad Words\".\n"
  quality_control: "Data filtering excluded obscene words from a block list as well\
    \ as short documents and some deduplication was done based on string overlap.\n"
  access:
    explanation: https://huggingface.co/datasets/c4
    value: open
  license: ODC-By 1.0
  intended_uses: To faciliate transfer learning research in NLP.
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/datasets/c4/discussions
- type: dataset
  name: Internal Google BERT dataset
  organization: Google
  description: "The dataset used to train Internal Google BERT models.\n"
  created_date:
    explanation: "The date of the Google product update blog announcing that BERT\
      \ models were for ranking and featured snippets in Search.\n"
    value: 2019-11-25
  url: https://blog.google/products/search/search-language-understanding-bert/
  datasheet: none
  modality: text
  size: unknown
  sample: []
  analysis: unknown
  dependencies: []
  included:
    explanation: "Although we don't exactly know the contents of the Internal Google\
      \ BERT dataset, it likely includes contents from web pages and search queries.\n"
    value: Web pages, and search queries
  excluded: unknown
  quality_control: unknown
  access: closed
  license: unknown
  intended_uses:
    explanation: "We don't have an exhaustive list of the intended use cases for\
      \ the Internal Google BERT dataset, but we know that BERT was used in Google\
      \ Search.\n"
    value: unknown
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
- type: dataset
  name: Conceptual Captions
  organization: Google
  description: "A dataset containing 3 million (image-URL, caption) pairs designed\
    \ for the training and evaluation of machine learned image captioning systems.\n"
  created_date:
    explanation: Due to the lack of information about the exact date, it is assumed
      to be the 1st of the known month of creation.
    value: 2018-07-01
  url: https://aclanthology.org/P18-1238/
  datasheet: none
  modality: image, text
  size: 3.3M (image, text) pairs
  sample: []
  analysis:
    explanation: See [[Experimental Results]](https://aclanthology.org/P18-1238.pdf#section.5)
    value: Authors evaluate the dataset on two image captioning models - RNN-based
      model and Transformer model, under two experimental conditions - using the
      training & development sets provided by the MS COCO dataset, versus training
      & development sets using the Conceptual dataset. They use three different
      test sets- the blind COCO-C40 test set, the Conceptual test set and the Flickr
      1K test set. They present both Human and Automatic evaluation results. Human
      evaluations indicate that the Conceptual-based models are superior. Automatic
      models fail to corroborate the human evaluation results. This highlights the
      weakness of these automatic metrics.
  dependencies: []
  included: ''
  excluded:
    explanation: See [[Conceptual Captions Dataset Creation]](https://aclanthology.org/P18-1238.pdf#section.3)
    value: "The following filtering steps are applied in the given order:\n1. Image-based\
      \ Filtering - \"It only keeps JPEG images where both dimensions are greater\
      \ than 400 pixels, and the ratio of larger to smaller dimension is no more\
      \ than 2. It excludes images that trigger pornography or profanity detectors.\
      \ These filters discard more than 65% of the candidates.\"\n2. Text-based\
      \ Filtering - \"Candidates with no determiner, no noun, or no preposition\
      \ are discarded; candidates with a high noun ratio are also discarded; candidates\
      \ with a high rate of token repetition are discarded; candidates where the\
      \ first word is not capitalized, or with too high capitalized-word ratio are\
      \ discarded; we use a vocabulary VW of 1B token types, appearing at least\
      \ 5 times in the English Wikipedia, and discard candidates that contain tokens\
      \ that are not found in this vocabulary. candidates that score too high or\
      \ too low on the polarity annotations, or trigger the pornography/profanity\
      \ detectors, are discarded; predefined boiler-plate prefix/suffix sequences\
      \ matching the text are cropped, e.g. “click to enlarge picture”, “stock photo”;\
      \ we also drop text which begins/ends in certain patterns, e.g. “embedded\
      \ image permalink”, “profile photo”. These filters only allow around 3% of\
      \ the incoming candidates to pass to the later stages.\"\n3. Image&Text-based\
      \ Filtering - \"We filter out candidates for which none of the text tokens\
      \ can be mapped to the content of the image. This filter discards around 60%\
      \ of the incoming candidates.\"\n4. Text Transformation with Hypernymization\
      \ - \"Noun modifiers of certain types (proper nouns, numbers, units) are removed;\
      \ dates, durations, and preposition-based locations (e.g., \"in Los Angeles\"\
      ) are removed; named-entities are identified, matched against the KG entries,\
      \ and substitute with their hypernym; resulting coordination noun-phrases\
      \ with the same head (e.g., \"actor and actor\") are resolved into a single-head,\
      \ pluralized form (e.g., \"actors\"). Around 20% of samples are discarded\
      \ during this transformation. We then cluster all resolved entities (e.g.,\
      \ 2560 \"actor\", \"dog\", \"neighborhood\", etc.) and keep only the candidates\
      \ for which all detected types have a count of over 100 (around 55% of the\
      \ candidates).\"\n"
  quality_control:
    explanation: See [[Conceptual Captions Dataset Creation]](https://aclanthology.org/P18-1238.pdf#section.3)
    value: Input candidate (image, caption) pairs pass through several stages of
      filtering and processing to ensure quality.
  access:
    explanation: Dataset can be downloaded at [[Download]](https://ai.google.com/research/ConceptualCaptions/download)
    value: open
  license:
    explanation: "[[Conceptual Captions License]](https://github.com/google-research-datasets/conceptual-captions/blob/master/LICENSE)\n"
    value: Conceptual Captions License
  intended_uses: ''
  prohibited_uses: unknown
  monitoring: unknown
  feedback: Feedback can be provided by creating an issue in the [[Conceptual Captions
    GitHub repository]](https://github.com/google-research-datasets/conceptual-captions)
    or by emailing at conceptual-captions at google.com
- type: dataset
  name: Conceptual 12M
  organization: Google
  description: "A dataset with 12 million image-text pairs specifically meant to\
    \ be used for vision-and-language pre-training.\n"
  created_date:
    explanation: "The date the [[paper]](https://arxiv.org/abs/2102.08981) was submitted.\n"
    value: 2021-02-17
  url: https://arxiv.org/pdf/2102.08981.pdf
  datasheet: none
  modality: image, text
  size: 12M (image, text) pairs
  sample: []
  analysis:
    explanation: See [[Evaluating Vision-and-Language PreTraining Data]](https://arxiv.org/pdf/2102.08981.pdf#section.3)
    value: "The dataset is benchmarked against CC3M on two most fundamental V+L\
      \ tasks: vision-to-language generation and vision-and-language matching, with\
      \ an emphasis on long-tail visual recognition. The results illustrate the\
      \ benefit of scaling up pre-training data for vision-and-language tasks, as\
      \ indicated by the new state-of-the-art results on both the nocaps and Conceptual\
      \ Captions benchmarks.\n"
  dependencies: []
  included: ''
  excluded:
    explanation: See [[Relaxing filters for higher recall]](https://arxiv.org/pdf/2102.08981.pdf#subsection.2.2)
    value: "Some of the filtering steps used in the preparation of Conceptual Captions\
      \ dataset are relaxed to trade off high-recall for low-precision. The following\
      \ steps are applied in the given order:\n1. Image-based Filtering - Only keep\
      \ JPEG images where both dimensions are greater than 400 pixels, and the ratio\
      \ of larger to smaller dimension is no more than 2.5. Exclude images that\
      \ trigger pornography or profanity detectors.\n2. Text-based Filtering - Allow\
      \ text between 3 and 256 words in the alt-text. Discard candidates with no\
      \ noun or no determiner, but permit ones without prepositions. Set the maximum\
      \ fraction of word repetition allowed to 0.2. Increase the threshold for counting\
      \ a word type as rare from 5 to 20.\n3. Image&Text-based Filtering - Filter\
      \ out candidates for which none of the text tokens can be mapped to the content\
      \ of the image.\n"
  quality_control:
    explanation: See [[Vision-and-Language Pre-Training Data]](https://arxiv.org/pdf/2102.08981.pdf#section.2)
    value: Input candidate (image, caption) pairs pass through several stages of
      filtering and processing to ensure quality. Person-name substitutions are
      performed in the alt-texts to protect the privacy of individuals in the associated
      images.
  access:
    explanation: Dataset is available at [[Conceptual 12M GitHub repository]](https://github.com/google-research-datasets/conceptual-12m).
    value: open
  license:
    explanation: "[[Conceptual Captions License]](https://github.com/google-research-datasets/conceptual-captions/blob/master/LICENSE)\n"
    value: Conceptual Captions License
  intended_uses: ''
  prohibited_uses: unknown
  monitoring: unknown
  feedback: Feedback can be provided by creating an issue in the [[Conceptual 12M
    GitHub repository]](https://github.com/google-research-datasets/conceptual-12m)
    or by emailing at conceptual-captions at google.com
- type: model
  name: T5
  organization: Google
  description: Text-To-Text Transfer Transformer (T5) is a model that unifies all
    NLP tasks under the text-to-text format.
  created_date:
    explanation: The date the T5 paper was made public.
    value: 2019-10-23
  url: https://arxiv.org/abs/1910.10683
  model_card: https://huggingface.co/t5-base
  modality: text; text
  analysis: https://huggingface.co/t5-base#evaluation
  size: 11B parameters (dense)
  dependencies: [C4]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 1,024 TPU v3 chips (Cloud TPU Pods)
  quality_control: The T5 paper documents many analyses/ablations that were considered
    before arriving at the final architecture/training procedure.
  access:
    explanation: https://huggingface.co/t5-large
    value: open
  license: Apache 2.0
  intended_uses: NLP tasks
  prohibited_uses: unknown
  monitoring: none
  feedback: https://huggingface.co/t5-large/discussions
- type: model
  name: Internal Google BERT
  organization: Google
  description: "Internal Google BERT model used to power Google Search products.\n"
  created_date:
    explanation: "The date of the Google product update blog announcing that BERT\
      \ models were for ranking and featured snippets in Search.\n"
    value: 2019-11-25
  url: https://blog.google/products/search/search-language-understanding-bert/
  model_card: unknown
  modality: text; text
  analysis: unknown
  size: unknown
  dependencies: [Internal Google BERT dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: closed
  license: unknown
  intended_uses:
    explanation: "We don't have an exhaustive list of the intended use cases for\
      \ the Internal Google BERT model, but we know that Google Search was powered\
      \ by a fine-tuned BERT.\n"
    value: unknown
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
- type: application
  name: Google Search
  organization: Google
  description: "Google Search is Google's search engine.\n"
  created_date:
    explanation: "The date of the Google product update blog announcing that BERT\
      \ models were for ranking and featured snippets in Search.\n"
    value: 2019-11-25
  url: https://blog.google/products/search/search-language-understanding-bert/
  dependencies: [Internal Google BERT, MUM]
  adaptation: unknown
  output_space: web page ranking
  quality_control: unknown
  access: open
  license: none
  terms_of_service: https://policies.google.com/terms
  intended_uses: Searching the web using text, voice or image
  prohibited_uses: "Prohibited use cases aren't specifically spelled out for Google\
    \ search, but several illegal and discouraged use cases are shared in the Respect\
    \ Others section of the [[Term of Service]](https://policies.google.com/terms).\n"
  monitoring: "It is implied that Google scan uses of its products for spam, malware\
    \ and illegal content in the [[Term of Service]](https://policies.google.com/terms).\n"
  feedback: "Feedback can be sent to Google Feedback using the product interface\
    \ [[Google Feedback]](https://www.google.com/tools/feedback).\n"
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: dataset
  name: Infiniset
  organization: Google
  description: "Infiniset \"is a combination of dialog data from public dialog data\
    \ and other public web documents\" [[Appendix E]](https://arxiv.org/pdf/2201.08239.pdf#appendix.E).\n"
  created_date:
    explanation: "The date of the Google company news blog announcing LaMDA [[Google\
      \ News Blog]](https://blog.google/technology/ai/lamda/).\n"
    value: 2021-06-18
  url: https://arxiv.org/pdf/2201.08239.pdf
  datasheet: none
  modality: code, text
  size:
    explanation: "The size of the dataset is unclear, but it is reported that the\
      \ dataset \"consists of 2.97B documents and 1.12B dialogs with 13.39B utterances\"\
      \ [[Appendix E]](https://arxiv.org/pdf/2201.08239.pdf#appendix.E).\n"
    value: unknown
  sample: []
  analysis: unknown
  dependencies: []
  included: "Included in the dataset are data from \"public forums (0%); C4 data\
    \ (12.5% ); code documents from sites related to programming like Q&A sites\
    \ tutorials, etc (12.5%); Wikipedia (English) (12.5%); English web documents\
    \ (6.25%); and Non-English web documents (6.25%).\"\n"
  excluded: unknown
  quality_control: unknown
  access: closed
  license: unknown
  intended_uses:
    explanation: "Intended uses of the dataset wasn't explicitly linked, but it\
      \ is likely intended for training language models specialized in dialogue.\n"
    value: unknown
  prohibited_uses: "The prohibited uses for Infiniset weren't specifically listed,\
    \ but the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1)\
    \ advises avoiding harm, unjust impact and misinformation, among others.\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information was not provided.\n"
    value: none
- type: model
  name: LaMDA
  organization: Google
  description: "LaMDA stands for Language Models for Dialog Application. It is a\
    \ transformer based language model trained on dialogue data.\n"
  created_date:
    explanation: "The date of the Google company news blog announcing LaMDA [[Google\
      \ News Blog]](https://blog.google/technology/ai/lamda/).\n"
    value: 2021-06-18
  url: https://arxiv.org/pdf/2201.08239.pdf
  model_card: none
  modality: text; text
  analysis: "The model performance was analyzed on sensibleness, specificity and\
    \ interestingness. The model was also analyzed on safety, following metrics\
    \ derived from Google AI Principles [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1).\
    \ Finally, the model was analyzed on groundedness, testing its ability to produce\
    \ responses that can be associated with \"known sources whenever possible [[Section\
    \ 4.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.4.1).\"\n"
  size: 137B parameters (dense)
  dependencies: [Infiniset]
  training_emissions:
    explanation: "\"...total carbon footprint of LaMDA’s pre-training of the largest\
      \ model is approximately 25.2 tCO2e. The carbon footprint of pre-training\
      \ of smaller models and fine-tuning of all models is approximately 0.7 tCO2e\
      \ ... which brings the total footprint of LaMDA to approximately 26 tCO2e\
      \ [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10)\"\n"
    value: 26 tCO2e
  training_time:
    explanation: "The total number of training flops of LaMDA was reported as 3.55E+23\
      \ (3.55E+8 petaflops) [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10),\
      \ which is equal to 4108.80 = 3.55E+8 / (60 * 60 * 24) petaflop/s-day.\n"
    value: 4108.80 petaflop/s-day
  training_hardware:
    explanation: "Reported in [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10).\n"
    value: 1024 TPU-V3 chips
  quality_control: "LaMDA was fine-tuned to predict sensibleness, specificity and\
    \ interestingness as well as safety. Then, the candidates were filtered out\
    \ if the model safety predictions were below a certain threshold. The next candidates\
    \ in the conversation were selected as a combination of these predictions. The\
    \ model was also fine-tuned for groundedness. The results are shown in [[Figure\
    \ 5]](https://arxiv.org/pdf/2201.08239.pdf#figure.caption.23).\n"
  access: closed
  license: unknown
  intended_uses: "LaMDA is a language model, so it can be used for regular langauge\
    \ modelling tasks without fine-tuning, but its fine-tuned for dialogue tasks.\n"
  prohibited_uses: "The prohibited uses of LaMDA weren't specifically listed, but\
    \ the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1)\
    \ advises avoiding harm, unjust impact and misinformation, among others.\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information was not provided.\n"
    value: none
- type: dataset
  name: PaLM dataset
  organization: Google
  description: "PaLM dataset \"was created for pre-training language models\" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\n"
  created_date:
    explanation: "The date of the Google AI blog announcing the details of PaLM\
      \ [[Google AI Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).\n"
    value: 2022-04-04
  url: https://arxiv.org/pdf/2204.02311.pdf
  datasheet: https://arxiv.org/pdf/2204.02311.pdf#appendix.D
  modality: code, text
  size:
    explanation: "Dataset size in GB is not reported, but the dataset is reported\
      \ to have 780 billion tokens [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\
      \ The code portion of the dataset is reported to be 5% totaling a 196GB of\
      \ source code [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\
      \ It is unclear whether the reported size is before or after de-duplication.\
      \ Nonetheless, one can estimate the dataset size by multiplying 196GB with\
      \ 20 = 3.92 TB.\n"
    value: 3.92 TB
  sample: []
  analysis: unknown
  dependencies: [Infiniset]
  included: "The dataset is based on Infiniset. It included multilingual text containing\
    \ text from over 100 languages. The breakdown of the data included is as follows:\
    \ Social media conversations (multilingual) 50, Filtered webpages (multilingual)\
    \ 27%, BooksCorpus (English) 13%, GitHub (code) 5%, Wikipedia (multilingual)\
    \ 4%, and News (English) 1%. Code was collected from GitHub repositories with\
    \ appropriate licenses, totalling 96GB of source code [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\n"
  excluded: "GitHub repositories with copyleft licenses were excluded. Programming\
    \ languageges other than the most common 24 were excluded [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\n"
  quality_control: "In order to reduce low quality web pages, the web pages were\
    \ sampled according to a \"quality score\" classifier. Code files were de-duplicated\
    \ using Levenshtein distance [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\n"
  access: closed
  license: unknown
  intended_uses:
    explanation: "As stated in [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\n"
    value: "\"The dataset was created for pre-training language models by a team\
      \ of researchers at Google\".\n"
  prohibited_uses: "\"... should not be used for any of the unacceptable language\
    \ model use cases, e.g., generation of toxic speech\" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information is shared in the paper [[Paper]](https://arxiv.org/pdf/2204.02311.pdf).\n"
    value: Contact the authors.
- type: model
  name: Flan-T5
  organization: Google
  description: Flan-T5 is a version of the T5 language model fine-tuned on instruction
    data
  created_date:
    explanation: Date paper was released
    value: 2022-10-20
  url: https://arxiv.org/abs/2210.11416
  model_card: https://arxiv.org/pdf/2210.11416.pdf
  modality: text; text
  analysis: Evaluated on a variety of standard language datasets.
  size: 11B parameters (dense)
  dependencies: [T5, Muffin, P3, NaturalInstructions-v2, Flan CoT]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: 512 v4 TPU Chips
  quality_control: Across different multitask datasets, templates and formatting
    were maintained. For the chain-of-thoughts (CoT) data, specific exemplars were
    used.
  access:
    explanation: Weights can be downloaded from [Github](https://github.com/google-research/t5x/blob/main/docs/models.md)
    value: open
  license:
    explanation: License on the [[Github repository]](https://github.com/google-research/google-research)
    value: Apache 2.0
  intended_uses: unknown
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/google/flan-t5-xxl/discussions
- type: model
  name: UL2
  organization: Google
  description: UL2 is a language model trained with a new pretraining objective
  created_date:
    explanation: Date model paper was released
    value: 2022-05-10
  url: https://arxiv.org/abs/2205.05131
  model_card: ''
  modality: text; text
  analysis: ''
  size: 20B parameters (dense)
  dependencies: [C4]
  training_emissions: ''
  training_time: ''
  training_hardware: 128 TPUv4
  quality_control: ''
  access:
    explanation: Model weights available for download in the [[Github repo]](https://github.com/google-research/google-research/tree/master/ul2)
    value: open
  license:
    explanation: 20B checkpoints only for three different iteration steps
    value: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Parti
  organization: Google
  description: Parti is a text-to-image diffusion model
  created_date:
    explanation: Date the model website was made public
    value: 2022-06-22
  url: https://parti.research.google/
  model_card: ''
  modality: text; image
  analysis: ''
  size: 20B parameters (dense)
  dependencies: [C4, LAION-400M, FIT400M, JFT-4B]
  training_emissions: ''
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access:
    explanation: Google does not provide access to Parti for external researchers.
    value: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Imagen
  organization: Google
  description: Imagen is a text-to-image diffusion model
  created_date:
    explanation: Date the model website was made public
    value: 2022-05-23
  url: https://imagen.research.google/
  model_card: ''
  modality: text; image
  analysis: ''
  size: 14B parameters (dense)
  dependencies: [LAION-400M, Google internal image-text dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 128 TPU-v4
  quality_control: ''
  access: open
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: VATT
  organization: Google
  description: VATT is a family of models trained on multimodal data
  created_date:
    explanation: Date the model paper was made public
    value: 2022-04-22
  url: https://arxiv.org/abs/2104.11178
  model_card: ''
  modality: text; audio, video
  analysis: ''
  size: 155M parameters (dense)
  dependencies: [AudioSet, HowTo100M]
  training_emissions: unknown
  training_time: 3 days
  training_hardware: 256 TPU-v3
  quality_control: ''
  access:
    explanation: Model checkpoints can be downloaded from the [[Github repository]](https://github.com/google-research/google-research/tree/master/vatt)
    value: open
  license:
    explanation: License on the [[Github repository]](https://github.com/google-research/google-research)
    value: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLM
  organization: Google
  description: "PaLM stands Pathways Language Model, \"dense decoder-only Transformer\
    \ model trained with the Pathways system\" [[Google ai Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).\n"
  created_date:
    explanation: "The date of the Google AI blog announcing the details of PaLM\
      \ [[Google AI Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).\n"
    value: 2022-04-04
  url: https://arxiv.org/pdf/2204.02311.pdf
  model_card: https://arxiv.org/pdf/2204.02311.pdf#appendix.E
  modality: text; text, code
  analysis: "\"PaLM is evaluated on English Natural Language Processing (NLP) tasks,\
    \ tasks from BIG-bench, reasoning tasks, code completion tasks, multilingual\
    \ generation and question answering tasks, translation tasks, and bias and toxicity\
    \ benchmarks\" [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).\n"
  size: 540B parameters (dense)
  dependencies: [PaLM dataset]
  training_emissions:
    explanation: "Reported in [[Appendix B]](https://arxiv.org/pdf/2204.02311.pdf#appendix.A)\n"
    value: 271.43 tCO2
  training_time:
    explanation: "Reported in [[Appendix B]](https://arxiv.org/pdf/2204.02311.pdf#appendix.A)\n"
    value: 29600 petaflop/s-days
  training_hardware:
    explanation: "Reported in [[Section 4]](https://arxiv.org/pdf/2204.02311.pdf#section.4).\n"
    value: 6144 TPU v4 chips
  quality_control: Unknown
  access:
    explanation: Made available through the PaLM API as of March 14, 2023.
    value: limited
  license: unknown
  intended_uses: "\"The primary use is research on language models, including: research\
    \ on NLP applications like machine translation and question answering, advancing\
    \ fairness and safety research, and understanding limitations of current LLMs.\
    \ Within Google, PaLM is being used for research on a variety of open- ended\
    \ text and code generation tasks, including reasoning [[Section 6.3]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.3)\
    \ and code synthesis and understanding [[Section 6.4]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.4)\"\
    \ [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).\n"
  prohibited_uses: "The model \"should not be used for downstream applications without\
    \ further analysis on factors in the proposed downstream application [[Model\
    \ Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E)\"\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information is shared in the paper [[Paper]](https://arxiv.org/pdf/2204.02311.pdf).\n"
    value: Contact the authors.
- type: application
  name: PaLM API
  organization: Google
  description: a new developer offering that makes it easy and safe to experiment
    with Google’s language models.
  created_date: 2023-03-14
  url: https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html
  dependencies: [PaLM]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license: unknown
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: Med-PaLM
  organization: Google
  description: ''
  created_date: 2022-12-26
  url: https://arxiv.org/abs/2212.13138
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [Flan-PaLM, MultiMedQA]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Med-PaLM Multimodal
  organization: Google
  description: ''
  created_date: 2023-07-26
  url: https://arxiv.org/pdf/2307.14334.pdf
  model_card: ''
  modality: image, text, genome sequence; text
  analysis: Evaluated on MultiMedBench tasks and radiologist evaluations of model-generated
    chest X-ray reports
  size: 562B parameters (dense)
  dependencies: [PaLM-E, MultiMedBench]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MultiMedQA
  organization: Google
  description: ''
  created_date: 2022-12-26
  url: https://arxiv.org/abs/2212.13138
  model_card: ''
  modality: text; text
  analysis: ''
  size: unknown
  dependencies:
    - MedQA
    - MedMCQA
    - PubMedQA
    - MMLU
    - LiveQA
    - Medication QA
    - HealthSearchQA
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Flan-PaLM
  organization: Google
  description: ''
  created_date: 2022-10-20
  url: https://arxiv.org/abs/2210.11416
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [PaLM, Muffin, P3, NaturalInstructions-v2]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Flan-U-PaLM
  organization: Google
  description: ''
  created_date: 2022-10-20
  url: https://arxiv.org/abs/2210.11416
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [U-PaLM, Muffin, P3, NaturalInstructions-v2]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Muffin
  organization: Google
  description: ''
  created_date: 2021-09-03
  url: https://arxiv.org/abs/2109.01652
  datasheet: ''
  modality: text
  size: 62 tasks
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: U-PaLM
  organization: Google
  description: ''
  created_date: 2022-10-20
  url: https://arxiv.org/abs/2210.11399
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [PaLM, PaLM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLM-SayCan
  organization: Google
  description: ''
  created_date: 2022-08-16
  url: https://arxiv.org/abs/2204.01691
  model_card: ''
  modality: text; robotics trajectories
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [PaLM]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown (model weights), Apache 2.0 (SayCan code)
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: GLaM
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  model_card: ''
  modality: text; text
  analysis: ''
  size: 1.2T parameters (sparse)
  dependencies:
    - GLaM Web dataset
    - Wikipedia
    - GLaM Conversations dataset
    - GLaM Forums dataset
    - BooksCorpus
    - GLaM News dataset
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM Web dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM Conversations dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM Forums dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM News dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MUM
  organization: Google
  description: MUM (Multitask Unified Model) is a multimodal model that is specialized
    for more complex queries.
  created_date: 2021-05-18
  url: https://blog.google/products/search/introducing-mum/
  model_card: ''
  modality: image, text; text
  analysis: ''
  size: unknown
  dependencies: [MUM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: MUM dataset
  organization: Google
  description: ''
  created_date: 2021-05-18
  url: https://blog.google/products/search/introducing-mum/
  datasheet: ''
  modality: image, text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Phenaki
  organization: Google
  description: ''
  created_date: 2023-02-01
  url: https://openreview.net/pdf?id=vOEXS39nOF
  model_card: ''
  modality: text; video
  analysis: ''
  size: 1.8B parameters (dense)
  dependencies: [LAION-400M, Phenaki Video-Text Corpus]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Phenaki Video-Text Corpus
  organization: Google
  description: ''
  created_date: 2023-02-01
  url: https://openreview.net/pdf?id=vOEXS39nOF
  datasheet: ''
  modality: text, video
  size: 15M text-video pairs at 8FPS
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Flan-UL2
  organization: Google
  description: ''
  created_date: 2023-03-02
  url: https://arxiv.org/abs/2205.05131
  model_card: ''
  modality: text; text
  analysis: ''
  size: 20B parameters (dense)
  dependencies: [UL2, Flan Collection]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Flan Collection
  organization: Google
  description: ''
  created_date: 2023-01-31
  url: https://arxiv.org/abs/2301.13688
  datasheet: ''
  modality: text
  size: 1836 tasks
  sample: []
  analysis: ''
  dependencies: [Flan dataset, P3, NaturalInstructions-v2]
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MusicLM
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: text; audio
  analysis: ''
  size: 1.4B parameters (dense)
  dependencies:
    - SoundStream
    - w2v-BERT
    - MuLan
    - MusicLM semantic model
    - MusicLM acoustic model
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: SoundStream
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: unknown
  dependencies: [Free Music Archive]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: w2v-BERT
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: 600M parameters (dense)
  dependencies: [Free Music Archive]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MuLan
  organization: Google
  description: ''
  created_date: 2022-08-26
  url: https://arxiv.org/abs/2208.12415
  model_card: ''
  modality: text; audio
  analysis: ''
  size: unknown
  dependencies: [AST, BERT, MuLan dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: MuLan dataset
  organization: Google
  description: ''
  created_date: 2022-08-26
  url: https://arxiv.org/abs/2208.12415
  datasheet: ''
  modality: audio, text
  size: 370K hours audio
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: MusicLM dataset
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  datasheet: ''
  modality: audio
  size: 280K hours audio
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MusicLM semantic model
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: 430M parameters (dense)
  dependencies: [MusicLM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MusicLM acoustic model
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: 430M parameters (dense)
  dependencies: [MusicLM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Noise2Music
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  model_card: ''
  modality: audio, text; audio
  analysis: ''
  size: unknown
  dependencies: [Noise2Music pseudolabel dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknkown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: LaMDA-LF
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: text
  size: 150k songs
  sample: []
  analysis: ''
  dependencies: [LaMDA]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Rater-LF
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: text
  size: 10k captions
  sample: []
  analysis: ''
  dependencies: [MusicCaps]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Rater-SF
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: text
  size: 24k captions
  sample: []
  analysis: ''
  dependencies: [MusicCaps]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Noise2Music pseudolabeler
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  model_card: ''
  modality: audio, text; audio
  analysis: ''
  size: unknown
  dependencies: [MuLan, MuLaMCap, LaMDA-LF, Rater-LF, Rater-SF]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Noise2Music audio dataset
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: audio
  size: 340k hours audio
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Noise2Music pseudolabel dataset
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: audio, text
  size: 340k hours audio with pseudolabels
  sample: []
  analysis: ''
  dependencies: [Noise2Music audio dataset, Noise2Music pseudolabeler]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n'"
    value: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: AI Test Kitchen
  organization: Google
  description: AI Test Kitchen provides a new way for people to learn about, experience,
    and give feedback on emerging AI technology, like LaMDA.
  created_date: 2022-08-25
  url: https://blog.google/technology/ai/join-us-in-the-ai-test-kitchen/
  dependencies: [LaMDA]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license: unknown
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Bard
  organization: Google
  description: Conversational AI service, powered by LaMDA
  created_date: 2023-02-06
  url: https://blog.google/technology/ai/bard-google-ai-search-updates/
  dependencies: [LaMDA]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: closed
  license: unknown
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: Minerva
  organization: Google
  description: ''
  created_date: 2022-06-29
  url: https://arxiv.org/abs/2206.14858
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies:
    - PaLM
    - arXiv
    - PaLM dataset
    - Minerva Math Web Pages dataset
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Minerva Math Web Pages dataset
  organization: Google
  description: ''
  created_date: 2022-06-29
  url: https://arxiv.org/abs/2206.14858
  datasheet: ''
  modality: text
  size: 17.5B tokens
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: USM
  organization: Google
  description: Universal Speech Model (USM) is a family of state-of-the-art speech
    models with 2B parameters trained on 12 million hours of speech and 28 billion
    sentences of text, spanning 300+ languages. USM, which is for use in YouTube
    (e.g., for closed captions), can perform automatic speech recognition (ASR)
    on widely-spoken languages like English and Mandarin, but also languages like
    Punjabi, Assamese, Santhali, Balinese, Shona, Malagasy, Luganda, Luo, Bambara,
    Soga, Maninka, Xhosa, Akan, Lingala, Chichewa, Nkore, Nzema to name a few. Some
    of these languages are spoken by fewer than twenty million people, making it
    very hard to find the necessary training data.
  created_date: 2023-03-06
  url: https://arxiv.org/abs/2303.01037
  model_card: ''
  modality: audio, text; text
  analysis: ''
  size: 2B parameters (dense)
  dependencies: [YT-NLU-U, Pub-U, Web-NTL, YT-SUP+, Pub-S]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: YouTube
  organization: Google
  description: YouTube is a global online video sharing and social media platform
  created_date: 2005-02-14
  url: https://www.youtube.com/
  dependencies: [USM]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license: ''
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: PaLM-E
  organization: Google
  description: ''
  created_date: 2023-03-06
  url: https://arxiv.org/abs/2303.03378
  model_card: ''
  modality: image, text; text
  analysis: ''
  size: 562B parameters (dense)
  dependencies: [PaLM, ViT-22B]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: ViT-22B
  organization: Google
  description: ''
  created_date: 2023-02-10
  url: https://arxiv.org/abs/2302.05442
  model_card: ''
  modality: image, image
  analysis: ''
  size: 22B parameters (dense)
  dependencies: [JFT]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: AudioLM
  organization: Google
  description: ''
  created_date: 2022-09-07
  url: https://arxiv.org/abs/2209.03143
  model_card: ''
  modality: audio, text; audio
  analysis: ''
  size: 1B parameters (dense)
  dependencies: [w2v-BERT, SoundStream]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLI
  organization: Google
  description: ''
  created_date: 2022-09-14
  url: https://arxiv.org/abs/2209.06794
  model_card: ''
  modality: text; image
  analysis: ''
  size: 17B parameters (dense)
  dependencies: [mT5, ViT-e, WebLI]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: ViT-e
  organization: Google
  description: ''
  created_date: 2022-09-14
  url: https://arxiv.org/abs/2209.06794
  model_card: ''
  modality: image; image
  analysis: ''
  size: 3.9B parameters (dense)
  dependencies: [JFT]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: WebLI
  organization: Google
  description: ''
  created_date: 2022-09-14
  url: https://arxiv.org/abs/2209.06794
  datasheet: ''
  modality: image, text
  size: 10B images, 12B alt-text
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Vid2Seq
  organization: Google
  description: ''
  created_date: 2023-02-27
  url: https://arxiv.org/abs/2302.14115
  model_card: ''
  modality: video; text
  analysis: ''
  size: 500M parameters (dense)
  dependencies: [T5, CLIP, YT-Temporal-1B]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Google Joint SLM
  organization: Google
  description: Joint speech and language model using a Speech2Text adapter and using
    a CTC-based blank-filtering.
  created_date: 2023-06-08
  url: https://arxiv.org/pdf/2306.07944.pdf
  model_card: ''
  modality: audio; text
  analysis: evaluated on DSTC11 Challenge Task, based on MultiWoz 2.1, with a focus
    on dialog state tracking.
  size: unknown
  dependencies: [CTC blank-filtering, Speech2Text adapter]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLM 2
  organization: Google
  description: PaLM 2 is a new state-of-the-art language model that has better multilingual
    and reasoning capabilities and is more compute-efficient than its predecessor
    PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives
    similar to UL2.
  created_date: 2023-05-10
  url: https://blog.google/technology/ai/google-palm-2-ai-large-language-model/
  model_card: https://ai.google/static/documents/palm2techreport.pdf
  modality: text; text
  analysis: Reports results on standard code benchmarks across a variety of programming
    languages.
  size: unknown
  dependencies: [PaLM 2 dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: TPU v4 (number unspecified)
  quality_control: Employed de-duplication, removal of sensitive-PII and filtering.
    Added control tokens marking toxicity of text.
  access: open
  license: unknown
  intended_uses: general use large language model that can be used for language,
    reasoning, and code tasks.
  prohibited_uses: becoming part of a general-purpose service or product or use
    within specific downstream applications without prior assessment
  monitoring: Google internal monitoring
  feedback: Specific queries provided by annotators
- type: model
  name: MedLM
  organization: Google
  description: MedLM is a collection of foundation models tuned to follow natural
    language instructions for tasks in medicine, such as question answering and
    creating draft summaries.
  created_date: 2023-12-13
  url: https://cloud.google.com/vertex-ai/docs/generative-ai/medlm/overview
  model_card: https://cloud.google.com/static/vertex-ai/docs/generative-ai/medlm/MedLM-model-card.pdf
  modality: text; text
  analysis: Assessed on medical benchmarks of professional medical exams, medical
    research, and consumer queries.
  size: unknown
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: limited
  license: unknown
  intended_uses: to be used for question answering and creating draft summaries
    from existing documentation, to be reviewed, edited, and approved by the user
    before use.
  prohibited_uses: ''
  monitoring: Google internal monitoring
  feedback: none
- type: model
  name: Gemini
  organization: Google
  description: As of release, Gemini is Google's most capable and flexible AI model,
    proficient in multimodal domains.
  created_date: 2023-12-06
  url: https://deepmind.google/technologies/gemini/#introduction
  model_card: none
  modality: text; image, text, video
  analysis: Evaluated on standard general, reasoning, math, coding, and multimodal
    benchmarks with results that surpass GPT-4 on almost all.
  size:
    explanation: Comes in sizes Ultra, Pro, and Nano.
    value: unknown
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: general use large language model that can be used for language,
    reasoning, and code tasks.
  prohibited_uses: becoming part of a general-purpose service or product or use
    within specific downstream applications without prior assessment
  monitoring: Google internal monitoring
  feedback: none
- type: model
  name: TimesFM
  organization: Google
  description: TimesFM is a single forecasting model pre-trained on a large time-series
    corpus of 100 billion real world time-points.
  created_date: 2024-02-02
  url: https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html
  model_card: none
  modality: ''
  analysis: Evaluated on popular time-series benchmarks.
  size: 200M parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: none
- type: model
  name: Gemma
  organization: Google
  description: Gemma is a family of lightweight, state-of-the-art open models from
    Google, based on the Gemini models. They are text-to-text, decoder-only large
    language models, available in English.
  created_date: 2024-02-21
  url: https://blog.google/technology/developers/gemma-open-models/
  model_card: https://huggingface.co/google/gemma-7b
  modality: text; text
  analysis: Evaluation was conducted on standard LLM benchmarks and includes internal
    red-teaming testing of relevant content policies.
  size: 7B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: TPUv5e
  quality_control: Multiple evaluations and red-teaming conducted, with particular
    focus on ethics, bias, fair use cases, and safety.
  access: open
  license:
    explanation: License can be found at https://ai.google.dev/gemma/terms.
    value: custom
  intended_uses: Text generation tasks including question answering, summarization,
    and reasoning; content creation, communication, research, and education.
  prohibited_uses: Prohibited uses are specified in the Gemma Prohibited Use Policy
    here https://ai.google.dev/gemma/prohibited_use_policy
  monitoring: ''
  feedback: https://huggingface.co/google/gemma-7b/discussions
- type: model
  name: Med-Gemini
  organization: Google
  description: Med-Gemini is a family of highly capable multimodal models that are
    specialized in medicine with the ability to seamlessly integrate the use of
    web search, and that can be efficiently tailored to novel modalities using custom
    encoders.
  created_date: 2024-04-29
  url: https://arxiv.org/pdf/2404.18416
  model_card: none
  modality: image, text; text
  analysis: Evaluated Med-Gemini on 14 medical benchmarks spanning text, multimodal
    and long-context applications, establishing new state-of-the-art (SoTA) performance
    on 10 of them, and surpassing the GPT-4 model family on every benchmark where
    a direct comparison is viable.
  size: unknown
  dependencies: [Gemini, MultiMedBench]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: To be used in areas of medical research including medical summarization,
    referral letter generation, and medical simplification tasks.
  prohibited_uses: Unfit for real-world deployment in the safety-critical medical
    domain.
  monitoring: ''
  feedback: none
- type: model
  name: FLAMe (Foundational Large Autorater Models)
  organization: Google DeepMind
  description: FLAMe is a family of Foundational Large Autorater Models. The model is trained on a large and diverse collection of over 100 quality assessment tasks comprising 5M+ human judgments. It is designed to serve as a powerful starting point for further downstream fine-tuning. Specific variants of the model have been developed for various tasks, notably FLAMe-RM-24B for reward modeling evaluation.
  created_date: 2024-07-15
  url: http://arxiv.org/pdf/2407.10817
  model_card: 
  modality: text; text
  analysis: FLAMe outperforms Large Language Models like GPT-4, Claude-3 on many tasks. It enhances generalization to a wide variety of held-out tasks. Notably, on RewardBench, FLAMe-RM-24B achieves an accuracy of 87.8%.
  size: 24B parameters. The model card states "FLAMe-RM-24B" which likely refers to the model having 24 billion parameters.
  dependencies: [GPT-4, Claude-3, Llama-3, PaLM-2-24B, Various quality assessment tasks]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: FLAMe is trained on extensive human judgments on quality assessment tasks, carefully curated and standardized. It also was fine-tuned and evaluated across a wide variety of tasks for robust performance.
  access: Unknown
  license: Not stated. However, it is mentioned that all data used to train the model are publicly available and permissively licensed.
  intended_uses: FLAMe is intended to be an effective tool for automatically evaluating the output of large language models. A specific use case presented in the model card is use as an autorater for reward modeling evaluation (FLAMe-RM-24B).
  prohibited_uses: FLAMe should not be used for applications where human-level rating accuracy is required. Also, it might carry the risk of reinforcing biases and hallucinations, as it is trained predominantly on human judgments.
  monitoring: Unknown
  feedback: Contact the corresponding author(s) via email.
