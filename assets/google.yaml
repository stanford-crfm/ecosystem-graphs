---
- type: dataset
  name: C4
  organization: Google
  description: The Colossal Clean Crawled Corpus (C4) is a processed version of
    Common Crawl to facilitate transfer learning in NLP.
  created_date:
    explanation: The date the T5 paper was made public.
    value: 2019-10-23
  url: https://arxiv.org/abs/1910.10683
  datasheet: https://huggingface.co/datasets/c4
  modality: text
  size: 750GB
  sample: [https://huggingface.co/datasets/c4/viewer/en/train]
  analysis: https://arxiv.org/abs/2104.08758
  dependencies: [CommonCrawl]
  included: none
  excluded: "Data was filtered for English using langdetect. Further, data was filtered\
    \ to end in terminal punctuation, to remove short pages (less than 5 sentences),\
    \ and to remove \"Dirty, Naughty, Obscene or Otherwise Bad Words\".\n"
  quality_control: "Data filtering excluded obscene words from a block list as well\
    \ as short documents and some deduplication was done based on string overlap.\n"
  access:
    explanation: https://huggingface.co/datasets/c4
    value: open
  license: ODC-By 1.0
  intended_uses: To faciliate transfer learning research in NLP.
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/datasets/c4/discussions
- type: dataset
  name: Internal Google BERT dataset
  organization: Google
  description: "The dataset used to train Internal Google BERT models.\n"
  created_date:
    explanation: "The date of the Google product update blog announcing that BERT\
      \ models were for ranking and featured snippets in Search.\n"
    value: 2019-11-25
  url: https://blog.google/products/search/search-language-understanding-bert/
  datasheet: none
  modality: text
  size: unknown
  sample: []
  analysis: unknown
  dependencies: []
  included:
    explanation: "Although we don't exactly know the contents of the Internal Google\
      \ BERT dataset, it likely includes contents from web pages and search queries.\n"
    value: Web pages, and search queries
  excluded: unknown
  quality_control: unknown
  access: closed
  license: unknown
  intended_uses:
    explanation: "We don't have an exhaustive list of the intended use cases for\
      \ the Internal Google BERT dataset, but we know that BERT was used in Google\
      \ Search.\n"
    value: unknown
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
- type: dataset
  name: Conceptual Captions
  organization: Google
  description: "A dataset containing 3 million (image-URL, caption) pairs designed\
    \ for the training and evaluation of machine learned image captioning systems.\n"
  created_date:
    explanation: Due to the lack of information about the exact date, it is assumed
      to be the 1st of the known month of creation.
    value: 2018-07-01
  url: https://aclanthology.org/P18-1238/
  datasheet: none
  modality: image, text
  size: 3.3M (image, text) pairs
  sample: []
  analysis:
    explanation: See [[Experimental Results]](https://aclanthology.org/P18-1238.pdf#section.5)
    value: Authors evaluate the dataset on two image captioning models - RNN-based
      model and Transformer model, under two experimental conditions - using the
      training & development sets provided by the MS COCO dataset, versus training
      & development sets using the Conceptual dataset. They use three different
      test sets- the blind COCO-C40 test set, the Conceptual test set and the Flickr
      1K test set. They present both Human and Automatic evaluation results. Human
      evaluations indicate that the Conceptual-based models are superior. Automatic
      models fail to corroborate the human evaluation results. This highlights the
      weakness of these automatic metrics.
  dependencies: []
  included: ''
  excluded:
    explanation: See [[Conceptual Captions Dataset Creation]](https://aclanthology.org/P18-1238.pdf#section.3)
    value: "The following filtering steps are applied in the given order:\n1. Image-based\
      \ Filtering - \"It only keeps JPEG images where both dimensions are greater\
      \ than 400 pixels, and the ratio of larger to smaller dimension is no more\
      \ than 2. It excludes images that trigger pornography or profanity detectors.\
      \ These filters discard more than 65% of the candidates.\"\n2. Text-based\
      \ Filtering - \"Candidates with no determiner, no noun, or no preposition\
      \ are discarded; candidates with a high noun ratio are also discarded; candidates\
      \ with a high rate of token repetition are discarded; candidates where the\
      \ first word is not capitalized, or with too high capitalized-word ratio are\
      \ discarded; we use a vocabulary VW of 1B token types, appearing at least\
      \ 5 times in the English Wikipedia, and discard candidates that contain tokens\
      \ that are not found in this vocabulary. candidates that score too high or\
      \ too low on the polarity annotations, or trigger the pornography/profanity\
      \ detectors, are discarded; predefined boiler-plate prefix/suffix sequences\
      \ matching the text are cropped, e.g. “click to enlarge picture”, “stock photo”;\
      \ we also drop text which begins/ends in certain patterns, e.g. “embedded\
      \ image permalink”, “profile photo”. These filters only allow around 3% of\
      \ the incoming candidates to pass to the later stages.\"\n3. Image&Text-based\
      \ Filtering - \"We filter out candidates for which none of the text tokens\
      \ can be mapped to the content of the image. This filter discards around 60%\
      \ of the incoming candidates.\"\n4. Text Transformation with Hypernymization\
      \ - \"Noun modifiers of certain types (proper nouns, numbers, units) are removed;\
      \ dates, durations, and preposition-based locations (e.g., \"in Los Angeles\"\
      ) are removed; named-entities are identified, matched against the KG entries,\
      \ and substitute with their hypernym; resulting coordination noun-phrases\
      \ with the same head (e.g., \"actor and actor\") are resolved into a single-head,\
      \ pluralized form (e.g., \"actors\"). Around 20% of samples are discarded\
      \ during this transformation. We then cluster all resolved entities (e.g.,\
      \ 2560 \"actor\", \"dog\", \"neighborhood\", etc.) and keep only the candidates\
      \ for which all detected types have a count of over 100 (around 55% of the\
      \ candidates).\"\n"
  quality_control:
    explanation: See [[Conceptual Captions Dataset Creation]](https://aclanthology.org/P18-1238.pdf#section.3)
    value: Input candidate (image, caption) pairs pass through several stages of
      filtering and processing to ensure quality.
  access:
    explanation: Dataset can be downloaded at [[Download]](https://ai.google.com/research/ConceptualCaptions/download)
    value: open
  license:
    explanation: "[[Conceptual Captions License]](https://github.com/google-research-datasets/conceptual-captions/blob/master/LICENSE)\n"
    value: Conceptual Captions License
  intended_uses: ''
  prohibited_uses: unknown
  monitoring: unknown
  feedback: Feedback can be provided by creating an issue in the [[Conceptual Captions
    GitHub repository]](https://github.com/google-research-datasets/conceptual-captions)
    or by emailing at conceptual-captions at google.com
- type: dataset
  name: Conceptual 12M
  organization: Google
  description: "A dataset with 12 million image-text pairs specifically meant to\
    \ be used for vision-and-language pre-training.\n"
  created_date:
    explanation: "The date the [[paper]](https://arxiv.org/abs/2102.08981) was submitted.\n"
    value: 2021-02-17
  url: https://arxiv.org/pdf/2102.08981.pdf
  datasheet: none
  modality: image, text
  size: 12M (image, text) pairs
  sample: []
  analysis:
    explanation: See [[Evaluating Vision-and-Language PreTraining Data]](https://arxiv.org/pdf/2102.08981.pdf#section.3)
    value: "The dataset is benchmarked against CC3M on two most fundamental V+L\
      \ tasks: vision-to-language generation and vision-and-language matching, with\
      \ an emphasis on long-tail visual recognition. The results illustrate the\
      \ benefit of scaling up pre-training data for vision-and-language tasks, as\
      \ indicated by the new state-of-the-art results on both the nocaps and Conceptual\
      \ Captions benchmarks.\n"
  dependencies: []
  included: ''
  excluded:
    explanation: See [[Relaxing filters for higher recall]](https://arxiv.org/pdf/2102.08981.pdf#subsection.2.2)
    value: "Some of the filtering steps used in the preparation of Conceptual Captions\
      \ dataset are relaxed to trade off high-recall for low-precision. The following\
      \ steps are applied in the given order:\n1. Image-based Filtering - Only keep\
      \ JPEG images where both dimensions are greater than 400 pixels, and the ratio\
      \ of larger to smaller dimension is no more than 2.5. Exclude images that\
      \ trigger pornography or profanity detectors.\n2. Text-based Filtering - Allow\
      \ text between 3 and 256 words in the alt-text. Discard candidates with no\
      \ noun or no determiner, but permit ones without prepositions. Set the maximum\
      \ fraction of word repetition allowed to 0.2. Increase the threshold for counting\
      \ a word type as rare from 5 to 20.\n3. Image&Text-based Filtering - Filter\
      \ out candidates for which none of the text tokens can be mapped to the content\
      \ of the image.\n"
  quality_control:
    explanation: See [[Vision-and-Language Pre-Training Data]](https://arxiv.org/pdf/2102.08981.pdf#section.2)
    value: Input candidate (image, caption) pairs pass through several stages of
      filtering and processing to ensure quality. Person-name substitutions are
      performed in the alt-texts to protect the privacy of individuals in the associated
      images.
  access:
    explanation: Dataset is available at [[Conceptual 12M GitHub repository]](https://github.com/google-research-datasets/conceptual-12m).
    value: open
  license:
    explanation: "[[Conceptual Captions License]](https://github.com/google-research-datasets/conceptual-captions/blob/master/LICENSE)\n"
    value: Conceptual Captions License
  intended_uses: ''
  prohibited_uses: unknown
  monitoring: unknown
  feedback: Feedback can be provided by creating an issue in the [[Conceptual 12M
    GitHub repository]](https://github.com/google-research-datasets/conceptual-12m)
    or by emailing at conceptual-captions at google.com
- type: model
  name: T5
  organization: Google
  description: Text-To-Text Transfer Transformer (T5) is a model that unifies all
    NLP tasks under the text-to-text format.
  created_date:
    explanation: The date the T5 paper was made public.
    value: 2019-10-23
  url: https://arxiv.org/abs/1910.10683
  model_card: https://huggingface.co/t5-base
  modality: text; text
  analysis: https://huggingface.co/t5-base#evaluation
  size: 11B parameters (dense)
  dependencies: [C4]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 1,024 TPU v3 chips (Cloud TPU Pods)
  quality_control: The T5 paper documents many analyses/ablations that were considered
    before arriving at the final architecture/training procedure.
  access:
    explanation: https://huggingface.co/t5-large
    value: open
  license: Apache 2.0
  intended_uses: NLP tasks
  prohibited_uses: unknown
  monitoring: none
  feedback: https://huggingface.co/t5-large/discussions
- type: model
  name: Internal Google BERT
  organization: Google
  description: "Internal Google BERT model used to power Google Search products.\n"
  created_date:
    explanation: "The date of the Google product update blog announcing that BERT\
      \ models were for ranking and featured snippets in Search.\n"
    value: 2019-11-25
  url: https://blog.google/products/search/search-language-understanding-bert/
  model_card: unknown
  modality: text; text
  analysis: unknown
  size: unknown
  dependencies: [Internal Google BERT dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: closed
  license: unknown
  intended_uses:
    explanation: "We don't have an exhaustive list of the intended use cases for\
      \ the Internal Google BERT model, but we know that Google Search was powered\
      \ by a fine-tuned BERT.\n"
    value: unknown
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
- type: application
  name: Google Search
  organization: Google
  description: "Google Search is Google's search engine.\n"
  created_date:
    explanation: "The date of the Google product update blog announcing that BERT\
      \ models were for ranking and featured snippets in Search.\n"
    value: 2019-11-25
  url: https://blog.google/products/search/search-language-understanding-bert/
  dependencies: [Internal Google BERT, MUM]
  adaptation: unknown
  output_space: web page ranking
  quality_control: unknown
  access: open
  license: none
  terms_of_service: https://policies.google.com/terms
  intended_uses: Searching the web using text, voice or image
  prohibited_uses: "Prohibited use cases aren't specifically spelled out for Google\
    \ search, but several illegal and discouraged use cases are shared in the Respect\
    \ Others section of the [[Term of Service]](https://policies.google.com/terms).\n"
  monitoring: "It is implied that Google scan uses of its products for spam, malware\
    \ and illegal content in the [[Term of Service]](https://policies.google.com/terms).\n"
  feedback: "Feedback can be sent to Google Feedback using the product interface\
    \ [[Google Feedback]](https://www.google.com/tools/feedback).\n"
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: dataset
  name: Infiniset
  organization: Google
  description: "Infiniset \"is a combination of dialog data from public dialog data\
    \ and other public web documents\" [[Appendix E]](https://arxiv.org/pdf/2201.08239.pdf#appendix.E).\n"
  created_date:
    explanation: "The date of the Google company news blog announcing LaMDA [[Google\
      \ News Blog]](https://blog.google/technology/ai/lamda/).\n"
    value: 2021-06-18
  url: https://arxiv.org/pdf/2201.08239.pdf
  datasheet: none
  modality: code, text
  size:
    explanation: "The size of the dataset is unclear, but it is reported that the\
      \ dataset \"consists of 2.97B documents and 1.12B dialogs with 13.39B utterances\"\
      \ [[Appendix E]](https://arxiv.org/pdf/2201.08239.pdf#appendix.E).\n"
    value: unknown
  sample: []
  analysis: unknown
  dependencies: []
  included: "Included in the dataset are data from \"public forums (0%); C4 data\
    \ (12.5% ); code documents from sites related to programming like Q&A sites\
    \ tutorials, etc (12.5%); Wikipedia (English) (12.5%); English web documents\
    \ (6.25%); and Non-English web documents (6.25%).\"\n"
  excluded: unknown
  quality_control: unknown
  access: closed
  license: unknown
  intended_uses:
    explanation: "Intended uses of the dataset wasn't explicitly linked, but it\
      \ is likely intended for training language models specialized in dialogue.\n"
    value: unknown
  prohibited_uses: "The prohibited uses for Infiniset weren't specifically listed,\
    \ but the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1)\
    \ advises avoiding harm, unjust impact and misinformation, among others.\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information was not provided.\n"
    value: none
- type: model
  name: LaMDA
  organization: Google
  description: "LaMDA stands for Language Models for Dialog Application. It is a\
    \ transformer based language model trained on dialogue data.\n"
  created_date:
    explanation: "The date of the Google company news blog announcing LaMDA [[Google\
      \ News Blog]](https://blog.google/technology/ai/lamda/).\n"
    value: 2021-06-18
  url: https://arxiv.org/pdf/2201.08239.pdf
  model_card: none
  modality: text; text
  analysis: "The model performance was analyzed on sensibleness, specificity and\
    \ interestingness. The model was also analyzed on safety, following metrics\
    \ derived from Google AI Principles [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1).\
    \ Finally, the model was analyzed on groundedness, testing its ability to produce\
    \ responses that can be associated with \"known sources whenever possible [[Section\
    \ 4.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.4.1).\"\n"
  size: 137B parameters (dense)
  dependencies: [Infiniset]
  training_emissions:
    explanation: "\"...total carbon footprint of LaMDA’s pre-training of the largest\
      \ model is approximately 25.2 tCO2e. The carbon footprint of pre-training\
      \ of smaller models and fine-tuning of all models is approximately 0.7 tCO2e\
      \ ... which brings the total footprint of LaMDA to approximately 26 tCO2e\
      \ [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10)\"\n"
    value: 26 tCO2e
  training_time:
    explanation: "The total number of training flops of LaMDA was reported as 3.55E+23\
      \ (3.55E+8 petaflops) [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10),\
      \ which is equal to 4108.80 = 3.55E+8 / (60 * 60 * 24) petaflop/s-day.\n"
    value: 4108.80 petaflop/s-day
  training_hardware:
    explanation: "Reported in [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10).\n"
    value: 1024 TPU-V3 chips
  quality_control: "LaMDA was fine-tuned to predict sensibleness, specificity and\
    \ interestingness as well as safety. Then, the candidates were filtered out\
    \ if the model safety predictions were below a certain threshold. The next candidates\
    \ in the conversation were selected as a combination of these predictions. The\
    \ model was also fine-tuned for groundedness. The results are shown in [[Figure\
    \ 5]](https://arxiv.org/pdf/2201.08239.pdf#figure.caption.23).\n"
  access: closed
  license: unknown
  intended_uses: "LaMDA is a language model, so it can be used for regular langauge\
    \ modelling tasks without fine-tuning, but its fine-tuned for dialogue tasks.\n"
  prohibited_uses: "The prohibited uses of LaMDA weren't specifically listed, but\
    \ the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1)\
    \ advises avoiding harm, unjust impact and misinformation, among others.\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information was not provided.\n"
    value: none
- type: dataset
  name: PaLM dataset
  organization: Google
  description: "PaLM dataset \"was created for pre-training language models\" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\n"
  created_date:
    explanation: "The date of the Google AI blog announcing the details of PaLM\
      \ [[Google AI Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).\n"
    value: 2022-04-04
  url: https://arxiv.org/pdf/2204.02311.pdf
  datasheet: https://arxiv.org/pdf/2204.02311.pdf#appendix.D
  modality: code, text
  size:
    explanation: "Dataset size in GB is not reported, but the dataset is reported\
      \ to have 780 billion tokens [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\
      \ The code portion of the dataset is reported to be 5% totaling a 196GB of\
      \ source code [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\
      \ It is unclear whether the reported size is before or after de-duplication.\
      \ Nonetheless, one can estimate the dataset size by multiplying 196GB with\
      \ 20 = 3.92 TB.\n"
    value: 3.92 TB
  sample: []
  analysis: unknown
  dependencies: [Infiniset]
  included: "The dataset is based on Infiniset. It included multilingual text containing\
    \ text from over 100 languages. The breakdown of the data included is as follows:\
    \ Social media conversations (multilingual) 50, Filtered webpages (multilingual)\
    \ 27%, BooksCorpus (English) 13%, GitHub (code) 5%, Wikipedia (multilingual)\
    \ 4%, and News (English) 1%. Code was collected from GitHub repositories with\
    \ appropriate licenses, totalling 96GB of source code [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\n"
  excluded: "GitHub repositories with copyleft licenses were excluded. Programming\
    \ languageges other than the most common 24 were excluded [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\n"
  quality_control: "In order to reduce low quality web pages, the web pages were\
    \ sampled according to a \"quality score\" classifier. Code files were de-duplicated\
    \ using Levenshtein distance [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).\n"
  access: closed
  license: unknown
  intended_uses:
    explanation: "As stated in [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\n"
    value: "\"The dataset was created for pre-training language models by a team\
      \ of researchers at Google\".\n"
  prohibited_uses: "\"... should not be used for any of the unacceptable language\
    \ model use cases, e.g., generation of toxic speech\" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information is shared in the paper [[Paper]](https://arxiv.org/pdf/2204.02311.pdf).\n"
    value: Contact the authors.
- type: model
  name: Flan-T5
  organization: Google
  description: Flan-T5 is a version of the T5 language model fine-tuned on instruction
    data
  created_date:
    explanation: Date paper was released
    value: 2022-10-20
  url: https://arxiv.org/abs/2210.11416
  model_card: https://arxiv.org/pdf/2210.11416.pdf
  modality: text; text
  analysis: Evaluated on a variety of standard language datasets.
  size: 11B parameters (dense)
  dependencies: [T5, Muffin, P3, NaturalInstructions-v2, Flan CoT]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: 512 v4 TPU Chips
  quality_control: Across different multitask datasets, templates and formatting
    were maintained. For the chain-of-thoughts (CoT) data, specific exemplars were
    used.
  access:
    explanation: Weights can be downloaded from [Github](https://github.com/google-research/t5x/blob/main/docs/models.md)
    value: open
  license:
    explanation: License on the [[Github repository]](https://github.com/google-research/google-research)
    value: Apache 2.0
  intended_uses: unknown
  prohibited_uses: none
  monitoring: none
  feedback: https://huggingface.co/google/flan-t5-xxl/discussions
- type: model
  name: UL2
  organization: Google
  description: UL2 is a language model trained with a new pretraining objective
  created_date:
    explanation: Date model paper was released
    value: 2022-05-10
  url: https://arxiv.org/abs/2205.05131
  model_card: ''
  modality: text; text
  analysis: ''
  size: 20B parameters (dense)
  dependencies: [C4]
  training_emissions: ''
  training_time: ''
  training_hardware: 128 TPUv4
  quality_control: ''
  access:
    explanation: Model weights available for download in the [[Github repo]](https://github.com/google-research/google-research/tree/master/ul2)
    value: open
  license:
    explanation: 20B checkpoints only for three different iteration steps
    value: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Parti
  organization: Google
  description: Parti is a text-to-image diffusion model
  created_date:
    explanation: Date the model website was made public
    value: 2022-06-22
  url: https://parti.research.google/
  model_card: ''
  modality: text; image
  analysis: ''
  size: 20B parameters (dense)
  dependencies: [C4, LAION-400M, FIT400M, JFT-4B]
  training_emissions: ''
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access:
    explanation: Google does not provide access to Parti for external researchers.
    value: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Imagen
  organization: Google
  description: Imagen is a text-to-image diffusion model
  created_date:
    explanation: Date the model website was made public
    value: 2022-05-23
  url: https://imagen.research.google/
  model_card: ''
  modality: text; image
  analysis: ''
  size: 14B parameters (dense)
  dependencies: [LAION-400M, Google internal image-text dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 128 TPU-v4
  quality_control: ''
  access: open
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: VATT
  organization: Google
  description: VATT is a family of models trained on multimodal data
  created_date:
    explanation: Date the model paper was made public
    value: 2022-04-22
  url: https://arxiv.org/abs/2104.11178
  model_card: ''
  modality: text; audio, video
  analysis: ''
  size: 155M parameters (dense)
  dependencies: [AudioSet, HowTo100M]
  training_emissions: unknown
  training_time: 3 days
  training_hardware: 256 TPU-v3
  quality_control: ''
  access:
    explanation: Model checkpoints can be downloaded from the [[Github repository]](https://github.com/google-research/google-research/tree/master/vatt)
    value: open
  license:
    explanation: License on the [[Github repository]](https://github.com/google-research/google-research)
    value: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLM
  organization: Google
  description: "PaLM stands Pathways Language Model, \"dense decoder-only Transformer\
    \ model trained with the Pathways system\" [[Google ai Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).\n"
  created_date:
    explanation: "The date of the Google AI blog announcing the details of PaLM\
      \ [[Google AI Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).\n"
    value: 2022-04-04
  url: https://arxiv.org/pdf/2204.02311.pdf
  model_card: https://arxiv.org/pdf/2204.02311.pdf#appendix.E
  modality: text; text, code
  analysis: "\"PaLM is evaluated on English Natural Language Processing (NLP) tasks,\
    \ tasks from BIG-bench, reasoning tasks, code completion tasks, multilingual\
    \ generation and question answering tasks, translation tasks, and bias and toxicity\
    \ benchmarks\" [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).\n"
  size: 540B parameters (dense)
  dependencies: [PaLM dataset]
  training_emissions:
    explanation: "Reported in [[Appendix B]](https://arxiv.org/pdf/2204.02311.pdf#appendix.A)\n"
    value: 271.43 tCO2
  training_time:
    explanation: "Reported in [[Appendix B]](https://arxiv.org/pdf/2204.02311.pdf#appendix.A)\n"
    value: 29600 petaflop/s-days
  training_hardware:
    explanation: "Reported in [[Section 4]](https://arxiv.org/pdf/2204.02311.pdf#section.4).\n"
    value: 6144 TPU v4 chips
  quality_control: Unknown
  access:
    explanation: Made available through the PaLM API as of March 14, 2023.
    value: limited
  license: unknown
  intended_uses: "\"The primary use is research on language models, including: research\
    \ on NLP applications like machine translation and question answering, advancing\
    \ fairness and safety research, and understanding limitations of current LLMs.\
    \ Within Google, PaLM is being used for research on a variety of open- ended\
    \ text and code generation tasks, including reasoning [[Section 6.3]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.3)\
    \ and code synthesis and understanding [[Section 6.4]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.4)\"\
    \ [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).\n"
  prohibited_uses: "The model \"should not be used for downstream applications without\
    \ further analysis on factors in the proposed downstream application [[Model\
    \ Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E)\"\n"
  monitoring: unknown
  feedback:
    explanation: "Author contact information is shared in the paper [[Paper]](https://arxiv.org/pdf/2204.02311.pdf).\n"
    value: Contact the authors.
- type: application
  name: PaLM API
  organization: Google
  description: a new developer offering that makes it easy and safe to experiment
    with Google’s language models.
  created_date: 2023-03-14
  url: https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html
  dependencies: [PaLM]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license: unknown
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: Med-PaLM
  organization: Google
  description: ''
  created_date: 2022-12-26
  url: https://arxiv.org/abs/2212.13138
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [Flan-PaLM, MultiMedQA]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Med-PaLM Multimodal
  organization: Google
  description: ''
  created_date: 2023-07-26
  url: https://arxiv.org/pdf/2307.14334.pdf
  model_card: ''
  modality: image, text, genome sequence; text
  analysis: Evaluated on MultiMedBench tasks and radiologist evaluations of model-generated
    chest X-ray reports
  size: 562B parameters (dense)
  dependencies: [PaLM-E, MultiMedBench]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MultiMedQA
  organization: Google
  description: ''
  created_date: 2022-12-26
  url: https://arxiv.org/abs/2212.13138
  model_card: ''
  modality: text; text
  analysis: ''
  size: unknown
  dependencies:
    - MedQA
    - MedMCQA
    - PubMedQA
    - MMLU
    - LiveQA
    - Medication QA
    - HealthSearchQA
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Flan-PaLM
  organization: Google
  description: ''
  created_date: 2022-10-20
  url: https://arxiv.org/abs/2210.11416
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [PaLM, Muffin, P3, NaturalInstructions-v2]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Flan-U-PaLM
  organization: Google
  description: ''
  created_date: 2022-10-20
  url: https://arxiv.org/abs/2210.11416
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [U-PaLM, Muffin, P3, NaturalInstructions-v2]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Muffin
  organization: Google
  description: ''
  created_date: 2021-09-03
  url: https://arxiv.org/abs/2109.01652
  datasheet: ''
  modality: text
  size: 62 tasks
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: U-PaLM
  organization: Google
  description: ''
  created_date: 2022-10-20
  url: https://arxiv.org/abs/2210.11399
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [PaLM, PaLM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLM-SayCan
  organization: Google
  description: ''
  created_date: 2022-08-16
  url: https://arxiv.org/abs/2204.01691
  model_card: ''
  modality: text; robotics trajectories
  analysis: ''
  size: 540B parameters (dense)
  dependencies: [PaLM]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown (model weights), Apache 2.0 (SayCan code)
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: GLaM
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  model_card: ''
  modality: text; text
  analysis: ''
  size: 1.2T parameters (sparse)
  dependencies:
    - GLaM Web dataset
    - Wikipedia
    - GLaM Conversations dataset
    - GLaM Forums dataset
    - BooksCorpus
    - GLaM News dataset
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM Web dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM Conversations dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM Forums dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: GLaM News dataset
  organization: Google
  description: ''
  created_date: 2021-12-13
  url: https://arxiv.org/abs/2112.06905
  datasheet: ''
  modality: text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MUM
  organization: Google
  description: MUM (Multitask Unified Model) is a multimodal model that is specialized
    for more complex queries.
  created_date: 2021-05-18
  url: https://blog.google/products/search/introducing-mum/
  model_card: ''
  modality: image, text; text
  analysis: ''
  size: unknown
  dependencies: [MUM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: MUM dataset
  organization: Google
  description: ''
  created_date: 2021-05-18
  url: https://blog.google/products/search/introducing-mum/
  datasheet: ''
  modality: image, text
  size: unknown
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Phenaki
  organization: Google
  description: ''
  created_date: 2023-02-01
  url: https://openreview.net/pdf?id=vOEXS39nOF
  model_card: ''
  modality: text; video
  analysis: ''
  size: 1.8B parameters (dense)
  dependencies: [LAION-400M, Phenaki Video-Text Corpus]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Phenaki Video-Text Corpus
  organization: Google
  description: ''
  created_date: 2023-02-01
  url: https://openreview.net/pdf?id=vOEXS39nOF
  datasheet: ''
  modality: text, video
  size: 15M text-video pairs at 8FPS
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Flan-UL2
  organization: Google
  description: ''
  created_date: 2023-03-02
  url: https://arxiv.org/abs/2205.05131
  model_card: ''
  modality: text; text
  analysis: ''
  size: 20B parameters (dense)
  dependencies: [UL2, Flan Collection]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Flan Collection
  organization: Google
  description: ''
  created_date: 2023-01-31
  url: https://arxiv.org/abs/2301.13688
  datasheet: ''
  modality: text
  size: 1836 tasks
  sample: []
  analysis: ''
  dependencies: [Flan dataset, P3, NaturalInstructions-v2]
  included: ''
  excluded: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MusicLM
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: text; audio
  analysis: ''
  size: 1.4B parameters (dense)
  dependencies:
    - SoundStream
    - w2v-BERT
    - MuLan
    - MusicLM semantic model
    - MusicLM acoustic model
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: SoundStream
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: unknown
  dependencies: [Free Music Archive]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: w2v-BERT
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: 600M parameters (dense)
  dependencies: [Free Music Archive]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MuLan
  organization: Google
  description: ''
  created_date: 2022-08-26
  url: https://arxiv.org/abs/2208.12415
  model_card: ''
  modality: text; audio
  analysis: ''
  size: unknown
  dependencies: [AST, BERT, MuLan dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: MuLan dataset
  organization: Google
  description: ''
  created_date: 2022-08-26
  url: https://arxiv.org/abs/2208.12415
  datasheet: ''
  modality: audio, text
  size: 370K hours audio
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: MusicLM dataset
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  datasheet: ''
  modality: audio
  size: 280K hours audio
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MusicLM semantic model
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: 430M parameters (dense)
  dependencies: [MusicLM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: MusicLM acoustic model
  organization: Google
  description: ''
  created_date: 2023-01-26
  url: https://arxiv.org/pdf/2301.11325.pdf
  model_card: ''
  modality: audio; audio
  analysis: ''
  size: 430M parameters (dense)
  dependencies: [MusicLM dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Noise2Music
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  model_card: ''
  modality: audio, text; audio
  analysis: ''
  size: unknown
  dependencies: [Noise2Music pseudolabel dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknkown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: LaMDA-LF
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: text
  size: 150k songs
  sample: []
  analysis: ''
  dependencies: [LaMDA]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Rater-LF
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: text
  size: 10k captions
  sample: []
  analysis: ''
  dependencies: [MusicCaps]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Rater-SF
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: text
  size: 24k captions
  sample: []
  analysis: ''
  dependencies: [MusicCaps]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Noise2Music pseudolabeler
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  model_card: ''
  modality: audio, text; audio
  analysis: ''
  size: unknown
  dependencies: [MuLan, MuLaMCap, LaMDA-LF, Rater-LF, Rater-SF]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Noise2Music audio dataset
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: audio
  size: 340k hours audio
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Noise2Music pseudolabel dataset
  organization: Google
  description: ''
  created_date: 2023-02-08
  url: https://google-research.github.io/noise2music/noise2music.pdf
  datasheet: ''
  modality: audio, text
  size: 340k hours audio with pseudolabels
  sample: []
  analysis: ''
  dependencies: [Noise2Music audio dataset, Noise2Music pseudolabeler]
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license:
    explanation: "The asset isn't released, and hence the license is unknown.\n'"
    value: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: AI Test Kitchen
  organization: Google
  description: AI Test Kitchen provides a new way for people to learn about, experience,
    and give feedback on emerging AI technology, like LaMDA.
  created_date: 2022-08-25
  url: https://blog.google/technology/ai/join-us-in-the-ai-test-kitchen/
  dependencies: [LaMDA]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license: unknown
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: application
  name: Bard
  organization: Google
  description: Conversational AI service, powered by LaMDA
  created_date: 2023-02-06
  url: https://blog.google/technology/ai/bard-google-ai-search-updates/
  dependencies: [LaMDA]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: closed
  license: unknown
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: Minerva
  organization: Google
  description: ''
  created_date: 2022-06-29
  url: https://arxiv.org/abs/2206.14858
  model_card: ''
  modality: text; text
  analysis: ''
  size: 540B parameters (dense)
  dependencies:
    - PaLM
    - arXiv
    - PaLM dataset
    - Minerva Math Web Pages dataset
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: Minerva Math Web Pages dataset
  organization: Google
  description: ''
  created_date: 2022-06-29
  url: https://arxiv.org/abs/2206.14858
  datasheet: ''
  modality: text
  size: 17.5B tokens
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: USM
  organization: Google
  description: Universal Speech Model (USM) is a family of state-of-the-art speech
    models with 2B parameters trained on 12 million hours of speech and 28 billion
    sentences of text, spanning 300+ languages. USM, which is for use in YouTube
    (e.g., for closed captions), can perform automatic speech recognition (ASR)
    on widely-spoken languages like English and Mandarin, but also languages like
    Punjabi, Assamese, Santhali, Balinese, Shona, Malagasy, Luganda, Luo, Bambara,
    Soga, Maninka, Xhosa, Akan, Lingala, Chichewa, Nkore, Nzema to name a few. Some
    of these languages are spoken by fewer than twenty million people, making it
    very hard to find the necessary training data.
  created_date: 2023-03-06
  url: https://arxiv.org/abs/2303.01037
  model_card: ''
  modality: audio, text; text
  analysis: ''
  size: 2B parameters (dense)
  dependencies: [YT-NLU-U, Pub-U, Web-NTL, YT-SUP+, Pub-S]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: limited
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: YouTube
  organization: Google
  description: YouTube is a global online video sharing and social media platform
  created_date: 2005-02-14
  url: https://www.youtube.com/
  dependencies: [USM]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: open
  license: ''
  terms_of_service: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: PaLM-E
  organization: Google
  description: ''
  created_date: 2023-03-06
  url: https://arxiv.org/abs/2303.03378
  model_card: ''
  modality: image, text; text
  analysis: ''
  size: 562B parameters (dense)
  dependencies: [PaLM, ViT-22B]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: ViT-22B
  organization: Google
  description: ''
  created_date: 2023-02-10
  url: https://arxiv.org/abs/2302.05442
  model_card: ''
  modality: image, image
  analysis: ''
  size: 22B parameters (dense)
  dependencies: [JFT]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: AudioLM
  organization: Google
  description: ''
  created_date: 2022-09-07
  url: https://arxiv.org/abs/2209.03143
  model_card: ''
  modality: audio, text; audio
  analysis: ''
  size: 1B parameters (dense)
  dependencies: [w2v-BERT, SoundStream]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLI
  organization: Google
  description: ''
  created_date: 2022-09-14
  url: https://arxiv.org/abs/2209.06794
  model_card: ''
  modality: text; image
  analysis: ''
  size: 17B parameters (dense)
  dependencies: [mT5, ViT-e, WebLI]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: ViT-e
  organization: Google
  description: ''
  created_date: 2022-09-14
  url: https://arxiv.org/abs/2209.06794
  model_card: ''
  modality: image; image
  analysis: ''
  size: 3.9B parameters (dense)
  dependencies: [JFT]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: dataset
  name: WebLI
  organization: Google
  description: ''
  created_date: 2022-09-14
  url: https://arxiv.org/abs/2209.06794
  datasheet: ''
  modality: image, text
  size: 10B images, 12B alt-text
  sample: []
  analysis: ''
  dependencies: []
  included: ''
  excluded: ''
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Vid2Seq
  organization: Google
  description: ''
  created_date: 2023-02-27
  url: https://arxiv.org/abs/2302.14115
  model_card: ''
  modality: video; text
  analysis: ''
  size: 500M parameters (dense)
  dependencies: [T5, CLIP, YT-Temporal-1B]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Google Joint SLM
  organization: Google
  description: Joint speech and language model using a Speech2Text adapter and using
    a CTC-based blank-filtering.
  created_date: 2023-06-08
  url: https://arxiv.org/pdf/2306.07944.pdf
  model_card: ''
  modality: audio; text
  analysis: evaluated on DSTC11 Challenge Task, based on MultiWoz 2.1, with a focus
    on dialog state tracking.
  size: unknown
  dependencies: [CTC blank-filtering, Speech2Text adapter]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access: open
  license: Google
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: PaLM 2
  organization: Google
  description: PaLM 2 is a new state-of-the-art language model that has better multilingual
    and reasoning capabilities and is more compute-efficient than its predecessor
    PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives
    similar to UL2.
  created_date: 2023-05-10
  url: https://blog.google/technology/ai/google-palm-2-ai-large-language-model/
  model_card: https://ai.google/static/documents/palm2techreport.pdf
  modality: text; text
  analysis: Reports results on standard code benchmarks across a variety of programming
    languages.
  size: unknown
  dependencies: [PaLM 2 dataset]
  training_emissions: ''
  training_time: ''
  training_hardware: TPU v4 (number unspecified)
  quality_control: Employed de-duplication, removal of sensitive-PII and filtering.
    Added control tokens marking toxicity of text.
  access: open
  license: unknown
  intended_uses: general use large language model that can be used for language,
    reasoning, and code tasks.
  prohibited_uses: becoming part of a general-purpose service or product or use
    within specific downstream applications without prior assessment
  monitoring: Google internal monitoring
  feedback: Specific queries provided by annotators
- type: model
  name: Gemini
  organization: Google
  description: As of release, Gemini is Google's most capable and flexible AI model,
    proficient in multimodal domains.
  created_date: 2023-12-06
  url: https://deepmind.google/technologies/gemini/#introduction
  model_card: none
  modality: text; image, text, video
  analysis: Evaluated on standard general, reasoning, math, coding, and multimodal
    benchmarks with results that surpass GPT-4 on almost all.
  size:
    explanation: Comes in sizes Ultra, Pro, and Nano.
    value: unknown
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: general use large language model that can be used for language,
    reasoning, and code tasks.
  prohibited_uses: becoming part of a general-purpose service or product or use
    within specific downstream applications without prior assessment
  monitoring: Google internal monitoring
  feedback: none
