---
- type: application
  name: Robin AI
  organization: Robin AI
  description: Robin AI uses Claude and Anthropic's models to understand language
    - including in technical domains like legal language. It's also very confident
    at drafting, summarising, translations, and explaining complex concepts in simple
    terms
  created_date: unknown
  url: https://www.robinai.co.uk/
  dependencies: [Anthropic API]
  adaptation: ''
  output_space: ''
  quality_control: ''
  access: limited
  license: none
  terms_of_service: https://www.robinai.co.uk/terms
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
  monthly_active_users: ''
  user_distribution: ''
  failures: ''
- type: model
  name: Voice Safety Classifier
  organization: Roblox
  description: A large classification model trained to detect and classify toxicity, profanity, sexting, racist content, and bullying in voice chats. The model is trained on 2,374 hours of voice chat audio clips, and it outputs a tensor indicating the likelihood of each class. Its evaluation on a dataset of 9,795 human-annotated samples achieved an average binary precision of 94.48% across the four toxicity classes.
  created_date: Unknown
  url: https://huggingface.co/Roblox/voice-safety-classifier
  model_card: https://huggingface.co/Roblox/voice-safety-classifier
  modality: Audio; Tensor
  analysis: The model was evaluated on a dataset consisting of 9,795 human annotated labels, with class distributions outlined for each potential violation category. It achieved an average binary precision of 94.48% across four toxicity classes.
  size: Unknown
  dependencies: ["WavLM base plus", "voice chat audio clips dataset"]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The model was evaluated on a data set of human-annotated labels and achieved high precision in classifying diverse categories of violations. 
  access: Open (Available on HuggingFace under roblox/voice-safety-classifier)
  license: Unknown
  intended_uses: The model can be used to analyze voice chat data for potential violations, including profanity, sexting, racism, and bullying, helping maintain safety and compliance in online environments.
  prohibited_uses: It should not be used to infringe on users' privacy rights, or utilized in contexts where the "Other" category of policy violation (which includes low prevalence categories such as drugs and alcohol or self-harm) is crucially important, as these are combined into a single category in this model.
  monitoring: Unknown
  feedback: Unknown.
