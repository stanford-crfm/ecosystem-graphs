---
- type: model
  name: BigTrans
  organization: Institute of Automation Chinese Academy of Sciences
  description: BigTrans is a model which adapts LLaMA that covers only 20 languages
    and enhances it with multilingual translation capability on more than 100 languages
  created_date: 2023-05-29
  url: https://arxiv.org/pdf/2305.18098v1.pdf
  model_card: https://huggingface.co/James-WYang/BigTrans
  modality: text; text
  analysis: Reports results on standard translation benchmarks across 102 languages
    in comparison with Google Translate and ChatGPT
  size: 13B parameters (dense)
  dependencies: [LLaMA, CLUE, BigTrans parallel dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: 16 A100 GPUs with 80 GB of RAM
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: Advancing future research in multilingual LLMs
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/James-WYang/BigTrans/discussions
- type: model
  name: YAYI 2
  organization: Institute of Automation Chinese Academy of Sciences
  description: YAYI 2 is an open source large language model trained in both English
    and Chinese.
  created_date: 2023-12-22
  url: https://arxiv.org/pdf/2312.14862.pdf
  model_card: https://huggingface.co/wenge-research/yayi2-30b
  modality: text; text
  analysis: Evaluated on standard benchmarks for knowledge and language understanding,
    mathematical reasoning, and programming ability in comparison to similarly sized
    open-source models.
  size: 30B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: over 1000 A800 GPUs
  quality_control: data is deduplicated, normalized, cleaned, and filtered for toxicity
  access: open
  license:
    explanation: Model is under a custom [license](https://github.com/wenge-research/YAYI2/blob/main/COMMUNITY_LICENSE),
      while code is Apache 2.0
    value: custom
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: https://huggingface.co/wenge-research/yayi2-30b/discussions
- type: model
  name: AstroPT
  organization: Aspia Space, Instituto de Astrofísica de Canarias (IAC), UniverseTBD, Astrophysics Research Institute, Liverpool John Moores University, Departamento Astrofísica, Universidad de la Laguna, Observatoire de Paris, LERMA, PSL University, Universit´e Paris-Cit´e
  description: AstroPT is an autoregressive pretrained transformer developed with astronomical use-cases in mind. The models are trained on 8.6 million 512x512 pixel grz-band galaxy postage stamp observations from the DESI Legacy Survey DR8. The training resulted in the creation of foundation models ranging in size from 1 million to 2.1 billion parameters. It is a step towards creating a 'Large Observation Model' – a model trained on data from observational sciences at a scale similar to natural language processing models.
  created_date: Unknown.
  url: https://arxiv.org/pdf/2405.14930v1
  model card: https://arxiv.org/pdf/2405.14930v1
  modality: Image; Image.
  analysis: The models' performance on downstream tasks, as measured by linear probing, was found to improve with model size up to a certain saturation point.
  size: The model ranges from 1 million to 2.1 billion parameters. Given that the specific mention of the models being a Mixture of Experts or sparse in the information provided, we can't affirm about the model's sparsity.
  dependencies: [DESI Legacy Survey DR8 Dataset].
  training_emissions: Unknown.
  training_time: Unknown.
  training_hardware: Unknown.
  quality_control: The models underwent linear probing to measure performance and identify the parameter saturation point beyond which size no longer improves performance.
  access: Open. The source code, weights, and dataset for AstroPT have been released under the MIT license.
  license: MIT.
  intended_uses: Developed with astronomical use-cases in mind. The models can be utilized to extract meaningful information from astronomical observations.
  prohibited_uses: Unknown.
  monitoring: Description of measures taken to monitor downstream uses of this model is not mentioned in the provided information.
  feedback: Potential collaborators and users are invited to join the research activities surrounding these models. It can be inferred that any feedback or issues can be reported to Michael J. Smith (mike@mjjsmith.com).
