---
- type: model
  name: Dolly
  organization: Databricks
  description: "\"Databricksâ€™ Dolly, a large language model trained on the Databricks\n\
    \ Machine Learning Platform, demonstrates that a two-years-old open source\n\
    \ model (GPT-J) can, when subjected to just 30 minutes of fine tuning on a\n\
    \ focused corpus of 50k records (Stanford Alpaca), exhibit surprisingly\n high\
    \ quality instruction following behavior not characteristic of the\n foundation\
    \ model on which it is based.\"\n [[Dolly Repository]](https://github.com/databrickslabs/dolly).\n"
  created_date:
    explanation: "The date the model was announced in the [[Cerebras blog post]](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html).\n"
    value: 2023-03-24
  url: https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html
  model_card: ''
  modality: text; text
  analysis: "\"We evaluated Dolly on the instruction-following capabilities described\
    \ in the InstructGPT paper that ChatGPT is based on and found that it exhibits\
    \ many of the same qualitative capabilities, including text generation, brainstorming\
    \ and open Q&A.\" [[Databricks Blog Post]] (https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html).\n"
  size: 6B parameters (dense)
  dependencies: [GPT-J, Alpaca dataset]
  training_emissions: unknown
  training_time:
    explanation: "According to [[Model Overview]](https://github.com/databrickslabs/dolly#model-overview)\n"
    value: 30 minutes
  training_hardware:
    explanation: "According to [[Model Overview]](https://github.com/databrickslabs/dolly#model-overview)\n"
    value: A single NDasrA100_v4 machine with 8x A100 40GB GPUs
  quality_control: none
  access:
    explanation: "Model training code can be accessed at the official Dolly repository.\
      \ Trained weights can be requested at hello-dolly@databricks.com. [[Dolly\
      \ Repository]](https://github.com/databrickslabs/dolly).\n"
    value: open
  license: Apache 2.0
  intended_uses: "\"Dolly is intended exclusively for research purposes and is not\
    \ licensed for commercial use.\" [[Limitations]](https://github.com/databrickslabs/dolly#limitations).\n"
  prohibited_uses: "Authors note the following limitations of the model: \"The Dolly\
    \ model family is under active development, and so any list of shortcomings\
    \ is unlikely to be exhaustive, but we include known limitations and misfires\
    \ here as a means to document and share our preliminary findings with the community.\
    \ In particular, dolly-6b struggles with syntactically complex prompts, mathematical\
    \ operations, factual errors, dates and times, open-ended question answering,\
    \ hallucination, enumerating lists of specific length, and stylistic mimicry.\"\
    \ [[Limitations]](https://github.com/databrickslabs/dolly#limitations).\n"
  monitoring: none
  feedback: https://github.com/databrickslabs/dolly/issues
- type: model
  name: DBRX
  organization: Databricks
  description: DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token prediction by Databricks. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 36B parameters are active on any input. DBRX only accepts text-based inputs and produces text-based outputs.
  created_date: 2024-03-27
  url: https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm
  model_card: https://huggingface.co/databricks/dbrx-base
  modality: text; text
  analysis: DBRX outperforms established open-source and open-weight base models on the Databricks Model Gauntlet, the Hugging Face Open LLM Leaderboard, and HumanEval. Full evaluation details can be found in the corresponding technical blog post. 
  size: 132B parameters (sparse)
  dependencies: []
  training_emissions: unknown
  training_time: 3 months
  training_hardware: 3072 NVIDIA H100s connected by 3.2Tbps Infiniband
  quality_control: Recommendations provided for retrieval augmented generation (RAG) in scenarios where accuracy and fidelity are important and additional testing around safety in the context of the specific application and domain is suggested. 
  access: open
  license: Databricks Open Model License
  intended_uses: DBRX models are open, general-purpose LLMs intended and licensed for both commercial and research applications. They can be further fine-tuned for various domain-specific natural language and coding tasks. 
  prohibited_uses: DBRX models are not intended to be used out-of-the-box in non-English languages, and do not support native code execution, function calling or any use that violates applicable laws or regulations or is otherwise prohibited by the Databricks Open Model License and Databricks Open Model Acceptable Use Policy. 
  monitoring: unknown
  feedback: https://huggingface.co/databricks/dbrx-base/discussions

