---
- type: model
  name: Dolly
  organization: Databricks
  description: "\"Databricksâ€™ Dolly, a large language model trained on the Databricks\n\
    \ Machine Learning Platform, demonstrates that a two-years-old open source\n\
    \ model (GPT-J) can, when subjected to just 30 minutes of fine tuning on a\n\
    \ focused corpus of 50k records (Stanford Alpaca), exhibit surprisingly\n high\
    \ quality instruction following behavior not characteristic of the\n foundation\
    \ model on which it is based.\"\n [[Dolly Repository]](https://github.com/databrickslabs/dolly).\n"
  created_date:
    explanation: "The date the model was announced in the [[Cerebras blog post]](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html).\n"
    value: 2023-03-24
  url: https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html
  model_card: ''
  modality: text; text
  analysis: "\"We evaluated Dolly on the instruction-following capabilities described\
    \ in the InstructGPT paper that ChatGPT is based on and found that it exhibits\
    \ many of the same qualitative capabilities, including text generation, brainstorming\
    \ and open Q&A.\" [[Databricks Blog Post]] (https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html).\n"
  size: 6B parameters (dense)
  dependencies: [GPT-J, Alpaca dataset]
  training_emissions: unknown
  training_time:
    explanation: "According to [[Model Overview]](https://github.com/databrickslabs/dolly#model-overview)\n"
    value: 30 minutes
  training_hardware:
    explanation: "According to [[Model Overview]](https://github.com/databrickslabs/dolly#model-overview)\n"
    value: A single NDasrA100_v4 machine with 8x A100 40GB GPUs
  quality_control: none
  access:
    explanation: "Model training code can be accessed at the official Dolly repository.\
      \ Trained weights can be requested at hello-dolly@databricks.com. [[Dolly\
      \ Repository]](https://github.com/databrickslabs/dolly).\n"
    value: open
  license: Apache 2.0
  intended_uses: "\"Dolly is intended exclusively for research purposes and is not\
    \ licensed for commercial use.\" [[Limitations]](https://github.com/databrickslabs/dolly#limitations).\n"
  prohibited_uses: "Authors note the following limitations of the model: \"The Dolly\
    \ model family is under active development, and so any list of shortcomings\
    \ is unlikely to be exhaustive, but we include known limitations and misfires\
    \ here as a means to document and share our preliminary findings with the community.\
    \ In particular, dolly-6b struggles with syntactically complex prompts, mathematical\
    \ operations, factual errors, dates and times, open-ended question answering,\
    \ hallucination, enumerating lists of specific length, and stylistic mimicry.\"\
    \ [[Limitations]](https://github.com/databrickslabs/dolly#limitations).\n"
  monitoring: none
  feedback: https://github.com/databrickslabs/dolly/issues
- type: model
  name: DBRX
  organization: Databricks
  description: DBRX is a transformer-based decoder-only large language model (LLM)
    that was trained using next-token prediction by Databricks. It uses a fine-grained
    mixture-of-experts (MoE) architecture with 132B total parameters of which 36B
    parameters are active on any input. DBRX only accepts text-based inputs and
    produces text-based outputs.
  created_date: 2024-03-27
  url: https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm
  model_card: https://huggingface.co/databricks/dbrx-base
  modality: text; text
  analysis: DBRX outperforms established open-source and open-weight base models
    on the Databricks Model Gauntlet, the Hugging Face Open LLM Leaderboard, and
    HumanEval. Full evaluation details can be found in the corresponding technical
    blog post.
  size: 132B parameters (sparse)
  dependencies: []
  training_emissions: unknown
  training_time: 3 months
  training_hardware: 3072 NVIDIA H100s connected by 3.2Tbps Infiniband
  quality_control: Recommendations provided for retrieval augmented generation (RAG)
    in scenarios where accuracy and fidelity are important and additional testing
    around safety in the context of the specific application and domain is suggested.
  access: open
  license: Databricks Open Model License
  intended_uses: DBRX models are open, general-purpose LLMs intended and licensed
    for both commercial and research applications. They can be further fine-tuned
    for various domain-specific natural language and coding tasks.
  prohibited_uses: DBRX models are not intended to be used out-of-the-box in non-English
    languages, and do not support native code execution, function calling or any
    use that violates applicable laws or regulations or is otherwise prohibited
    by the Databricks Open Model License and Databricks Open Model Acceptable Use
    Policy.
  monitoring: unknown
  feedback: https://huggingface.co/databricks/dbrx-base/discussions
- type: model
  name: Meta Llama 3.1
  organization: Databricks in partnership with Meta
  description: The Llama 3.1 is a series of open source models designed for a variety of tasks, including document understanding, metadata extraction, summarization, chatbots, virtual assistants, advanced multi-step reasoning workflows, content generation, research review, brainstorming, and advanced data analysis. It introduces many new capabilities, including high-quality reasoning, steerability, general knowledge, expanded context length of 128k tokens, support across 8 languages, improved tool use and function calling. It also features the LlamaGuard model and Safety Models for secure and responsible deployment.
  created_date: 2024-07-23
  url: https://dbricks.co/4dcJpC3
  model_card: 
  modality: text; text
  analysis: Unknown
  size: The Meta Llama 3.1 model collection includes models that have 8B, 70B, and 405B parameters. The largest such model has 405 billion parameters and the models are presumably dense.
  dependencies: Unknown
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The integration of improved safety models such as LlamaGuard for secure and responsible deployment of models.
  access: Open
  license: Unknown
  intended_uses: The models are intended for use in tasks like document understanding, metadata extraction, summarization, chatbots, virtual assistants, advanced multi-step reasoning workflows, content generation, research review, brainstorming, and advanced data analysis among other GenAI applications.
  prohibited_uses: Unknown
  monitoring: Unknown
  feedback: Unknown
