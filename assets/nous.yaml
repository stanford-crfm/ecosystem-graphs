---
- type: model
  name: Nous Hermes 2
  organization: Nous Research
  description: Nous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research
    model trained over theÂ Mixtral 8x7B MoE LLM.
  created_date: 2024-01-10
  url: https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO
  model_card: https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO
  modality: text; code, text
  analysis: Evaluated across standard benchmarks and generally performs better than
    Mixtral, which it was fine-tuned on.
  size: 7B parameters (dense)
  dependencies: [Mixtral]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO/discussions
- type: model
  name: YaRN LLaMA 2
  organization: Nous Research, EleutherAI, University of Geneva
  description: YaRN LLaMA 2 is an adapted version of LLaMA 2 using the YaRN extension
    method.
  created_date: 2023-11-01
  url: https://arxiv.org/pdf/2309.00071.pdf
  model_card: https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k
  modality: text; text
  analysis: Evaluated across a variety of standard benchmarks in comparison to LLaMA
    2.
  size: 70B parameters (dense)
  dependencies: [LLaMA 2]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: LLaMA 2
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/NousResearch/Yarn-Llama-2-70b-32k/discussions
- type: model
  name: Nous Capybara
  organization: Nous Research
  description: The Capybara series is a series of LLMs and the first Nous collection
    of models made by fine-tuning mostly on data created by Nous in-house.
  created_date: 2023-11-13
  url: https://huggingface.co/NousResearch/Nous-Capybara-34B
  model_card: https://huggingface.co/NousResearch/Nous-Capybara-34B
  modality: text; text
  analysis: none
  size: 34B parameters (dense)
  dependencies: [Yi]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/NousResearch/Nous-Capybara-34B/discussions
- type: model
  name: YaRN Mistral
  organization: Nous Research, EleutherAI, University of Geneva
  description: YaRN Mistral is an adapted version of Mistral using the YaRN extension
    method.
  created_date: 2023-11-01
  url: https://arxiv.org/pdf/2309.00071.pdf
  model_card: https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k
  modality: text; text
  analysis: Evaluated across a variety of standard benchmarks in comparison to Mistral.
  size: 7B parameters (dense)
  dependencies: [Mistral]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k/discussions
- type: model
  name: OpenHermes 2.5 Mistral
  organization:
    explanation: developed as a personal project by Teknium, co-founder of Nous
      Research
    value: Nous Research
  description: OpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune,
    a continuation of OpenHermes 2 model, trained on additional code datasets.
  created_date: 2023-11-03
  url: https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B
  model_card: https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B
  modality: text; text
  analysis: Evaluated on common LLM benchmarks in comparison to other Mistral derivatives.
  size: 7B parameters (dense)
  dependencies: [Mistral]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: open
  license: Apache 2.0
  intended_uses: ''
  prohibited_uses: ''
  monitoring: unknown
  feedback: https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B/discussions
- type: model
  name: Hermes 2 Pro-Mistral
  organization: Nous
  description: Hermes 2 Pro on Mistral 7B is an upgraded, retrained version of Nous
    Hermes 2. This improved version excels at function calling, JSON Structured
    Outputs, and several other areas, scoring positively on various benchmarks.
  created_date: 2024-03-10
  url: https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B
  model_card: https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B
  modality: text; text
  analysis: The model was examined across a range of benchmarks including GPT4All,
    AGIEval, BigBench, TruthfulQA and in-house evaluations of function calling and
    JSON mode.
  size: 7B parameters (dense)
  dependencies: [Mistral, OpenHermes 2.5 Dataset, Nous Hermes 2]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: The model was evaluated across multiple tasks, displaying notable
    scores in GPT4All, AGIEval, BigBench, and TruthfulQA. It also has a high score
    on function calling and JSON mode, indicating the robustness of its capabilities.
  access: open
  license: Apache 2.0
  intended_uses: The model is intended for general task and conversation capabilities,
    function calling, and JSON structured outputs.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B/discussions
- type: model
  name: Genstruct
  organization: Nous
  description: Genstruct is an instruction-generation model, designed to create
    valid instructions given a raw text corpus. This enables the creation of new,
    partially synthetic instruction finetuning datasets from any raw-text corpus.
    This work was inspired by Ada-Instruct and the model is also trained to generate
    questions involving complex scenarios that require detailed reasoning.
  created_date: 2024-03-07
  url: https://huggingface.co/NousResearch/Genstruct-7B
  model_card: https://huggingface.co/NousResearch/Genstruct-7B
  modality: text; text
  analysis: unknown
  size: 7B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: open
  license: Apache 2.0
  intended_uses: The model is intended for instruction-generation, creating questions
    involving complex scenarios and generating reasoning steps for those questions.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: https://huggingface.co/NousResearch/Genstruct-7B/discussions
- type: model
  name: Arcee-Agent
  organization: CrusoeAI
  description: The Arcee Agent is a cutting-edge 7B parameter language model designed specifically for function calling and tool use. It presents a competitive performance, hosting advanced function calling and multi-format support. It can exist as a standalone chat agent or work as a tool router, routing requests to appropriate processing tools. Not only does it excel in function calling and tool use, but it also offers a high degree of speed and efficiency.
  created_date: unknown
  url: https://huggingface.co/arcee-ai/Arcee-Agent
  model card: https://huggingface.co/arcee-ai/Arcee-Agent
  modality: Text; Text
  analysis: The model showcases competitive performance in function calling and tool use tasks. However, detailed evaluations and benchmarking are not provided.
  size: 7B parameters
  dependencies: [Qwen2-7B]
  training_emissions: unknown
  training_time: unknown
  training_hardware: Compute for training Arcee-Agent was provided by CrusoeAI.
  quality_control: The model's function and API integration, database operations, and multi-step task executions are advanced features that ensure high-quality results. However, since the model focuses on function calling, general knowledge, and capabilities outside of this may be limited, and performance in unrelated core functionalities may not match those of models trained on diverse data. 
  access: open
  license: unknown
  intended_uses: Arcee Agent is designed for a wide range of applications where efficient function calling and tool use are crucial, such as developing chatbots, middleware, process automation in resource-constrained environments, prototyping tool-use scenarios, building interactive documentation systems, and AI-powered research assistants.
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown.
