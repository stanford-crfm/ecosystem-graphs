---

- type: dataset
  name: The Pile
  # General
  organization: EleutherAI
  description: >
    A latge language model training dataset, used to train GPT-NeoX-20B.
  created_date:
    value: 2021-01-01
  url: https://arxiv.org/pdf/2101.00027.pdf
  datasheet: https://arxiv.org/pdf/2201.07311.pdf
  modality: Text (English) and Code
  size: 825 GB
  sample:
    - '...pot trending topics and the coverage around them. First up, there’s a
      bit of a visual redesign. Previously, clicking on a trending topic would highlight
      a story from one publication, and you’d have to scroll down past a live video
      section to view related stories. Facebook is replacing that system with a
      simple carousel, which does a better job of showing you different coverage
      options. To be clear, the change doesn’t affect how stories are sourced, according
      to Facebook. It’s still the same algorithm pickin...'
    - Total knee arthroplasty (TKA) is a promising treatment for endstage osteoarthritis
      (OA) of the knee for alleviating pain and restoring the function of the knee.
      Some of the cases with bilateral TKA are symptomatic, necessitating revision
      arthroplasty in both the knees. A bilateral revision TKA can be done ei
    - On the converse, the set-valued map $\Phi:[0,3]\rightrightarrows [0,3]$ $$\Phi(x):=\left\{\begin{array}{ll}
      \{1\} & \mbox{ if } 0\leq x<1\\ {}[1,2] & \mbox{ if } 1\leq x\leq 2\\ \{2\}
      &
    - This Court thus uses the same interpretation of V.R.C.P. 52(a) as it did *487
      under the previous statutory requirement found in 12 V.S.A. § 2385.  In essense,
      the defendants urge that this Court should reconsider the case of Green Mountain
      Marble Co. v. Highway Board, supra, and follow the Federal practice of looking
      to the evide
  analysis:
    value: >
      Analyses of the data's composition, document statistics,
      language/dialectal coverage, topical distribution, and biases are
      conducted are conducted in the paper
      [[The Pile Paper]](https://arxiv.org/pdf/2101.00027.pdf).
  # Construction
  dependencies: []
  license:
    value: >
      The Pile uses the MIT License, allowing proprietary or open source use
      on the condition that the terms of the license as well as a copyright
      notice [[MIT License]](https://arxiv.org/pdf/2201.07311.pdf).
  included: >
    The Pile data come from 22 sources, with over half of the data being from
    Common Crawl (Pile-CC; 227GB), fiction and nonfiction books (Books3; 101GB),
    biomedical articles (PubMed Central; 90GB), and code (Github; 95 GB).
    Refer to the paper for full decomposition
    [[Table 1]](https://arxiv.org/pdf/2101.00027.pdf#table.caption.2).
  excluded: >
    Authors report that they have excluded some datasets "because they were too
    small to be worth spending time or because the English component of the data
    did not merit inclusion on its own. Three datasets were excluded for other
    reasons: (1) US Congressional Records were excluded because it "reflects the
    opinions and biases of the political class over the past 200 years,
    including segregationism and xenophobia." (2) Online Fanfiction resources
    amounting to Hundreds of GiB were excluded on logistical grounds.
    (3) Literotica, platform where users can upload short-form erotic fiction,
    was excluded because the authors decided to exclude fanfiction, the
    corpus would require significant investigation, and corpus contain
    significant amount of stereotyping
    [[Appendix B]](https://arxiv.org/pdf/2101.00027.pdf).
  quality_control:
    value: >
      In addition to the data inclusion and exclusion decisions, the quality was
      controlled through filtering for English (pycld2 language classifier),
      filtering for documents similar to OpenWebText2 (classifier on CommonCrawl),
      and several forms of deduplication as detailed in the paper
      [[Appendix C]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.C)
      [[Appendix D]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.D).
  # Downstream
  access:
    value: Full public access
    explanation: >
      The dataset is freely available to the public and
      can be downloaded from The Eye
      [[The Pile]](https://mystic.the-eye.eu/public/AI/pile/).
  intended_uses:
    value: >
      The Pile was intended to be used as a high quality large text dataset for
      language modeling tasks, explained in more detail in the paper
      [[Section 1]](https://arxiv.org/pdf/2101.00027.pdf#section.1).
  prohibited_uses:
    value: None
  monitoring:
    value: None
  feedback:
    value: >
      Feedback can be given by emailing the authors at contact at eleuther.ai.

- type: model
  name: GPT-J 6B
  organization: EleutherAI
  description: GPT-J is an open-source autoregressive large language model.
  created_date:
    value: 2021-06-04
    explanation: Date model blog post was published
  url: https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/
  model_card: ''
  modality: Text (English)
  analysis: ''
  size: 6B parameters (dense model)
  dependencies: ''
  training_emissions: ''
  training_time: ''
  training_hardware: 'TRC (Unspecified # of TPU v3-8s)'
  quality_control: ''
  access:
    value: Full public access
    explanation: >
      The model can be downloaded for free from [[The Eye]](https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500.tar.zstd)
  license:
    value: Apache 2.0
    explanation: >
      As indicated in the Github repository https://github.com/kingoflolz/mesh-transformer-jax
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: GPT-Neo
  organization: EleutherAI
  description: ''
  created_date:
    value: 2021-03-21
    explanation: Date Github repo was update
  url: https://github.com/EleutherAI/gpt-neo
  model_card: ''
  modality: Text (English)
  analysis: ''
  size: 2.7B parameters (dense model)
  dependencies: ''
  training_emissions: ''
  training_time: ''
  training_hardware: .nan
  quality_control: ''
  access:
    value: Full public access
    explanation: >
      The model can be downloaded for free from [[The Eye]](https://mystic.the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/)
  license:
    value: MIT
    explanation: .nan
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: GPT-NeoX-20B
  # General
  organization: EleutherAI
  description: >
    GPT-NeoX-20B is an open-sourced autoregressive large language model.
  created_date:
    value: 2022-02-02
  url: http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf
  model_card: https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md
  modality: Text (English) and Code
  size: 20B parameters (dense model)
  analysis:
    value: >
      The model was evaluated on standard NLP benchmarks: LAMBADA, ANLI,
      HellaSwag, MMLU among others
      [[Section 4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#section.4).
  # Construction
  dependencies:
    - The Pile
  training_emissions:
    value: 31.73 tCO2e
    explanation: >
      The amount of emission during the development and training of
      the model based on the author's estimation
      [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).
  training_time:
    value: 47.10 petaflop/s-day
    explanation: >
      Training time was reported as 1830 hours reported by the authors, equaling
      76.25 days.
      [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).
      The authors report that 96 (12 * 8) A100 GPUs were used during the
      training.
      The A100 GPUs have a single precision performance of 0.0195 petaflops
      [[A100 Datasheet]](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf).
      Assuming the estimated utilization is 33%, following
      [[OpenAI AI and Computer Blog]](https://openai.com/blog/ai-and-compute/#addendum),
      the training time is 47.10 petaflop/s-day (76.25 * 96 * 0.0195 * 0.33).
  training_hardware:
    value: 12 x 8 A100 GPUs
    explanation: >
      As outline by the authors
      [[Section 2.3]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.2.3)
  quality_control: >
    value:  None
  # Downstream
  access:
    value: Full public access
    explanation: >
      The model can be downloaded for free The Eye
      [[GPT-NeoX-20B]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/).
  license:
    value: Apache 2.0
    explanation: >
      As indicated in the accompanying blog post
      [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/).
  intended_uses:
    value: >
      As stated in the model card: "GPT-NeoX-20B learns an inner representation
      of the English language that can be used to extract features useful for
      downstream tasks. The model is best at what it was pretrained for however,
      which is generating text from a prompt.
      Due to the generality of the pretraining set, it has acquired the ability
      to generate completions across a wide range of tasks - from programming to
      fiction writing
      [[Model Card]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md).
  prohibited_uses:
    value: None
  monitoring:
    value: None
  feedback:
    value: >
      Feedback can be provided using the  # 20b channel in EleutherAI Discord
      group
      [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/).
      Find the Discord link in the FAQ page
      [[FAQ]](https://www.eleuther.ai/faq/).


- type: application
  name: GooseAI API
  # General
  organization: GooseAI
  description: >
    GooseAI API is an API service providing access to NLP services.
  created_date: Unknown
  url: goose.ai
  # Construction
  dependencies:
    - GPT-NeoX-20B
  adaptation: Unknown
  output_space:
    value: Text Generation, Text Completion
    explanation: >
      Question/Answer and Classification tasks are coming soon according to
      GooseAI
      [[Main Page]](goose.ai).
  quality_control: Unknown
  # Downstream
  access:
    value: Limited public access
    explanation: >
      GooseAI API can be accessed by signing up on the goose.ai website.
  license:
    value: Limited use license
    explanation: >
      "Subject to Customer’s strict compliance with this TOS, GooseAI grants
      Customer a limited, non-exclusive, non-transferable, non-sublicensable,
      revocable license to access and use the Platform as described in and
      subject to this TOS"
      [[GooseAI Terms of Service]](https://goose.ai/docs/tos)
  terms_of_service: https://goose.ai/docs/tos
  intended_uses: >
    Intended to be used as an NLP infrastructure.
  prohibited_uses:
    value: >
      Illegal or abusive activity, security violations, network abuse
    explanation: >
      Prohibited uses are detailed in the Acceptable Use Policy
      [[GooseAI Acceptable Use Policy]](https://goose.ai/docs/aup).
  monitoring:
    value: At will monitoring by the provider
    explanation: >
      In the "GooseAI Monitoring and Enforcement" section of GooseAI's
      Acceptable Use Policy (AUP), it is stated that Goose.AI has the right to
      investigate any suspected violation of its AUP
      [[GooseAI Acceptable Use Policy]](https://goose.ai/docs/aup).
  feedback:
    value: Email support
    explanation: >
      In the "Error Reporting and Feedback" section of the Goose.ai Terms of
      Service, GooseAI asks all the feedback to be sent to support at goose.ai
      [[GooseAI Terms of Service]](https://goose.ai/docs/tos).
  # Deployment
  monthly_active_users: Unknown
  user_distribution: Unknown
  failures: Unknown
